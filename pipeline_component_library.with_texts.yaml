annotations: {}
folders:
- name: Quick start
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/57f780b15922061e59833541b71f3d099e710177/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml
    digest: 42030acab16b71bbc3ad018563b5aedb94d373e72f7cb2b322bc27dbbee75e1b
    text: |
      name: Chicago Taxi Trips dataset
      description: |
        City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew

        The input parameters configure the SQL query to the database.
        The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.
        Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml'
      inputs:
      - {name: Where, type: String, default: 'trip_start_timestamp>="1900-01-01" AND trip_start_timestamp<"2100-01-01"'}
      - {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}
      - {name: Select, type: String, default: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total'}
      - {name: Format, type: String, default: 'csv', description: 'Output data format. Supports csv,tsv,cml,rdf,json'}
      outputs:
      - {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}
      implementation:
        container:
          # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
          image: alpine/curl:8.14.1
          command:
          - sh
          - -c
          - |
            set -e -x -o pipefail
            output_path="$0"
            select="$1"
            where="$2"
            limit="$3"
            format="$4"
            mkdir -p "$(dirname "$output_path")"
            curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}" \
                --data-urlencode '$limit='"${limit}" \
                --data-urlencode '$where='"${where}" \
                --data-urlencode '$select='"${select}" \
                | sed -E 's/"([^",\]*)"/\1/g' > "$output_path"  # Removing unneeded quotes around all numbers
          - {outputPath: Table}
          - {inputValue: Select}
          - {inputValue: Where}
          - {inputValue: Limit}
          - {inputValue: Format}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/component.yaml
    digest: 5b8bec6716337d7bd8ecafd6dd46bc44527783bc05950cfe03a60206b43c7654
    text: |
      name: Train XGBoost model on CSV
      description: Trains an XGBoost model.
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'}
      inputs:
      - {name: training_data, type: CSV, description: Training data in CSV format.}
      - {name: label_column_name, type: String, description: Name of the column containing
          the label data.}
      - {name: starting_model, type: XGBoostModel, description: Existing trained model to
          start from (in the binary XGBoost format)., optional: true}
      - {name: num_iterations, type: Integer, description: Number of boosting iterations.,
        default: '10', optional: true}
      - name: objective
        type: String
        description: |-
          The learning task and the corresponding learning objective.
          See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
          The most common values are:
          "reg:squarederror" - Regression with squared loss (default).
          "reg:logistic" - Logistic regression.
          "binary:logistic" - Logistic regression for binary classification, output probability.
          "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
          "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
          "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
        default: reg:squarederror
        optional: true
      - {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,
          `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`
          uses linear functions.', default: gbtree, optional: true}
      - {name: learning_rate, type: Float, description: 'Step size shrinkage used in update
          to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}
      - name: min_split_loss
        type: Float
        description: |-
          Minimum loss reduction required to make a further partition on a leaf node of the tree.
          The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
        default: '0'
        optional: true
      - name: max_depth
        type: Integer
        description: |-
          Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
          0 indicates no limit on depth. Range: [0,Inf].
        default: '6'
        optional: true
      - {name: booster_params, type: JsonObject, description: 'Parameters for the booster.
          See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}
      outputs:
      - {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost
          format.}
      - {name: model_config, type: XGBoostModelConfig, description: The internal parameter
          configuration of Booster as a JSON string.}
      implementation:
        container:
          image: python:3.10
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
            --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def train_XGBoost_model_on_CSV(
                training_data_path,
                model_path,
                model_config_path,
                label_column_name,
                starting_model_path = None,
                num_iterations = 10,
                # Booster parameters
                objective = "reg:squarederror",
                booster = "gbtree",
                learning_rate = 0.3,
                min_split_loss = 0,
                max_depth = 6,
                booster_params = None,
            ):
                """Trains an XGBoost model.

                Args:
                    training_data_path: Training data in CSV format.
                    model_path: Trained model in the binary XGBoost format.
                    model_config_path: The internal parameter configuration of Booster as a JSON string.
                    starting_model_path: Existing trained model to start from (in the binary XGBoost format).
                    label_column_name: Name of the column containing the label data.
                    num_iterations: Number of boosting iterations.
                    booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                    objective: The learning task and the corresponding learning objective.
                        See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                        The most common values are:
                        "reg:squarederror" - Regression with squared loss (default).
                        "reg:logistic" - Logistic regression.
                        "binary:logistic" - Logistic regression for binary classification, output probability.
                        "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                        "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                        "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
                    booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.
                    learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].
                    min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.
                        The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
                    max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                        0 indicates no limit on depth. Range: [0,Inf].

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                """
                import pandas
                import xgboost

                df = pandas.read_csv(
                    training_data_path,
                ).convert_dtypes()
                print("Training data information:")
                df.info(verbose=True)
                # Converting column types that XGBoost does not support
                for column_name, dtype in df.dtypes.items():
                    if dtype in ["string", "object"]:
                        print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                        df[column_name] = df[column_name].astype("category")
                        print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                    # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                    if pandas.api.types.is_float_dtype(dtype):
                        # Converting from "Float64" to "float64"
                        df[column_name] = df[column_name].astype(dtype.name.lower())
                print()
                print("Final training data information:")
                df.info(verbose=True)

                training_data = xgboost.DMatrix(
                    data=df.drop(columns=[label_column_name]),
                    label=df[[label_column_name]],
                    enable_categorical=True,
                )

                booster_params = booster_params or {}
                booster_params.setdefault("objective", objective)
                booster_params.setdefault("booster", booster)
                booster_params.setdefault("learning_rate", learning_rate)
                booster_params.setdefault("min_split_loss", min_split_loss)
                booster_params.setdefault("max_depth", max_depth)

                starting_model = None
                if starting_model_path:
                    starting_model = xgboost.Booster(model_file=starting_model_path)

                print()
                print("Training the model:")
                model = xgboost.train(
                    params=booster_params,
                    dtrain=training_data,
                    num_boost_round=num_iterations,
                    xgb_model=starting_model,
                    evals=[(training_data, "training_data")],
                )

                # Saving the model in binary format
                model.save_model(model_path)

                model_config_str = model.save_config()
                with open(model_config_path, "w") as model_config_file:
                    model_config_file.write(model_config_str)

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Train XGBoost model on CSV', description='Trains an XGBoost model.')
            _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = train_XGBoost_model_on_CSV(**_parsed_args)
          args:
          - --training-data
          - {inputPath: training_data}
          - --label-column-name
          - {inputValue: label_column_name}
          - if:
              cond: {isPresent: starting_model}
              then:
              - --starting-model
              - {inputPath: starting_model}
          - if:
              cond: {isPresent: num_iterations}
              then:
              - --num-iterations
              - {inputValue: num_iterations}
          - if:
              cond: {isPresent: objective}
              then:
              - --objective
              - {inputValue: objective}
          - if:
              cond: {isPresent: booster}
              then:
              - --booster
              - {inputValue: booster}
          - if:
              cond: {isPresent: learning_rate}
              then:
              - --learning-rate
              - {inputValue: learning_rate}
          - if:
              cond: {isPresent: min_split_loss}
              then:
              - --min-split-loss
              - {inputValue: min_split_loss}
          - if:
              cond: {isPresent: max_depth}
              then:
              - --max-depth
              - {inputValue: max_depth}
          - if:
              cond: {isPresent: booster_params}
              then:
              - --booster-params
              - {inputValue: booster_params}
          - --model
          - {outputPath: model}
          - --model-config
          - {outputPath: model_config}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/component.yaml
    digest: 6fd7196d2061e6f49ec98459c90f4b1e4e63bca21170c70b23f7679921ce01d7
    text: |
      name: Xgboost predict on CSV
      description: Makes predictions using a trained XGBoost model.
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'}
      inputs:
      - {name: data, type: CSV, description: Feature data in Apache Parquet format.}
      - {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}
      - {name: label_column_name, type: String, description: Optional. Name of the column
          containing the label data that is excluded during the prediction., optional: true}
      outputs:
      - {name: predictions, description: Model predictions.}
      implementation:
        container:
          image: python:3.10
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
            --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def xgboost_predict_on_CSV(
                data_path,
                model_path,
                predictions_path,
                label_column_name = None,
            ):
                """Makes predictions using a trained XGBoost model.

                Args:
                    data_path: Feature data in Apache Parquet format.
                    model_path: Trained model in binary XGBoost format.
                    predictions_path: Model predictions.
                    label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                """
                from pathlib import Path

                import numpy
                import pandas
                import xgboost

                df = pandas.read_csv(
                    data_path,
                ).convert_dtypes()
                print("Evaluation data information:")
                df.info(verbose=True)
                # Converting column types that XGBoost does not support
                for column_name, dtype in df.dtypes.items():
                    if dtype in ["string", "object"]:
                        print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                        df[column_name] = df[column_name].astype("category")
                        print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                    # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                    if pandas.api.types.is_float_dtype(dtype):
                        # Converting from "Float64" to "float64"
                        df[column_name] = df[column_name].astype(dtype.name.lower())
                print("Final evaluation data information:")
                df.info(verbose=True)

                if label_column_name is not None:
                    df = df.drop(columns=[label_column_name])

                testing_data = xgboost.DMatrix(
                    data=df,
                    enable_categorical=True,
                )

                model = xgboost.Booster(model_file=model_path)

                predictions = model.predict(testing_data)

                Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                numpy.savetxt(predictions_path, predictions)

            import argparse
            _parser = argparse.ArgumentParser(prog='Xgboost predict on CSV', description='Makes predictions using a trained XGBoost model.')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = xgboost_predict_on_CSV(**_parsed_args)
          args:
          - --data
          - {inputPath: data}
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: label_column_name}
              then:
              - --label-column-name
              - {inputValue: label_column_name}
          - --predictions
          - {outputPath: predictions}
- name: Basics
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/c36d9a4893c3caae0d166d9e783af28fcd3cdfe4/components/basics/Format_date_time/component.yaml
    digest: 7784069b3bd1301674cfb4bbf7170b7db91ffd289138eb2d8838e3368158b70c
    text: |
      name: Format date and time
      inputs:
        - {name: Date, type: DateTime}
        - {name: Format, type: String, default: "%Y-%m-%d %H:%M:%S.%N", description: "Format string for date and time. See [man date](https://linux.die.net/man/1/date)."}
      outputs:
        - {name: Formatted date, type: String}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/basics/Format_date_time/component.yaml'
      implementation:
        container:
          image: ubuntu
          command:
            - sh
            - -exc
            - |
              date="$0"
              format="$1"
              output_path="$2"
              mkdir -p "$(dirname "$output_path")"

              date -d "$date" +"$format" >"$output_path"
            - {inputValue: Date}
            - {inputValue: Format}
            - {outputPath: Formatted date}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/37d98d43ad3193cf3516c134899f272d9643117c/components/basics/Calculate_hash/component.yaml
    digest: 6afc1b9d9c845fcdf0e9820aa97c9544c0f8b1ec2b7c1cf481975231711f6503
    text: |
      name: Calculate data hash
      inputs:
      - {name: Data}
      - {name: Hash algorithm, type: String, default: SHA256, description: "Hash algorithm to use. Supported values are MD5, SHA1, SHA256, SHA512, SHA3"}
      outputs:
      - {name: Hash}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/basics/Calculate_hash/component.yaml'
      implementation:
        container:
          image: alpine
          command:
          - sh
          - -exc
          - |
            data_path="$0"
            hash_algorithm="$1"
            hash_path="$2"
            mkdir -p "$(dirname "$hash_path")"

            hash_algorithm=$(echo "$hash_algorithm" | tr '[:upper:]' '[:lower:]')
            case "$hash_algorithm" in
                md5|sha1|sha256|sha512|sha3)  hash_program="${hash_algorithm}sum";;
                *)  echo "Unsupported hash algorithm $hash_algorithm"; exit 1;;
            esac

            if [ -d "$data_path" ]; then
                # Calculating hash for directory
                cd "$data_path"
                find . -type f -print0 |
                    sort -z |
                    xargs -0 "$hash_program" |
                    "$hash_program" |
                    cut -d ' ' -f 1 > "$hash_path"
            else
                # Calculating hash for file
                "$hash_program" "$data_path" |
                    cut -d ' ' -f 1 > "$hash_path"
            fi
          - {inputPath: Data}
          - {inputValue: Hash algorithm}
          - {outputPath: Hash}
  folders:
  - name: File system
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/605cefc151797369752b51530a91cc02e7f913f8/components/filesystem/create_directory/component.yaml
      digest: 6fa449465ecf656c02d07622acb5df045c3863acc15c607e1817f4e9f8078134
      text: |
        name: Create directory from files (5)
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/filesystem/create_directory/component.yaml"
        inputs:
          - { name: File 1 }
          - { name: File 1 name, type: String, default: "1" }
          - { name: File 2, optional: true }
          - { name: File 2 name, type: String, default: "2" }
          - { name: File 3, optional: true }
          - { name: File 3 name, type: String, default: "3" }
          - { name: File 4, optional: true }
          - { name: File 4 name, type: String, default: "4" }
          - { name: File 5, optional: true }
          - { name: File 5 name, type: String, default: "5" }
        outputs:
          - { name: Directory, type: Directory }
        implementation:
          container:
            image: alpine
            command:
              - sh
              - -ec
              - |
                output_path="$1"
                shift
                mkdir -p "$output_path"
                while [ "$#" -gt 0 ]; do
                  input_path="$1"
                  file_name="$2"
                  shift 2
                  cp -r "$input_path" "$output_path/$file_name"
                done
              - -c  # stabilize $0 vs $1
              - { outputPath: Directory }
              - { inputPath: File 1 }
              - { inputValue: File 1 name }
              - if:
                  cond: { isPresent: File 2 }
                  then:
                    - { inputPath: File 2 }
                    - { inputValue: File 2 name }
              - if:
                  cond: { isPresent: File 3 }
                  then:
                    - { inputPath: File 3 }
                    - { inputValue: File 3 name }
              - if:
                  cond: { isPresent: File 4 }
                  then:
                    - { inputPath: File 4 }
                    - { inputValue: File 4 name }
              - if:
                  cond: { isPresent: File 5 }
                  then:
                    - { inputPath: File 5 }
                    - { inputValue: File 5 name }
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/filesystem/get_file/component.yaml
      digest: 4415166532fa69baa33378631802fab630f51ea1677ce6408cef946672f4a9d2
      text: |
        name: Get file
        description: Get file from directory.
        inputs:
        - {name: Directory, type: Directory}
        - {name: Subpath, type: String}
        outputs:
        - {name: File}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/filesystem/get_file/component.yaml'
        implementation:
          container:
            image: alpine
            command:
            - sh
            - -ex
            - -c
            - |
              mkdir -p "$(dirname "$2")"
              cp -r "$0/$1" "$2"
            - inputPath: Directory
            - inputValue: Subpath
            - outputPath: File
- name: Datasets
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/57f780b15922061e59833541b71f3d099e710177/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml
    digest: 42030acab16b71bbc3ad018563b5aedb94d373e72f7cb2b322bc27dbbee75e1b
    text: |
      name: Chicago Taxi Trips dataset
      description: |
        City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew

        The input parameters configure the SQL query to the database.
        The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.
        Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml'
      inputs:
      - {name: Where, type: String, default: 'trip_start_timestamp>="1900-01-01" AND trip_start_timestamp<"2100-01-01"'}
      - {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}
      - {name: Select, type: String, default: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total'}
      - {name: Format, type: String, default: 'csv', description: 'Output data format. Supports csv,tsv,cml,rdf,json'}
      outputs:
      - {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}
      implementation:
        container:
          # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
          image: alpine/curl:8.14.1
          command:
          - sh
          - -c
          - |
            set -e -x -o pipefail
            output_path="$0"
            select="$1"
            where="$2"
            limit="$3"
            format="$4"
            mkdir -p "$(dirname "$output_path")"
            curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}" \
                --data-urlencode '$limit='"${limit}" \
                --data-urlencode '$where='"${where}" \
                --data-urlencode '$select='"${select}" \
                | sed -E 's/"([^",\]*)"/\1/g' > "$output_path"  # Removing unneeded quotes around all numbers
          - {outputPath: Table}
          - {inputValue: Select}
          - {inputValue: Where}
          - {inputValue: Limit}
          - {inputValue: Format}
- name: Data manipulation
  folders:
  - name: Parquet
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Select_columns/in_ApacheParquet_format/component.yaml
      digest: 7cf49b426385086fbb58b2d7c900637439f9dfff0df5661591cb6840b125d57e
      text: |
        name: Select columns using Pandas on ApacheParquet data
        description: Selects columns from a data table.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Select_columns/in_ApacheParquet_format/component.yaml'}
        inputs:
        - {name: table, type: ApacheParquet, description: Input data table.}
        - {name: column_names, type: JsonArray, description: Names of the columns to select
            from the table.}
        outputs:
        - {name: transformed_table, type: ApacheParquet, description: Transformed data table
            that only has the chosen columns.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def select_columns_using_Pandas_on_ApacheParquet_data(
                  table_path,
                  transformed_table_path,
                  column_names,
              ):
                  """Selects columns from a data table.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table that only has the chosen columns.
                      column_names: Names of the columns to select from the table.
                  """
                  import pandas

                  df = pandas.read_parquet(table_path)
                  df = df[column_names]
                  df.to_parquet(transformed_table_path, index=False)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Select columns using Pandas on ApacheParquet data', description='Selects columns from a data table.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = select_columns_using_Pandas_on_ApacheParquet_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-names
            - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Fill_all_missing_values/in_ApacheParquet_format/component.yaml
      digest: 0fd11307bd634351c521d4ae5d32f547e0a77304b17e2eab046e092debb3b2b4
      text: |
        name: Fill all missing values using Pandas on ApacheParquet data
        description: Fills the missing column items with the specified replacement value.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Fill_all_missing_values/in_ApacheParquet_format/component.yaml'}
        inputs:
        - {name: table, type: ApacheParquet, description: Input data table.}
        - {name: replacement_value, type: String, description: The value to use when replacing
            the missing items., default: '0', optional: true}
        - {name: column_names, type: JsonArray, description: Names of the columns where to
            perform the replacement., optional: true}
        outputs:
        - {name: transformed_table, type: ApacheParquet, description: Transformed data table
            where missing values are filed.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def fill_all_missing_values_using_Pandas_on_ApacheParquet_data(
                  table_path,
                  transformed_table_path,
                  replacement_value = "0",
                  column_names = None,
              ):
                  """Fills the missing column items with the specified replacement value.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table where missing values are filed.
                      replacement_value: The value to use when replacing the missing items.
                      column_names: Names of the columns where to perform the replacement.
                  """
                  import pandas

                  df = pandas.read_parquet(path=table_path)
                  for column_name in column_names or df.columns:
                      column = df[column_name]
                      # The `.astype` method does not work correctly on booleans
                      # So we need to special-case them
                      if pandas.api.types.is_bool_dtype(column.dtype):
                          if replacement_value.lower() in ("true", "1"):
                              converted_replacement_value = True
                          elif replacement_value.lower() in ("false", "0"):
                              converted_replacement_value = False
                          else:
                              raise ValueError(
                                  f"Cannot convert value '{replacement_value}' to boolean for column {column_name}."
                              )
                      else:
                          # Using Pandas to convert the replacement_value to column.dtype.
                          converted_replacement_value = pandas.Series(
                              replacement_value, dtype=column.dtype
                          ).tolist()[0]

                      print(
                          f"Filling missing values in column '{column_name}' with '{converted_replacement_value}'"
                      )
                      column.fillna(value=converted_replacement_value)

                  df.to_parquet(
                      path=transformed_table_path,
                      index=False,
                  )

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Fill all missing values using Pandas on ApacheParquet data', description='Fills the missing column items with the specified replacement value.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--replacement-value", dest="replacement_value", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = fill_all_missing_values_using_Pandas_on_ApacheParquet_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: replacement_value}
                then:
                - --replacement-value
                - {inputValue: replacement_value}
            - if:
                cond: {isPresent: column_names}
                then:
                - --column-names
                - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Binarize_column/in_ApacheParquet_format/component.yaml
      digest: 3a8d435090075b9064e62974a07baf316063b248b4c175d9e1f42bdf1bdd3439
      text: |
        name: Binarize column using Pandas on ApacheParquet data
        description: Transforms a table column into a binary class column using a predicate.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Binarize_column/in_ApacheParquet_format/component.yaml'}
        inputs:
        - {name: table, type: ApacheParquet, description: Input data table.}
        - {name: column_name, type: String, description: Name of the column to transform to
            binary class.}
        - {name: predicate, type: String, description: Expression that determines whether
            the column value is mapped to class 0 (false) or class 1 (true)., default: '>
            0', optional: true}
        - {name: new_column_name, type: String, description: Name for the new class column.
            Equals column_name by default., optional: true}
        - name: keep_original_column
          type: Boolean
          description: Whether to keep the original column (column_name) in the table.
          default: "False"
          optional: true
        outputs:
        - {name: transformed_table, type: ApacheParquet, description: Transformed data table
            with the binary class column.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def binarize_column_using_Pandas_on_ApacheParquet_data(
                  table_path,
                  transformed_table_path,
                  column_name,
                  predicate = "> 0",
                  new_column_name = None,
                  keep_original_column = False,
              ):
                  """Transforms a table column into a binary class column using a predicate.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table with the binary class column.
                      column_name: Name of the column to transform to binary class.
                      predicate: Expression that determines whether the column value is mapped to class 0 (false) or class 1 (true).
                      new_column_name: Name for the new class column. Equals column_name by default.
                      keep_original_column: Whether to keep the original column (column_name) in the table.
                  """
                  import pandas

                  df = pandas.read_parquet(path=table_path)
                  original_series = df[column_name]

                  # Dynamically executing the predicate code
                  # Variable namespace for code execution
                  namespace = dict(x=original_series)
                  # I though that there should be no space before `predicate` so that "dot" predicate methods like ".between(min, max)" work.
                  # However Python allows spaces before dot: `df .isna()`.
                  # So having a space is not a problem
                  transform_code = f"""new_series_boolean = x {predicate}"""
                  # Note: exec() takes no keyword arguments
                  # exec(__source=transform_code, __globals=namespace)
                  exec(transform_code, namespace)
                  new_series_boolean = namespace["new_series_boolean"]

                  # There are multiple ways to convert boolean column to integer.
                  # .apply(int) might be faster. https://stackoverflow.com/a/49804868/1497385
                  # TODO: Do a proper benchmark.
                  new_series = new_series_boolean.apply(int)
                  # new_series = new_series_boolean.astype(int)
                  # new_series = new_series_boolean.replace({False: 0, True: 1})

                  if new_column_name:
                      df.insert(loc=0, column=new_column_name, value=new_series)
                      if not keep_original_column:
                          df = df.drop(columns=[column_name])
                  else:
                      df[column_name] = new_series

                  df.to_parquet(path=transformed_table_path)

              def _deserialize_bool(s) -> bool:
                  from distutils.util import strtobool
                  return strtobool(s) == 1

              import argparse
              _parser = argparse.ArgumentParser(prog='Binarize column using Pandas on ApacheParquet data', description='Transforms a table column into a binary class column using a predicate.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-name", dest="column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--predicate", dest="predicate", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--new-column-name", dest="new_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--keep-original-column", dest="keep_original_column", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = binarize_column_using_Pandas_on_ApacheParquet_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-name
            - {inputValue: column_name}
            - if:
                cond: {isPresent: predicate}
                then:
                - --predicate
                - {inputValue: predicate}
            - if:
                cond: {isPresent: new_column_name}
                then:
                - --new-column-name
                - {inputValue: new_column_name}
            - if:
                cond: {isPresent: keep_original_column}
                then:
                - --keep-original-column
                - {inputValue: keep_original_column}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml
      digest: 45cad5c51cbebcc10c6c552709ba367e3f516f98a33dfaadd776737d69375885
      text: |
        name: Pandas Transform DataFrame in ApacheParquet format
        description: |-
          Transform DataFrame loaded from an ApacheParquet file.

              Inputs:
                  table: DataFrame to transform.
                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                      The DataFrame variable is called "df".
                      Examples:
                      - `df['prod'] = df['X'] * df['Y']`
                      - `df = df[['X', 'prod']]`
                      - `df.insert(0, "is_positive", df["X"] > 0)`

              Outputs:
                  transformed_table: Transformed DataFrame.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: table, type: ApacheParquet}
        - {name: transform_code, type: PythonCode}
        outputs:
        - {name: transformed_table, type: ApacheParquet}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.0.4' 'pyarrow==0.14.1' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' 'pyarrow==0.14.1' 'numpy<2'
              --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def Pandas_Transform_DataFrame_in_ApacheParquet_format(
                  table_path,
                  transformed_table_path,
                  transform_code,
              ):
                  '''Transform DataFrame loaded from an ApacheParquet file.

                  Inputs:
                      table: DataFrame to transform.
                      transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                          The DataFrame variable is called "df".
                          Examples:
                          - `df['prod'] = df['X'] * df['Y']`
                          - `df = df[['X', 'prod']]`
                          - `df.insert(0, "is_positive", df["X"] > 0)`

                  Outputs:
                      transformed_table: Transformed DataFrame.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import pandas

                  df = pandas.read_parquet(table_path)
                  # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                  namespace = locals()
                  exec(transform_code, namespace)
                  namespace['df'].to_parquet(transformed_table_path)

              import argparse
              _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in ApacheParquet format', description='Transform DataFrame loaded from an ApacheParquet file.\n\n    Inputs:\n        table: DataFrame to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed DataFrame.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = Pandas_Transform_DataFrame_in_ApacheParquet_format(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --transform-code
            - {inputValue: transform_code}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml
      digest: ca3834a0b29b0647f0e48f9d45419bb7ae05c6f4cd6fc98f4d99a28979915284
      text: |
        name: Split table into folds
        description: |-
          Splits the data table into the specified number of folds.

              The data is split into the specified number of folds k (default: 5).
              Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
              Each training subsample has (k-1)/k fraction of samples.
              The train_i subsample is produced by excluding test_i subsample form all samples.

              Inputs:
                  table: The data to split by rows
                  number_of_folds: Number of folds to split data into
                  random_seed: Random seed for reproducible splitting

              Outputs:
                  train_i: The i-th training subsample
                  test_i: The i-th testing subsample

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'
        inputs:
        - {name: table, type: CSV}
        - {name: number_of_folds, type: Integer, default: '5', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: train_1, type: CSV}
        - {name: train_2, type: CSV}
        - {name: train_3, type: CSV}
        - {name: train_4, type: CSV}
        - {name: train_5, type: CSV}
        - {name: test_1, type: CSV}
        - {name: test_2, type: CSV}
        - {name: test_3, type: CSV}
        - {name: test_4, type: CSV}
        - {name: test_5, type: CSV}
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def split_table_into_folds(
                  table_path,

                  train_1_path,
                  train_2_path,
                  train_3_path,
                  train_4_path,
                  train_5_path,

                  test_1_path,
                  test_2_path,
                  test_3_path,
                  test_4_path,
                  test_5_path,

                  number_of_folds = 5,
                  random_seed = 0,
              ):
                  """Splits the data table into the specified number of folds.

                  The data is split into the specified number of folds k (default: 5).
                  Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
                  Each training subsample has (k-1)/k fraction of samples.
                  The train_i subsample is produced by excluding test_i subsample form all samples.

                  Inputs:
                      table: The data to split by rows
                      number_of_folds: Number of folds to split data into
                      random_seed: Random seed for reproducible splitting

                  Outputs:
                      train_i: The i-th training subsample
                      test_i: The i-th testing subsample

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>

                  """
                  import pandas
                  from sklearn import model_selection

                  max_number_of_folds = 5

                  if number_of_folds < 1 or number_of_folds > max_number_of_folds:
                      raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )
                  splitter = model_selection.KFold(
                      n_splits=number_of_folds,
                      shuffle=True,
                      random_state=random_seed,
                  )
                  folds = list(splitter.split(df))

                  fold_paths = [
                      (train_1_path, test_1_path),
                      (train_2_path, test_2_path),
                      (train_3_path, test_3_path),
                      (train_4_path, test_4_path),
                      (train_5_path, test_5_path),
                  ]

                  for i in range(max_number_of_folds):
                      (train_path, test_path) = fold_paths[i]
                      if i < len(folds):
                          (train_indices, test_indices) = folds[i]
                          train_fold = df.iloc[train_indices]
                          test_fold = df.iloc[test_indices]
                      else:
                          train_fold = df.iloc[0:0]
                          test_fold = df.iloc[0:0]
                      train_fold.to_csv(train_path, index=False)
                      test_fold.to_csv(test_path, index=False)

              import argparse
              _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-folds", dest="number_of_folds", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--train-1", dest="train_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-2", dest="train_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-3", dest="train_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-4", dest="train_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-5", dest="train_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-1", dest="test_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-2", dest="test_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-3", dest="test_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-4", dest="test_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-5", dest="test_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = split_table_into_folds(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: number_of_folds}
                then:
                - --number-of-folds
                - {inputValue: number_of_folds}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --train-1
            - {outputPath: train_1}
            - --train-2
            - {outputPath: train_2}
            - --train-3
            - {outputPath: train_3}
            - --train-4
            - {outputPath: train_4}
            - --train-5
            - {outputPath: train_5}
            - --test-1
            - {outputPath: test_1}
            - --test-2
            - {outputPath: test_2}
            - --test-3
            - {outputPath: test_3}
            - --test-4
            - {outputPath: test_4}
            - --test-5
            - {outputPath: test_5}
  - name: CSV
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Select_columns/in_CSV_format/component.yaml
      digest: 7160345cf18c12fa2a5230c1ed5ceef9f2cc40b8b72d39995c72f99755622bec
      text: |
        name: Select columns using Pandas on CSV data
        description: Selects columns from a data table.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Select_columns/in_CSV_format/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: column_names, type: JsonArray, description: Names of the columns to select
            from the table.}
        outputs:
        - {name: transformed_table, type: CSV, description: Transformed data table that only
            has the chosen columns.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.2' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.2' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def select_columns_using_Pandas_on_CSV_data(
                  table_path,
                  transformed_table_path,
                  column_names,
              ):
                  """Selects columns from a data table.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table that only has the chosen columns.
                      column_names: Names of the columns to select from the table.
                  """
                  import pandas

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )
                  df = df[column_names]
                  df.to_csv(transformed_table_path, index=False)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Select columns using Pandas on CSV data', description='Selects columns from a data table.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = select_columns_using_Pandas_on_CSV_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-names
            - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml
      digest: c873de1f3a95bb7fb17efbbc5a72c4a13a1ef581dc162054707441ddce2c06d2
      text: |
        name: Fill all missing values using Pandas on CSV data
        description: Fills the missing column items with the specified replacement value.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: replacement_value, type: String, description: The value to use when replacing
            the missing items., default: '0', optional: true}
        - {name: column_names, type: JsonArray, description: Names of the columns where to
            perform the replacement., optional: true}
        outputs:
        - {name: transformed_table, type: CSV, description: Transformed data table where missing
            values are filed.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.1' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.1' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def fill_all_missing_values_using_Pandas_on_CSV_data(
                  table_path,
                  transformed_table_path,
                  replacement_value = "0",
                  column_names = None,
              ):
                  """Fills the missing column items with the specified replacement value.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table where missing values are filed.
                      replacement_value: The value to use when replacing the missing items.
                      column_names: Names of the columns where to perform the replacement.
                  """
                  import pandas

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )

                  for column_name in column_names or df.columns:
                      df[column_name] = df[column_name].fillna(value=replacement_value)

                  df.to_csv(
                      transformed_table_path, index=False,
                  )

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Fill all missing values using Pandas on CSV data', description='Fills the missing column items with the specified replacement value.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--replacement-value", dest="replacement_value", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = fill_all_missing_values_using_Pandas_on_CSV_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: replacement_value}
                then:
                - --replacement-value
                - {inputValue: replacement_value}
            - if:
                cond: {isPresent: column_names}
                then:
                - --column-names
                - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Binarize_column/in_CSV_format/component.yaml
      digest: 98e0babb5e2ac915e5c8969b76f61d8e7b78637aef66f7fb508a8f3f35d63cf1
      text: |
        name: Binarize column using Pandas on CSV data
        description: Transforms a table column into a binary class column using a predicate.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Binarize_column/in_CSV_format/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: column_name, type: String, description: Name of the column to transform to
            binary class.}
        - {name: predicate, type: String, description: Expression that determines whether
            the column value is mapped to class 0 (false) or class 1 (true)., default: '>
            0', optional: true}
        - {name: new_column_name, type: String, description: Name for the new class column.
            Equals column_name by default., optional: true}
        - name: keep_original_column
          type: Boolean
          description: Whether to keep the original column (column_name) in the table.
          default: "False"
          optional: true
        outputs:
        - {name: transformed_table, type: CSV, description: Transformed data table with the
            binary class column.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def binarize_column_using_Pandas_on_CSV_data(
                  table_path,
                  transformed_table_path,
                  column_name,
                  predicate = "> 0",
                  new_column_name = None,
                  keep_original_column = False,
              ):
                  """Transforms a table column into a binary class column using a predicate.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table with the binary class column.
                      column_name: Name of the column to transform to binary class.
                      predicate: Expression that determines whether the column value is mapped to class 0 (false) or class 1 (true).
                      new_column_name: Name for the new class column. Equals column_name by default.
                      keep_original_column: Whether to keep the original column (column_name) in the table.
                  """
                  import pandas

                  df = pandas.read_csv(table_path).convert_dtypes()
                  original_series = df[column_name]

                  # Dynamically executing the predicate code
                  # Variable namespace for code execution
                  namespace = dict(x=original_series)
                  # I though that there should be no space before `predicate` so that "dot" predicate methods like ".between(min, max)" work.
                  # However Python allows spaces before dot: `df .isna()`.
                  # So having a space is not a problem
                  transform_code = f"""new_series_boolean = x {predicate}"""
                  # Note: exec() takes no keyword arguments
                  # exec(__source=transform_code, __globals=namespace)
                  exec(transform_code, namespace)
                  new_series_boolean = namespace["new_series_boolean"]

                  # There are multiple ways to convert boolean column to integer.
                  # .apply(int) might be faster. https://stackoverflow.com/a/49804868/1497385
                  # TODO: Do a proper benchmark.
                  new_series = new_series_boolean.apply(int)
                  # new_series = new_series_boolean.astype(int)
                  # new_series = new_series_boolean.replace({False: 0, True: 1})

                  if new_column_name:
                      df.insert(loc=0, column=new_column_name, value=new_series)
                      if not keep_original_column:
                          df = df.drop(columns=[column_name])
                  else:
                      df[column_name] = new_series

                  df.to_csv(transformed_table_path, index=False)

              def _deserialize_bool(s) -> bool:
                  from distutils.util import strtobool
                  return strtobool(s) == 1

              import argparse
              _parser = argparse.ArgumentParser(prog='Binarize column using Pandas on CSV data', description='Transforms a table column into a binary class column using a predicate.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-name", dest="column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--predicate", dest="predicate", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--new-column-name", dest="new_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--keep-original-column", dest="keep_original_column", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = binarize_column_using_Pandas_on_CSV_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-name
            - {inputValue: column_name}
            - if:
                cond: {isPresent: predicate}
                then:
                - --predicate
                - {inputValue: predicate}
            - if:
                cond: {isPresent: new_column_name}
                then:
                - --new-column-name
                - {inputValue: new_column_name}
            - if:
                cond: {isPresent: keep_original_column}
                then:
                - --keep-original-column
                - {inputValue: keep_original_column}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml
      digest: 6cf3a8e4fedec9f5e92ade9616b06c0717cc7f7622f2da8c6c646d2bed17fa00
      text: |
        name: Pandas Transform DataFrame in CSV format
        description: |-
          Transform DataFrame loaded from a CSV file.

              Inputs:
                  table: Table to transform.
                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                      The DataFrame variable is called "df".
                      Examples:
                      - `df['prod'] = df['X'] * df['Y']`
                      - `df = df[['X', 'prod']]`
                      - `df.insert(0, "is_positive", df["X"] > 0)`

              Outputs:
                  transformed_table: Transformed table.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: table, type: CSV}
        - {name: transform_code, type: PythonCode}
        outputs:
        - {name: transformed_table, type: CSV}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def Pandas_Transform_DataFrame_in_CSV_format(
                  table_path,
                  transformed_table_path,
                  transform_code,
              ):
                  '''Transform DataFrame loaded from a CSV file.

                  Inputs:
                      table: Table to transform.
                      transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                          The DataFrame variable is called "df".
                          Examples:
                          - `df['prod'] = df['X'] * df['Y']`
                          - `df = df[['X', 'prod']]`
                          - `df.insert(0, "is_positive", df["X"] > 0)`

                  Outputs:
                      transformed_table: Transformed table.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import pandas

                  df = pandas.read_csv(
                      table_path,
                  ).convert_dtypes()
                  # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                  namespace = locals()
                  exec(transform_code, namespace)
                  namespace['df'].to_csv(
                      transformed_table_path,
                      index=False,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --transform-code
            - {inputValue: transform_code}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml
      digest: ca3834a0b29b0647f0e48f9d45419bb7ae05c6f4cd6fc98f4d99a28979915284
      text: |
        name: Split table into folds
        description: |-
          Splits the data table into the specified number of folds.

              The data is split into the specified number of folds k (default: 5).
              Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
              Each training subsample has (k-1)/k fraction of samples.
              The train_i subsample is produced by excluding test_i subsample form all samples.

              Inputs:
                  table: The data to split by rows
                  number_of_folds: Number of folds to split data into
                  random_seed: Random seed for reproducible splitting

              Outputs:
                  train_i: The i-th training subsample
                  test_i: The i-th testing subsample

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'
        inputs:
        - {name: table, type: CSV}
        - {name: number_of_folds, type: Integer, default: '5', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: train_1, type: CSV}
        - {name: train_2, type: CSV}
        - {name: train_3, type: CSV}
        - {name: train_4, type: CSV}
        - {name: train_5, type: CSV}
        - {name: test_1, type: CSV}
        - {name: test_2, type: CSV}
        - {name: test_3, type: CSV}
        - {name: test_4, type: CSV}
        - {name: test_5, type: CSV}
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def split_table_into_folds(
                  table_path,

                  train_1_path,
                  train_2_path,
                  train_3_path,
                  train_4_path,
                  train_5_path,

                  test_1_path,
                  test_2_path,
                  test_3_path,
                  test_4_path,
                  test_5_path,

                  number_of_folds = 5,
                  random_seed = 0,
              ):
                  """Splits the data table into the specified number of folds.

                  The data is split into the specified number of folds k (default: 5).
                  Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
                  Each training subsample has (k-1)/k fraction of samples.
                  The train_i subsample is produced by excluding test_i subsample form all samples.

                  Inputs:
                      table: The data to split by rows
                      number_of_folds: Number of folds to split data into
                      random_seed: Random seed for reproducible splitting

                  Outputs:
                      train_i: The i-th training subsample
                      test_i: The i-th testing subsample

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>

                  """
                  import pandas
                  from sklearn import model_selection

                  max_number_of_folds = 5

                  if number_of_folds < 1 or number_of_folds > max_number_of_folds:
                      raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )
                  splitter = model_selection.KFold(
                      n_splits=number_of_folds,
                      shuffle=True,
                      random_state=random_seed,
                  )
                  folds = list(splitter.split(df))

                  fold_paths = [
                      (train_1_path, test_1_path),
                      (train_2_path, test_2_path),
                      (train_3_path, test_3_path),
                      (train_4_path, test_4_path),
                      (train_5_path, test_5_path),
                  ]

                  for i in range(max_number_of_folds):
                      (train_path, test_path) = fold_paths[i]
                      if i < len(folds):
                          (train_indices, test_indices) = folds[i]
                          train_fold = df.iloc[train_indices]
                          test_fold = df.iloc[test_indices]
                      else:
                          train_fold = df.iloc[0:0]
                          test_fold = df.iloc[0:0]
                      train_fold.to_csv(train_path, index=False)
                      test_fold.to_csv(test_path, index=False)

              import argparse
              _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-folds", dest="number_of_folds", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--train-1", dest="train_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-2", dest="train_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-3", dest="train_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-4", dest="train_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-5", dest="train_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-1", dest="test_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-2", dest="test_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-3", dest="test_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-4", dest="test_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-5", dest="test_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = split_table_into_folds(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: number_of_folds}
                then:
                - --number-of-folds
                - {inputValue: number_of_folds}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --train-1
            - {outputPath: train_1}
            - --train-2
            - {outputPath: train_2}
            - --train-3
            - {outputPath: train_3}
            - --train-4
            - {outputPath: train_4}
            - --train-5
            - {outputPath: train_5}
            - --test-1
            - {outputPath: test_1}
            - --test-2
            - {outputPath: test_2}
            - --test-3
            - {outputPath: test_3}
            - --test-4
            - {outputPath: test_4}
            - --test-5
            - {outputPath: test_5}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml
      digest: 1d1b505ec8a538347e8c0db84a92f76ebae55c5ef85ff3e91bc1d7eb7a86dc06
      text: |
        name: Split rows into subsets
        description: Splits the data table according to the split fractions.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: fraction_1, type: Float, description: 'The proportion of the lines to put
            into the 1st split. Range: [0, 1]'}
        - name: fraction_2
          type: Float
          description: |-
            The proportion of the lines to put into the 2nd split. Range: [0, 1]
            If fraction_2 is not specified, then fraction_2 = 1 - fraction_1.
            The remaining lines go to the 3rd split (if any).
          optional: true
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: split_1, type: CSV, description: Subset of the data table.}
        - {name: split_2, type: CSV, description: Subset of the data table.}
        - {name: split_3, type: CSV, description: Subset of the data table.}
        - {name: split_1_count, type: Integer}
        - {name: split_2_count, type: Integer}
        - {name: split_3_count, type: Integer}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def split_rows_into_subsets(
                  table_path,
                  split_1_path,
                  split_2_path,
                  split_3_path,
                  fraction_1,
                  fraction_2 = None,
                  random_seed = 0,
              ):
                  """Splits the data table according to the split fractions.

                  Args:
                      table_path: Input data table.
                      split_1_path: Subset of the data table.
                      split_2_path: Subset of the data table.
                      split_3_path: Subset of the data table.
                      fraction_1: The proportion of the lines to put into the 1st split. Range: [0, 1]
                      fraction_2: The proportion of the lines to put into the 2nd split. Range: [0, 1]
                          If fraction_2 is not specified, then fraction_2 = 1 - fraction_1.
                          The remaining lines go to the 3rd split (if any).
                      random_seed: Controls the seed of the random processes.
                  """
                  import random

                  random.seed(random_seed)

                  SHUFFLE_BUFFER_SIZE = 10000

                  num_splits = 3

                  if fraction_1 < 0 or fraction_1 > 1:
                      raise ValueError("fraction_1 must be in between 0 and 1.")

                  if fraction_2 is None:
                      fraction_2 = 1 - fraction_1
                  if fraction_2 < 0 or fraction_2 > 1:
                      raise ValueError("fraction_2 must be in between 0 and 1.")

                  fraction_3 = 1 - fraction_1 - fraction_2

                  fractions = [
                      fraction_1,
                      fraction_2,
                      fraction_3,
                  ]

                  assert sum(fractions) == 1

                  written_line_counts = [0] * num_splits

                  output_files = [
                      open(split_1_path, "wb"),
                      open(split_2_path, "wb"),
                      open(split_3_path, "wb"),
                  ]

                  with open(table_path, "rb") as input_file:
                      # Writing the headers
                      header_line = input_file.readline()
                      for output_file in output_files:
                          output_file.write(header_line)

                      while True:
                          line_buffer = []
                          for i in range(SHUFFLE_BUFFER_SIZE):
                              line = input_file.readline()
                              if not line:
                                  break
                              line_buffer.append(line)

                          # We need to exactly partition the lines between the output files
                          # To overcome possible systematic bias, we could calculate the total numbers
                          # of lines written to each file and take that into account.
                          num_read_lines = len(line_buffer)
                          number_of_lines_for_files = [0] * num_splits
                          # List that will have the index of the destination file for each line
                          file_index_for_line = []
                          remaining_lines = num_read_lines
                          remaining_fraction = 1
                          for i in range(num_splits):
                              number_of_lines_for_file = (
                                  round(remaining_lines * (fractions[i] / remaining_fraction))
                                  if remaining_fraction > 0
                                  else 0
                              )
                              number_of_lines_for_files[i] = number_of_lines_for_file
                              remaining_lines -= number_of_lines_for_file
                              remaining_fraction -= fractions[i]
                              file_index_for_line.extend([i] * number_of_lines_for_file)

                          assert remaining_lines == 0, f"{remaining_lines}"
                          assert len(file_index_for_line) == num_read_lines

                          random.shuffle(file_index_for_line)

                          for i in range(num_read_lines):
                              output_files[file_index_for_line[i]].write(line_buffer[i])
                              written_line_counts[file_index_for_line[i]] += 1

                          # Exit if the file ended before we were able to fully fill the buffer
                          if len(line_buffer) != SHUFFLE_BUFFER_SIZE:
                              break

                  for output_file in output_files:
                      output_file.close()

                  return written_line_counts

              def _serialize_int(int_value: int) -> str:
                  if isinstance(int_value, str):
                      return int_value
                  if not isinstance(int_value, int):
                      raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                  return str(int_value)

              import argparse
              _parser = argparse.ArgumentParser(prog='Split rows into subsets', description='Splits the data table according to the split fractions.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--fraction-1", dest="fraction_1", type=float, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--fraction-2", dest="fraction_2", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--split-1", dest="split_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--split-2", dest="split_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--split-3", dest="split_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = split_rows_into_subsets(**_parsed_args)

              _output_serializers = [
                  _serialize_int,
                  _serialize_int,
                  _serialize_int,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - --table
            - {inputPath: table}
            - --fraction-1
            - {inputValue: fraction_1}
            - if:
                cond: {isPresent: fraction_2}
                then:
                - --fraction-2
                - {inputValue: fraction_2}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --split-1
            - {outputPath: split_1}
            - --split-2
            - {outputPath: split_2}
            - --split-3
            - {outputPath: split_3}
            - '----output-paths'
            - {outputPath: split_1_count}
            - {outputPath: split_2_count}
            - {outputPath: split_3_count}
  - name: JSON lines
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_JsonLines_format/component.yaml
      digest: a16b7f91eaf2eecf351c2775d4d33c960457257a90025ac058313946a3c7fc44
      text: |
        name: Transform using Pandas DataFrame on JsonLines data
        description: Transform DataFrame loaded from an ApacheParquet file.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_JsonLines_format/component.yaml'}
        inputs:
        - {name: table, type: JsonLines, description: DataFrame to transform.}
        - name: transform_code
          type: PythonCode
          description: |-
            Transformation code. Code is written in Python and can consist of multiple lines.
            The DataFrame variable is called "df".
            Examples:
            - `df['prod'] = df['X'] * df['Y']`
            - `df = df[['X', 'prod']]`
            - `df.insert(0, "is_positive", df["X"] > 0)`
        outputs:
        - {name: transformed_table, type: JsonLines, description: Transformed DataFrame.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def transform_using_Pandas_DataFrame_on_JsonLines_data(
                  table_path,
                  transformed_table_path,
                  transform_code,
              ):
                  """Transform DataFrame loaded from an ApacheParquet file.

                  Args:
                      table_path: DataFrame to transform.
                      transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                          The DataFrame variable is called "df".
                          Examples:
                          - `df['prod'] = df['X'] * df['Y']`
                          - `df = df[['X', 'prod']]`
                          - `df.insert(0, "is_positive", df["X"] > 0)`
                      transformed_table_path: Transformed DataFrame.
                  """
                  import pandas

                  df = pandas.read_json(path_or_buf=table_path, lines=True)
                  # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                  namespace = dict(df=df)
                  exec(transform_code, namespace)
                  df = namespace["df"]
                  df.to_json(path_or_buf=transformed_table_path, orient="records", lines=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Transform using Pandas DataFrame on JsonLines data', description='Transform DataFrame loaded from an ApacheParquet file.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = transform_using_Pandas_DataFrame_on_JsonLines_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --transform-code
            - {inputValue: transform_code}
            - --transformed-table
            - {outputPath: transformed_table}
  - name: JSON
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_index/component.yaml
      digest: c7b09bd3bf9cf9e42e22bedcc4822c32c31f81f92e45c019f9cca626961cf2a2
      text: |
        name: Get element by index from JSON
        inputs:
        - {name: Json}
        - {name: Index, type: Integer}
        outputs:
        - {name: Output}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_index/component.yaml'
        implementation:
          container:
            image: stedolan/jq:latest
            command:
            - sh
            - -exc
            - |
              input_path=$0
              output_path=$1
              index=$2
              mkdir -p "$(dirname "$output_path")"
              < "$input_path" jq --raw-output --join-output .["$index"] > "$output_path"
            - {inputPath: Json}
            - {outputPath: Output}
            - {inputValue: Index}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_key/component.yaml
      digest: 2790d7a0b9983e3b9ddca1fb02b243a7b3e0280198cd859438d622ad1ca8d3b0
      text: |
        name: Get element by key from JSON
        inputs:
        - {name: Json}
        - {name: Key, type: String}
        outputs:
        - {name: Output}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_key/component.yaml'
        implementation:
          container:
            image: stedolan/jq:latest
            command:
            - sh
            - -exc
            - |
              input_path=$0
              output_path=$1
              key=$2
              mkdir -p "$(dirname "$output_path")"
              < "$input_path" jq --raw-output --join-output '.["'"$key"'"]' > "$output_path"
            - {inputPath: Json}
            - {outputPath: Output}
            - {inputValue: Key}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Query/component.yaml
      digest: 879a8ddf73f5bec05643901f88b024f82c83cf1443536b98a624331a89d8a9f5
      text: |
        name: Query JSON using JQ
        inputs:
        - {name: Json}
        - {name: Query, type: String}
        - {name: Options, type: String, default: '--raw-output'}
        outputs:
        - {name: Output}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Query/component.yaml'
        implementation:
          container:
            image: stedolan/jq:latest
            command:
            - sh
            - -exc
            - |
              input_path=$0
              output_path=$1
              query=$2
              options=$3
              mkdir -p "$(dirname "$output_path")"
              < "$input_path" jq $options "$query" > "$output_path"
            - {inputPath: Json}
            - {outputPath: Output}
            - {inputValue: Query}
            - {inputValue: Options}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_dict/component.yaml
      digest: a3d12264c5d5cd52900d2a8d16c96630954de1762c0000103c999a0d48396a7a
      text: |
        name: Build dict
        description: Creates a JSON object from multiple key and value pairs.
        inputs:
        - {name: key_1, type: String, optional: true}
        - {name: value_1, type: JsonObject, optional: true}
        - {name: key_2, type: String, optional: true}
        - {name: value_2, type: JsonObject, optional: true}
        - {name: key_3, type: String, optional: true}
        - {name: value_3, type: JsonObject, optional: true}
        - {name: key_4, type: String, optional: true}
        - {name: value_4, type: JsonObject, optional: true}
        - {name: key_5, type: String, optional: true}
        - {name: value_5, type: JsonObject, optional: true}
        outputs:
        - {name: Output, type: JsonObject}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_dict/component.yaml'
        implementation:
          container:
            image: python:3.8
            command:
            - python3
            - -u
            - -c
            - |
              def build_dict(
                  key_1 = None,
                  value_1 = None,
                  key_2 = None,
                  value_2 = None,
                  key_3 = None,
                  value_3 = None,
                  key_4 = None,
                  value_4 = None,
                  key_5 = None,
                  value_5 = None,
              ):
                  """Creates a JSON object from multiple key and value pairs.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = dict([
                      (key_1, value_1),
                      (key_2, value_2),
                      (key_3, value_3),
                      (key_4, value_4),
                      (key_5, value_5),
                  ])
                  if None in result:
                      del result[None]
                  return result

              import json
              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build dict', description='Creates a JSON object from multiple key and value pairs.')
              _parser.add_argument("--key-1", dest="key_1", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-1", dest="value_1", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-2", dest="key_2", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-2", dest="value_2", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-3", dest="key_3", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-3", dest="value_3", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-4", dest="key_4", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-4", dest="value_4", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-5", dest="key_5", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-5", dest="value_5", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_dict(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: key_1}
                then:
                - --key-1
                - {inputValue: key_1}
            - if:
                cond: {isPresent: value_1}
                then:
                - --value-1
                - {inputValue: value_1}
            - if:
                cond: {isPresent: key_2}
                then:
                - --key-2
                - {inputValue: key_2}
            - if:
                cond: {isPresent: value_2}
                then:
                - --value-2
                - {inputValue: value_2}
            - if:
                cond: {isPresent: key_3}
                then:
                - --key-3
                - {inputValue: key_3}
            - if:
                cond: {isPresent: value_3}
                then:
                - --value-3
                - {inputValue: value_3}
            - if:
                cond: {isPresent: key_4}
                then:
                - --key-4
                - {inputValue: key_4}
            - if:
                cond: {isPresent: value_4}
                then:
                - --value-4
                - {inputValue: value_4}
            - if:
                cond: {isPresent: key_5}
                then:
                - --key-5
                - {inputValue: key_5}
            - if:
                cond: {isPresent: value_5}
                then:
                - --value-5
                - {inputValue: value_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_list/component.yaml
      digest: 24b33123e1cb13712aec208e81e6c1dc2d6d643c8a1fe323581e8858c3c8993a
      text: |
        name: Build list
        description: Creates a JSON array from multiple items.
        inputs:
        - {name: item_1, type: JsonObject, optional: true}
        - {name: item_2, type: JsonObject, optional: true}
        - {name: item_3, type: JsonObject, optional: true}
        - {name: item_4, type: JsonObject, optional: true}
        - {name: item_5, type: JsonObject, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list/component.yaml'
        implementation:
          container:
            image: python:3.8
            command:
            - python3
            - -u
            - -c
            - |
              def build_list(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple items.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              import json
              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list', description='Creates a JSON array from multiple items.')
              _parser.add_argument("--item-1", dest="item_1", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/aecac18d4023c73c561d7f21192253e9593b9932/components/json/Build_list_of_strings/component.yaml
      digest: 76963afe607da688f1334e8cff2017da891f92609855d6103ccbab627c6eb2ef
      text: |
        name: Build list of strings
        description: Creates a JSON array from multiple strings.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_strings/component.yaml'}
        inputs:
        - {name: item_1, type: String, optional: true}
        - {name: item_2, type: String, optional: true}
        - {name: item_3, type: String, optional: true}
        - {name: item_4, type: String, optional: true}
        - {name: item_5, type: String, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def build_list_of_strings(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple strings.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list of strings', description='Creates a JSON array from multiple strings.')
              _parser.add_argument("--item-1", dest="item_1", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list_of_strings(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_integers/component.yaml
      digest: 965f5a4f2f0a34f07419ab5d60cc3c3505ab2d7579baa0dd36d9a32ea97a7ffa
      text: |
        name: Build list of integers
        description: Creates a JSON array from multiple integer numbers.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_integers/component.yaml'}
        inputs:
        - {name: item_1, type: Integer, optional: true}
        - {name: item_2, type: Integer, optional: true}
        - {name: item_3, type: Integer, optional: true}
        - {name: item_4, type: Integer, optional: true}
        - {name: item_5, type: Integer, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        implementation:
          container:
            image: python:3.8
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def build_list_of_integers(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple integer numbers.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list of integers', description='Creates a JSON array from multiple integer numbers.')
              _parser.add_argument("--item-1", dest="item_1", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list_of_integers(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_floats/component.yaml
      digest: b3ad1eeb79066b286809fe4a1b078aee08bbadd07d977b5327880e854bec1ff2
      text: |
        name: Build list of floats
        description: Creates a JSON array from multiple floating-point numbers.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_floats/component.yaml'}
        inputs:
        - {name: item_1, type: Float, optional: true}
        - {name: item_2, type: Float, optional: true}
        - {name: item_3, type: Float, optional: true}
        - {name: item_4, type: Float, optional: true}
        - {name: item_5, type: Float, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        implementation:
          container:
            image: python:3.8
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def build_list_of_floats(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple floating-point numbers.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list of floats', description='Creates a JSON array from multiple floating-point numbers.')
              _parser.add_argument("--item-1", dest="item_1", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list_of_floats(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Combine_lists/component.yaml
      digest: f2b024570631dd504413a7836297d02067423ff9d64b62ebc76be1a943488f4a
      text: |
        name: Combine lists
        description: Combines multiple JSON arrays into one.
        inputs:
        - {name: list_1, type: JsonArray, optional: true}
        - {name: list_2, type: JsonArray, optional: true}
        - {name: list_3, type: JsonArray, optional: true}
        - {name: list_4, type: JsonArray, optional: true}
        - {name: list_5, type: JsonArray, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Combine_lists/component.yaml'
        implementation:
          container:
            image: python:3.8
            command:
            - python3
            - -u
            - -c
            - |
              def combine_lists(
                  list_1 = None,
                  list_2 = None,
                  list_3 = None,
                  list_4 = None,
                  list_5 = None,
              ):
                  """Combines multiple JSON arrays into one.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for list in [list_1, list_2, list_3, list_4, list_5]:
                      if list is not None:
                          result.extend(list)
                  return result

              import json
              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Combine lists', description='Combines multiple JSON arrays into one.')
              _parser.add_argument("--list-1", dest="list_1", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-2", dest="list_2", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-3", dest="list_3", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-4", dest="list_4", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-5", dest="list_5", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = combine_lists(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: list_1}
                then:
                - --list-1
                - {inputValue: list_1}
            - if:
                cond: {isPresent: list_2}
                then:
                - --list-2
                - {inputValue: list_2}
            - if:
                cond: {isPresent: list_3}
                then:
                - --list-3
                - {inputValue: list_3}
            - if:
                cond: {isPresent: list_4}
                then:
                - --list-4
                - {inputValue: list_4}
            - if:
                cond: {isPresent: list_5}
                then:
                - --list-5
                - {inputValue: list_5}
            - '----output-paths'
            - {outputPath: Output}
- name: Upload/Download
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/f328b3f4842838b6578b701d975b22f7908aca48/components/web/Download/component.yaml
    digest: 44c3cf1caa233e1d9057a25181ce622359643fc9345140b84556b00c8042ffb4
    text: |
      name: Download data
      inputs:
      - {name: Url, type: URI}
      - {name: curl options, type: string, default: '--location', description: 'Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html'}
      outputs:
      - {name: Data}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml'
      implementation:
        container:
          # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
          image: alpine/curl:8.14.1
          command:
          - sh
          - -exc
          - |
            url="$0"
            output_path="$1"
            curl_options="$2"

            mkdir -p "$(dirname "$output_path")"
            curl --get "$url" --output "$output_path" $curl_options
          - inputValue: Url
          - outputPath: Data
          - inputValue: curl options
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml
    digest: 30c424ac6156c478aa0c3027b470baf9cb7dbbf90aebcabde7469bfbd02a512e
    text: |
      name: Download from GCS
      inputs:
      - {name: GCS path, type: URI}
      outputs:
      - {name: Data}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download/component.yaml'
      implementation:
          container:
              image: google/cloud-sdk
              command:
              - bash # Pattern comparison only works in Bash
              - -ex
              - -c
              - |
                  if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                      gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                  fi

                  uri="$0"
                  output_path="$1"

                  # Checking whether the URI points to a single blob, a directory or a URI pattern
                  # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI
                  if [[ "$uri" != */ ]] && (gsutil ls "$uri" | grep --fixed-strings --line-regexp "$uri"); then
                      mkdir -p "$(dirname "$output_path")"
                      gsutil -m cp -r "$uri" "$output_path"
                  else
                      mkdir -p "$output_path" # When source path is a directory, gsutil requires the destination to also be a directory
                      gsutil -m rsync -r "$uri" "$output_path" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.
                  fi
              - inputValue: GCS path
              - outputPath: Data
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml
    digest: 074b5c68679117f5aeb41e36ee75b8c349ec88631ba552cbf6991bd5942d9192
    text: |-
      name: Upload to GCS with unique name
      description: Upload to GCS with unique URI suffix
      inputs:
      - {name: Data}
      - {name: GCS path prefix, type: URI}
      outputs:
      - {name: GCS path, type: String}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_unique_uri/component.yaml'
      implementation:
          container:
              image: google/cloud-sdk
              command:
              - sh
              - -ex
              - -c
              - |
                  data_path="$0"
                  url_prefix="$1"
                  output_path="$2"
                  random_string=$(< dev/urandom tr -dc A-Za-z0-9 | head -c 64)
                  uri="${url_prefix}${random_string}"
                  if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                      gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                  fi
                  gsutil cp -r "$data_path" "$uri"
                  mkdir -p "$(dirname "$output_path")"
                  printf "%s" "$uri" > "$output_path"
              - inputPath: Data
              - {inputValue: GCS path prefix}
              - outputPath: GCS path
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml
    digest: e3cfa607ab9e2e0312ef4268dad157edf30c3d4ba5059b2e295fe047258aa31d
    text: |
      name: Upload to GCS
      inputs:
      - {name: Data}
      - {name: GCS path, type: URI}
      outputs:
      - {name: GCS path, type: String}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'
      implementation:
          container:
              image: google/cloud-sdk
              command:
              - sh
              - -ex
              - -c
              - |
                  if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                      gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                  fi
                  gsutil cp -r "$0" "$1"
                  mkdir -p "$(dirname "$2")"
                  printf "%s" "$1" > "$2"
              - inputPath: Data
              - inputValue: GCS path
              - outputPath: GCS path
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/cca2d8569d01b527df10c629258be04d52eacc43/components/Download_and_upload/IPFS/Download/component.yaml
    digest: 6fedde78a50fcece87df20eeba865d6f4a2c2513d514ff1386efe2e0d1445421
    text: |
      name: Download data from IPFS
      description: |
        Download data from IPFS using ipget.
        See https://ipfs.io/
        See https://github.com/ipfs/ipget
      inputs:
      - {name: Path, type: String, description: "IPFS Path. Example: /ipfs/QmWATWQ7fVPP2EFGu71UkfnqhYXDYH566qy47CnJDgvs8u"}
      outputs:
      - {name: Data}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/Download_and_upload/IPFS/Download/component.yaml"
      implementation:
        container:
          image: alpine
          command:
          - sh
          - -exc
          - |
            path=$0
            output_data_path=$1
            mkdir -p "$(dirname "$output_data_path")"

            wget https://dist.ipfs.io/ipget/v0.7.0/ipget_v0.7.0_linux-amd64.tar.gz
            tar -xzf ipget_v0.7.0_linux-amd64.tar.gz
            cd ipget

            # Fixing IPFS on Alpine issue:
            # https://discuss.ipfs.io/t/why-go-ipfs-cant-run-in-alphine-linux/6625
            mkdir /lib64 && ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2

            ./ipget --output "$output_data_path" "$path"
          - {inputValue: Path}
          - {outputPath: Data}
- name: ML frameworks
  folders:
  - name: Scikit learn
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml
      digest: 57ecdf4c3d54c5e6a032ba60667fea7af140d3568552bf5ea090510af5821a90
      text: |
        name: Train linear regression model using scikit learn from CSV
        description: Trains linear regression model using Scikit-learn.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml'}
        inputs:
        - {name: dataset, type: CSV, description: Tabular dataset for training.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label.}
        outputs:
        - {name: model, type: ScikitLearnPickleModel, description: Trained model in Scikit-learn
            pickle format.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_linear_regression_model_using_scikit_learn_from_CSV(
                  dataset_path,
                  model_path,
                  label_column_name,
              ):
                  """Trains linear regression model using Scikit-learn.

                  See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression

                  Args:
                      dataset_path: Tabular dataset for training.
                      model_path: Trained model in Scikit-learn pickle format.
                      label_column_name: Name of the table column to use as label.
                  """
                  import pandas
                  import pickle
                  from sklearn import linear_model

                  df = pandas.read_csv(dataset_path)
                  model = linear_model.LinearRegression()
                  model.fit(
                      X=df.drop(columns=label_column_name),
                      y=df[label_column_name],
                  )

                  with open(model_path, "wb") as f:
                      pickle.dump(model, f)

              import argparse
              _parser = argparse.ArgumentParser(prog='Train linear regression model using scikit learn from CSV', description='Trains linear regression model using Scikit-learn.')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_linear_regression_model_using_scikit_learn_from_CSV(**_parsed_args)
            args:
            - --dataset
            - {inputPath: dataset}
            - --label-column-name
            - {inputValue: label_column_name}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml
      digest: 25410cf204a3a5d4274906541d5f0140c2257b719c0b98e07ab6e85b066e84e4
      text: |
        name: Train logistic regression model using scikit learn from CSV
        description: Trains logistic regression model using Scikit-learn.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml'}
        inputs:
        - {name: dataset, type: CSV, description: Tabular dataset for training.}
        - {name: label_column_name, type: String, description: Name of the data column to
            use as label.}
        - name: penalty
          type: String
          description: |-
            Used to specify the norm used in the penalization.
            Possible values: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'
            The 'newton-cg',
            'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is
            only supported by the 'saga' solver. If 'none' (not supported by the
            liblinear solver), no regularization is applied.
          default: l2
          optional: true
        - name: solver
          type: String
          description: |-
            Algorithm to use in the optimization problem.
            Possible values: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'

            - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
            'saga' are faster for large ones.
            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
            handle multinomial loss; 'liblinear' is limited to one-versus-rest
            schemes.
            - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty
            - 'liblinear' and 'saga' also handle L1 penalty
            - 'saga' also supports 'elasticnet' penalty
            - 'liblinear' does not support setting ``penalty='none'``

            Note that 'sag' and 'saga' fast convergence is only guaranteed on
            features with approximately the same scale. You can
            preprocess the data with a scaler from sklearn.preprocessing.
          default: lbfgs
          optional: true
        - {name: max_iterations, type: Integer, description: Maximum number of iterations
            taken for the solvers to converge., default: '100', optional: true}
        - name: multi_class_mode
          type: String
          description: |-
            Possible values: {'auto', 'ovr', 'multinomial'}, default='auto'
            If the option chosen is 'ovr', then a binary problem is fit for each
            label. For 'multinomial' the loss minimised is the multinomial loss fit
            across the entire probability distribution, *even when the data is
            binary*. 'multinomial' is unavailable when solver='liblinear'.
            'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
            and otherwise selects 'multinomial'.
          default: auto
          optional: true
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: model, type: ScikitLearnPickleModel, description: Trained model in Scikit-learn
            pickle format.}
        - {name: model_parameters, type: JsonObject}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_logistic_regression_model_using_scikit_learn_from_CSV(
                  dataset_path,
                  model_path,
                  label_column_name,
                  penalty = "l2", # l1, l2, elasticnet, none
                  solver = "lbfgs", # newton-cg, lbfgs, liblinear, sag, saga
                  max_iterations = 100,
                  multi_class_mode = "auto", # auto, ovr, multinomial
                  random_seed = 0,
              ):
                  """Trains logistic regression model using Scikit-learn.

                  See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

                  Args:
                      dataset_path: Tabular dataset for training.
                      model_path: Trained model in Scikit-learn pickle format.
                      label_column_name: Name of the data column to use as label.
                      penalty: Used to specify the norm used in the penalization.
                          Possible values: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'
                          The 'newton-cg',
                          'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is
                          only supported by the 'saga' solver. If 'none' (not supported by the
                          liblinear solver), no regularization is applied.
                      solver: Algorithm to use in the optimization problem.
                          Possible values: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'

                          - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
                          'saga' are faster for large ones.
                          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
                          handle multinomial loss; 'liblinear' is limited to one-versus-rest
                          schemes.
                          - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty
                          - 'liblinear' and 'saga' also handle L1 penalty
                          - 'saga' also supports 'elasticnet' penalty
                          - 'liblinear' does not support setting ``penalty='none'``

                          Note that 'sag' and 'saga' fast convergence is only guaranteed on
                          features with approximately the same scale. You can
                          preprocess the data with a scaler from sklearn.preprocessing.
                      max_iterations: Maximum number of iterations taken for the solvers to converge.
                      multi_class_mode: Possible values: {'auto', 'ovr', 'multinomial'}, default='auto'
                          If the option chosen is 'ovr', then a binary problem is fit for each
                          label. For 'multinomial' the loss minimised is the multinomial loss fit
                          across the entire probability distribution, *even when the data is
                          binary*. 'multinomial' is unavailable when solver='liblinear'.
                          'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
                          and otherwise selects 'multinomial'.
                      random_seed: Controls the seed of the random processes.
                  """
                  import json
                  import pandas
                  import pickle
                  from sklearn import linear_model

                  df = pandas.read_csv(dataset_path)
                  model = linear_model.LogisticRegression(
                      penalty=penalty,
                      #dual=False,
                      #tol=1e-4,
                      #C=1.0,
                      #fit_intercept=True,
                      #intercept_scaling=1,
                      #class_weight=None,
                      random_state=random_seed,
                      solver=solver,
                      max_iter=max_iterations,
                      multi_class=multi_class_mode,
                      #l1_ratio=None,
                      verbose=1,
                  )

                  model_parameters = model.get_params()
                  model_parameters_json = json.dumps(model_parameters, indent=2)
                  print("Model parameters:")
                  print(model_parameters_json)
                  print()

                  model.fit(
                      X=df.drop(columns=label_column_name),
                      y=df[label_column_name],
                  )

                  with open(model_path, "wb") as f:
                      pickle.dump(model, f)

                  return (model_parameters_json,)

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Train logistic regression model using scikit learn from CSV', description='Trains logistic regression model using Scikit-learn.')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--penalty", dest="penalty", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--solver", dest="solver", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--max-iterations", dest="max_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--multi-class-mode", dest="multi_class_mode", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = train_logistic_regression_model_using_scikit_learn_from_CSV(**_parsed_args)

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - --dataset
            - {inputPath: dataset}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: penalty}
                then:
                - --penalty
                - {inputValue: penalty}
            - if:
                cond: {isPresent: solver}
                then:
                - --solver
                - {inputValue: solver}
            - if:
                cond: {isPresent: max_iterations}
                then:
                - --max-iterations
                - {inputValue: max_iterations}
            - if:
                cond: {isPresent: multi_class_mode}
                then:
                - --multi-class-mode
                - {inputValue: multi_class_mode}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --model
            - {outputPath: model}
            - '----output-paths'
            - {outputPath: model_parameters}
  - name: XGBoost
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/component.yaml
      digest: 5b8bec6716337d7bd8ecafd6dd46bc44527783bc05950cfe03a60206b43c7654
      text: |
        name: Train XGBoost model on CSV
        description: Trains an XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'}
        inputs:
        - {name: training_data, type: CSV, description: Training data in CSV format.}
        - {name: label_column_name, type: String, description: Name of the column containing
            the label data.}
        - {name: starting_model, type: XGBoostModel, description: Existing trained model to
            start from (in the binary XGBoost format)., optional: true}
        - {name: num_iterations, type: Integer, description: Number of boosting iterations.,
          default: '10', optional: true}
        - name: objective
          type: String
          description: |-
            The learning task and the corresponding learning objective.
            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
            The most common values are:
            "reg:squarederror" - Regression with squared loss (default).
            "reg:logistic" - Logistic regression.
            "binary:logistic" - Logistic regression for binary classification, output probability.
            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
          default: reg:squarederror
          optional: true
        - {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,
            `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`
            uses linear functions.', default: gbtree, optional: true}
        - {name: learning_rate, type: Float, description: 'Step size shrinkage used in update
            to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}
        - name: min_split_loss
          type: Float
          description: |-
            Minimum loss reduction required to make a further partition on a leaf node of the tree.
            The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
          default: '0'
          optional: true
        - name: max_depth
          type: Integer
          description: |-
            Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
            0 indicates no limit on depth. Range: [0,Inf].
          default: '6'
          optional: true
        - {name: booster_params, type: JsonObject, description: 'Parameters for the booster.
            See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}
        outputs:
        - {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost
            format.}
        - {name: model_config, type: XGBoostModelConfig, description: The internal parameter
            configuration of Booster as a JSON string.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_XGBoost_model_on_CSV(
                  training_data_path,
                  model_path,
                  model_config_path,
                  label_column_name,
                  starting_model_path = None,
                  num_iterations = 10,
                  # Booster parameters
                  objective = "reg:squarederror",
                  booster = "gbtree",
                  learning_rate = 0.3,
                  min_split_loss = 0,
                  max_depth = 6,
                  booster_params = None,
              ):
                  """Trains an XGBoost model.

                  Args:
                      training_data_path: Training data in CSV format.
                      model_path: Trained model in the binary XGBoost format.
                      model_config_path: The internal parameter configuration of Booster as a JSON string.
                      starting_model_path: Existing trained model to start from (in the binary XGBoost format).
                      label_column_name: Name of the column containing the label data.
                      num_iterations: Number of boosting iterations.
                      booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                      objective: The learning task and the corresponding learning objective.
                          See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                          The most common values are:
                          "reg:squarederror" - Regression with squared loss (default).
                          "reg:logistic" - Logistic regression.
                          "binary:logistic" - Logistic regression for binary classification, output probability.
                          "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                          "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                          "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
                      booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.
                      learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].
                      min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.
                          The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
                      max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                          0 indicates no limit on depth. Range: [0,Inf].

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  import pandas
                  import xgboost

                  df = pandas.read_csv(
                      training_data_path,
                  ).convert_dtypes()
                  print("Training data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print()
                  print("Final training data information:")
                  df.info(verbose=True)

                  training_data = xgboost.DMatrix(
                      data=df.drop(columns=[label_column_name]),
                      label=df[[label_column_name]],
                      enable_categorical=True,
                  )

                  booster_params = booster_params or {}
                  booster_params.setdefault("objective", objective)
                  booster_params.setdefault("booster", booster)
                  booster_params.setdefault("learning_rate", learning_rate)
                  booster_params.setdefault("min_split_loss", min_split_loss)
                  booster_params.setdefault("max_depth", max_depth)

                  starting_model = None
                  if starting_model_path:
                      starting_model = xgboost.Booster(model_file=starting_model_path)

                  print()
                  print("Training the model:")
                  model = xgboost.train(
                      params=booster_params,
                      dtrain=training_data,
                      num_boost_round=num_iterations,
                      xgb_model=starting_model,
                      evals=[(training_data, "training_data")],
                  )

                  # Saving the model in binary format
                  model.save_model(model_path)

                  model_config_str = model.save_config()
                  with open(model_config_path, "w") as model_config_file:
                      model_config_file.write(model_config_str)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train XGBoost model on CSV', description='Trains an XGBoost model.')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_XGBoost_model_on_CSV(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: objective}
                then:
                - --objective
                - {inputValue: objective}
            - if:
                cond: {isPresent: booster}
                then:
                - --booster
                - {inputValue: booster}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: min_split_loss}
                then:
                - --min-split-loss
                - {inputValue: min_split_loss}
            - if:
                cond: {isPresent: max_depth}
                then:
                - --max-depth
                - {inputValue: max_depth}
            - if:
                cond: {isPresent: booster_params}
                then:
                - --booster-params
                - {inputValue: booster_params}
            - --model
            - {outputPath: model}
            - --model-config
            - {outputPath: model_config}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/component.yaml
      digest: 6fd7196d2061e6f49ec98459c90f4b1e4e63bca21170c70b23f7679921ce01d7
      text: |
        name: Xgboost predict on CSV
        description: Makes predictions using a trained XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'}
        inputs:
        - {name: data, type: CSV, description: Feature data in Apache Parquet format.}
        - {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}
        - {name: label_column_name, type: String, description: Optional. Name of the column
            containing the label data that is excluded during the prediction., optional: true}
        outputs:
        - {name: predictions, description: Model predictions.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def xgboost_predict_on_CSV(
                  data_path,
                  model_path,
                  predictions_path,
                  label_column_name = None,
              ):
                  """Makes predictions using a trained XGBoost model.

                  Args:
                      data_path: Feature data in Apache Parquet format.
                      model_path: Trained model in binary XGBoost format.
                      predictions_path: Model predictions.
                      label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  from pathlib import Path

                  import numpy
                  import pandas
                  import xgboost

                  df = pandas.read_csv(
                      data_path,
                  ).convert_dtypes()
                  print("Evaluation data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print("Final evaluation data information:")
                  df.info(verbose=True)

                  if label_column_name is not None:
                      df = df.drop(columns=[label_column_name])

                  testing_data = xgboost.DMatrix(
                      data=df,
                      enable_categorical=True,
                  )

                  model = xgboost.Booster(model_file=model_path)

                  predictions = model.predict(testing_data)

                  Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Xgboost predict on CSV', description='Makes predictions using a trained XGBoost model.')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = xgboost_predict_on_CSV(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/from_ApacheParquet/component.yaml
      digest: eb6a6f2e21760995351eda950d9a350bf339410afb327eceab7df0618cceb656
      text: |
        name: Train XGBoost model on ApacheParquet
        description: Trains an XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/from_ApacheParquet/component.yaml'}
        inputs:
        - {name: training_data, type: ApacheParquet, description: Training data in the Apache
            Parquet format.}
        - {name: label_column_name, type: String, description: Name of the column containing
            the label data.}
        - {name: starting_model, type: XGBoostModel, description: Existing trained model to
            start from (in the binary XGBoost format)., optional: true}
        - {name: num_iterations, type: Integer, description: Number of boosting iterations.,
          default: '10', optional: true}
        - name: objective
          type: String
          description: |-
            The learning task and the corresponding learning objective.
            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
            The most common values are:
            "reg:squarederror" - Regression with squared loss (default).
            "reg:logistic" - Logistic regression.
            "binary:logistic" - Logistic regression for binary classification, output probability.
            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
          default: reg:squarederror
          optional: true
        - {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,
            `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`
            uses linear functions.', default: gbtree, optional: true}
        - {name: learning_rate, type: Float, description: 'Step size shrinkage used in update
            to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}
        - name: min_split_loss
          type: Float
          description: |-
            Minimum loss reduction required to make a further partition on a leaf node of the tree.
            The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
          default: '0'
          optional: true
        - name: max_depth
          type: Integer
          description: |-
            Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
            0 indicates no limit on depth. Range: [0,Inf].
          default: '6'
          optional: true
        - {name: booster_params, type: JsonObject, description: 'Parameters for the booster.
            See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}
        outputs:
        - {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost
            format.}
        - {name: model_config, type: XGBoostModelConfig, description: The internal parameter
            configuration of Booster as a JSON string.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1
              python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              'pyarrow==9.0.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_XGBoost_model_on_ApacheParquet(
                  training_data_path,
                  model_path,
                  model_config_path,
                  label_column_name,
                  starting_model_path = None,
                  num_iterations = 10,
                  # Booster parameters
                  objective = "reg:squarederror",
                  booster = "gbtree",
                  learning_rate = 0.3,
                  min_split_loss = 0,
                  max_depth = 6,
                  booster_params = None,
              ):
                  """Trains an XGBoost model.

                  Args:
                      training_data_path: Training data in the Apache Parquet format.
                      model_path: Trained model in the binary XGBoost format.
                      model_config_path: The internal parameter configuration of Booster as a JSON string.
                      starting_model_path: Existing trained model to start from (in the binary XGBoost format).
                      label_column_name: Name of the column containing the label data.
                      num_iterations: Number of boosting iterations.
                      booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                      objective: The learning task and the corresponding learning objective.
                          See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                          The most common values are:
                          "reg:squarederror" - Regression with squared loss (default).
                          "reg:logistic" - Logistic regression.
                          "binary:logistic" - Logistic regression for binary classification, output probability.
                          "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                          "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                          "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
                      booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.
                      learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].
                      min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.
                          The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
                      max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                          0 indicates no limit on depth. Range: [0,Inf].

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  import pandas
                  import xgboost

                  # Loading data
                  df = pandas.read_parquet(training_data_path)
                  print("Training data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print()
                  print("Final training data information:")
                  df.info(verbose=True)

                  training_data = xgboost.DMatrix(
                      data=df.drop(columns=[label_column_name]),
                      label=df[[label_column_name]],
                      enable_categorical=True,
                  )
                  # Training
                  booster_params = booster_params or {}
                  booster_params.setdefault("objective", objective)
                  booster_params.setdefault("booster", booster)
                  booster_params.setdefault("learning_rate", learning_rate)
                  booster_params.setdefault("min_split_loss", min_split_loss)
                  booster_params.setdefault("max_depth", max_depth)

                  starting_model = None
                  if starting_model_path:
                      starting_model = xgboost.Booster(model_file=starting_model_path)

                  print()
                  print("Training the model:")
                  model = xgboost.train(
                      params=booster_params,
                      dtrain=training_data,
                      num_boost_round=num_iterations,
                      xgb_model=starting_model,
                      evals=[(training_data, "training_data")],
                  )

                  # Saving the model in binary format
                  model.save_model(model_path)

                  model_config_str = model.save_config()
                  with open(model_config_path, "w") as model_config_file:
                      model_config_file.write(model_config_str)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train XGBoost model on ApacheParquet', description='Trains an XGBoost model.')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_XGBoost_model_on_ApacheParquet(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: objective}
                then:
                - --objective
                - {inputValue: objective}
            - if:
                cond: {isPresent: booster}
                then:
                - --booster
                - {inputValue: booster}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: min_split_loss}
                then:
                - --min-split-loss
                - {inputValue: min_split_loss}
            - if:
                cond: {isPresent: max_depth}
                then:
                - --max-depth
                - {inputValue: max_depth}
            - if:
                cond: {isPresent: booster_params}
                then:
                - --booster-params
                - {inputValue: booster_params}
            - --model
            - {outputPath: model}
            - --model-config
            - {outputPath: model_config}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/from_ApacheParquet/component.yaml
      digest: 012fca583d79e22d2121f00251b52e2632052de8bc3d8a6666e881e18fb3ee59
      text: |
        name: Xgboost predict on ApacheParquet
        description: Makes predictions using a trained XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/from_ApacheParquet/component.yaml'}
        inputs:
        - {name: data, type: ApacheParquet, description: Feature data in Apache Parquet format.}
        - {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}
        - {name: label_column_name, type: String, description: Optional. Name of the column
            containing the label data that is excluded during the prediction., optional: true}
        outputs:
        - {name: predictions, description: Model predictions.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1
              python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              'pyarrow==9.0.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def xgboost_predict_on_ApacheParquet(
                  data_path,
                  model_path,
                  predictions_path,
                  label_column_name = None,
              ):
                  """Makes predictions using a trained XGBoost model.

                  Args:
                      data_path: Feature data in Apache Parquet format.
                      model_path: Trained model in binary XGBoost format.
                      predictions_path: Model predictions.
                      label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  from pathlib import Path

                  import numpy
                  import pandas
                  import xgboost

                  # Loading data
                  df = pandas.read_parquet(data_path)
                  print("Evaluation data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print("Final evaluation data information:")
                  df.info(verbose=True)

                  if label_column_name:
                      df = df.drop(columns=[label_column_name])

                  evaluation_data = xgboost.DMatrix(
                      data=df,
                      enable_categorical=True,
                  )

                  # Training
                  model = xgboost.Booster(model_file=model_path)

                  predictions = model.predict(evaluation_data)

                  Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Xgboost predict on ApacheParquet', description='Makes predictions using a trained XGBoost model.')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = xgboost_predict_on_ApacheParquet(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - --predictions
            - {outputPath: predictions}
  - name: PyTorch
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Create_fully_connected_network/component.yaml
      digest: 6074655c1dddbafcd7a4d618ba1d2489fe3f53444399f1450a23c4acc69280c5
      text: |
        name: Create fully connected pytorch network
        description: Creates fully-connected network in PyTorch ScriptModule format
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_fully_connected_network/component.yaml'
        inputs:
        - {name: layer_sizes, type: JsonArray}
        - {name: activation_name, type: String, default: relu, optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: network, type: PyTorchScriptModule}
        implementation:
          container:
            image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def create_fully_connected_pytorch_network(
                  layer_sizes,
                  network_path,
                  activation_name = 'relu',
                  random_seed = 0,
              ):
                  '''Creates fully-connected network in PyTorch ScriptModule format'''
                  import torch
                  torch.manual_seed(random_seed)

                  activation = getattr(torch, activation_name, None) or getattr(torch.nn.functional, activation_name, None)
                  if not activation:
                      raise ValueError(f'Activation "{activation_name}" was not found.')

                  class ActivationLayer(torch.nn.Module):
                      def forward(self, input):
                          return activation(input)

                  layers = []
                  for layer_idx in range(len(layer_sizes) - 1):
                      layer = torch.nn.Linear(layer_sizes[layer_idx], layer_sizes[layer_idx + 1])
                      layers.append(layer)
                      if layer_idx < len(layer_sizes) - 2:
                          layers.append(ActivationLayer())

                  network = torch.nn.Sequential(*layers)
                  script_module = torch.jit.script(network)
                  print(script_module)
                  script_module.save(network_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Create fully connected pytorch network', description='Creates fully-connected network in PyTorch ScriptModule format')
              _parser.add_argument("--layer-sizes", dest="layer_sizes", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--activation-name", dest="activation_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--network", dest="network_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = create_fully_connected_pytorch_network(**_parsed_args)
            args:
            - --layer-sizes
            - {inputValue: layer_sizes}
            - if:
                cond: {isPresent: activation_name}
                then:
                - --activation-name
                - {inputValue: activation_name}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --network
            - {outputPath: network}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml
      digest: 3c84cd7eddeda2d59ca84d37ea416bd78427aea8eeccd85005e958c203914b67
      text: |
        name: Train pytorch model from csv
        description: Trains PyTorch model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml'}
        inputs:
        - {name: model, type: PyTorchScriptModule, description: Model in PyTorch format.}
        - {name: training_data, type: CSV, description: Tabular dataset for training.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label.}
        - {name: loss_function_name, type: String, description: Name of the loss function.,
          default: mse_loss, optional: true}
        - {name: number_of_epochs, type: Integer, description: Number of training epochs.,
          default: '1', optional: true}
        - {name: learning_rate, type: Float, description: Learning rate of the optimizer.,
          default: '0.1', optional: true}
        - {name: optimizer_name, type: String, description: Name of the optimizer., default: Adadelta,
          optional: true}
        - {name: optimizer_parameters, type: JsonObject, description: Optimizer parameters
            in dictionary form., optional: true}
        - {name: batch_size, type: Integer, description: Number of training samples to use
            in each batch., default: '32', optional: true}
        - {name: batch_log_interval, type: Integer, description: Print training summary after
            every N batches., default: '100', optional: true}
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: trained_model, type: PyTorchScriptModule, description: Trained model in PyTorch
            format.}
        implementation:
          container:
            image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_pytorch_model_from_csv(
                  model_path,
                  training_data_path,
                  trained_model_path,
                  label_column_name,
                  loss_function_name = 'mse_loss',
                  number_of_epochs = 1,
                  learning_rate = 0.1,
                  optimizer_name = 'Adadelta',
                  optimizer_parameters = None,
                  batch_size = 32,
                  batch_log_interval = 100,
                  random_seed = 0,
              ):
                  """Trains PyTorch model.

                  Args:
                      model_path: Model in PyTorch format.
                      training_data_path: Tabular dataset for training.
                      trained_model_path: Trained model in PyTorch format.
                      label_column_name: Name of the table column to use as label.
                      loss_function_name: Name of the loss function.
                      number_of_epochs: Number of training epochs.
                      learning_rate: Learning rate of the optimizer.
                      optimizer_name: Name of the optimizer.
                      optimizer_parameters: Optimizer parameters in dictionary form.
                      batch_size: Number of training samples to use in each batch.
                      batch_log_interval: Print training summary after every N batches.
                      random_seed: Controls the seed of the random processes.
                  """
                  import pandas
                  import torch

                  torch.manual_seed(random_seed)

                  use_cuda = torch.cuda.is_available()
                  device = torch.device("cuda" if use_cuda else "cpu")

                  model = torch.jit.load(model_path)
                  model.to(device)
                  model.train()

                  optimizer_class = getattr(torch.optim, optimizer_name, None)
                  if not optimizer_class:
                      raise ValueError(f'Optimizer "{optimizer_name}" was not found.')

                  optimizer_parameters = optimizer_parameters or {}
                  optimizer_parameters['lr'] = learning_rate
                  optimizer = optimizer_class(model.parameters(), **optimizer_parameters)

                  loss_function = getattr(torch, loss_function_name, None) or getattr(torch.nn, loss_function_name, None) or getattr(torch.nn.functional, loss_function_name, None)
                  if not loss_function:
                      raise ValueError(f'Loss function "{loss_function_name}" was not found.')

                  class CsvDataset(torch.utils.data.Dataset):

                      def __init__(self, file_path, label_column_name, drop_nan_columns_or_rows = 'columns'):
                          dataframe = pandas.read_csv(file_path).convert_dtypes()
                          # Preventing error: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object
                          if drop_nan_columns_or_rows == 'columns':
                              non_nan_data = dataframe.dropna(axis='columns')
                              removed_columns = set(dataframe.columns) - set(non_nan_data.columns)
                              if removed_columns:
                                  print('Skipping columns with NaNs: ' + str(removed_columns))
                              dataframe = non_nan_data
                          if drop_nan_columns_or_rows == 'rows':
                              non_nan_data = dataframe.dropna(axis='index')
                              number_of_removed_rows = len(dataframe) - len(non_nan_data)
                              if number_of_removed_rows:
                                  print(f'Skipped {number_of_removed_rows} rows with NaNs.')
                              dataframe = non_nan_data
                          numerical_data = dataframe.select_dtypes(include='number')
                          non_numerical_data = dataframe.select_dtypes(exclude='number')
                          if not non_numerical_data.empty:
                              print('Skipping non-number columns:')
                              print(non_numerical_data.dtypes)
                          self._dataframe = dataframe
                          self.labels = numerical_data[[label_column_name]]
                          self.features = numerical_data.drop(columns=[label_column_name])

                      def __len__(self):
                          return len(self._dataframe)

                      def __getitem__(self, index):
                          return [self.features.loc[index].to_numpy(dtype='float32'), self.labels.loc[index].to_numpy(dtype='float32')]

                  dataset = CsvDataset(
                      file_path=training_data_path,
                      label_column_name=label_column_name,
                  )
                  train_loader = torch.utils.data.DataLoader(
                      dataset=dataset,
                      batch_size=batch_size,
                      shuffle=True,
                  )

                  last_full_batch_loss = None
                  for epoch in range(1, number_of_epochs + 1):
                      for batch_idx, (data, target) in enumerate(train_loader):
                          data, target = data.to(device), target.to(device)
                          optimizer.zero_grad()
                          output = model(data)
                          loss = loss_function(output, target)
                          loss.backward()
                          optimizer.step()
                          if len(data) == batch_size:
                              last_full_batch_loss = loss.item()
                          if batch_idx % batch_log_interval == 0:
                              print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                                  epoch, batch_idx * len(data), len(train_loader.dataset),
                                  100. * batch_idx / len(train_loader), loss.item()))
                      print(f'Training epoch {epoch} completed. Last full batch loss: {last_full_batch_loss:.6f}')

                  # print(optimizer.state_dict())
                  model.save(trained_model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train pytorch model from csv', description='Trains PyTorch model.')
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function-name", dest="loss_function_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-epochs", dest="number_of_epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-name", dest="optimizer_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-parameters", dest="optimizer_parameters", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-log-interval", dest="batch_log_interval", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--trained-model", dest="trained_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_pytorch_model_from_csv(**_parsed_args)
            args:
            - --model
            - {inputPath: model}
            - --training-data
            - {inputPath: training_data}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: loss_function_name}
                then:
                - --loss-function-name
                - {inputValue: loss_function_name}
            - if:
                cond: {isPresent: number_of_epochs}
                then:
                - --number-of-epochs
                - {inputValue: number_of_epochs}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: optimizer_name}
                then:
                - --optimizer-name
                - {inputValue: optimizer_name}
            - if:
                cond: {isPresent: optimizer_parameters}
                then:
                - --optimizer-parameters
                - {inputValue: optimizer_parameters}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - if:
                cond: {isPresent: batch_log_interval}
                then:
                - --batch-log-interval
                - {inputValue: batch_log_interval}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --trained-model
            - {outputPath: trained_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml
      digest: 7edca2cd8212000a52dedf6c60aeb808fd022818ccb0ee64ddc85259b84e6186
      text: |
        name: Convert to onnx from pytorch script module
        description: Creates fully-connected network in PyTorch ScriptModule format
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml'
        inputs:
        - {name: model, type: PyTorchScriptModule}
        - {name: list_of_input_shapes, type: JsonArray}
        outputs:
        - {name: converted_model, type: OnnxModel}
        implementation:
          container:
            image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def convert_to_onnx_from_pytorch_script_module(
                  model_path,
                  converted_model_path,
                  list_of_input_shapes,
              ):
                  '''Creates fully-connected network in PyTorch ScriptModule format'''
                  import torch
                  model = torch.jit.load(model_path)
                  example_inputs = [
                      torch.ones(*input_shape)
                      for input_shape in list_of_input_shapes
                  ]
                  example_outputs = model.forward(*example_inputs)
                  torch.onnx.export(
                      model=model,
                      args=example_inputs,
                      f=converted_model_path,
                      verbose=True,
                      training=torch.onnx.TrainingMode.EVAL,
                      example_outputs=example_outputs,
                  )

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Convert to onnx from pytorch script module', description='Creates fully-connected network in PyTorch ScriptModule format')
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--list-of-input-shapes", dest="list_of_input_shapes", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = convert_to_onnx_from_pytorch_script_module(**_parsed_args)
            args:
            - --model
            - {inputPath: model}
            - --list-of-input-shapes
            - {inputValue: list_of_input_shapes}
            - --converted-model
            - {outputPath: converted_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/46d51383e6554b7f3ab4fd8cf614d8c2b422fb22/components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml
      digest: 8298b5ee1b0f0879f893add4cf352c8dec7cf9e21bb9db134c91a2d046cdb0ec
      text: |
        name: Create PyTorch Model Archive with base handler
        inputs:
        - {name: Model, type: PyTorchScriptModule}
        - {name: Model name, type: String, default: model}
        - {name: Model version, type: String, default: "1.0"}
        outputs:
        - {name: Model archive, type: PyTorchModelArchive}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml'
        implementation:
          container:
            image: pytorch/torchserve:0.6.0-cpu
            command:
            - bash
            - -exc
            - |
              model_path=$0
              model_name=$1
              model_version=$2
              output_model_archive_path=$3

              mkdir -p "$(dirname "$output_model_archive_path")"

              # TODO: Use the built-in base_handler once my fix is merged: https://github.com/pytorch/serve/pull/1682
              echo '
              from ts.torch_handler import base_handler
              class BaseHandler(base_handler.BaseHandler):
                  pass
              ' > base_handler.py  # torch-model-archiver needs the handler to have .py extension
              torch-model-archiver --model-name "$model_name" --version "$model_version" --serialized-file "$model_path" --handler base_handler.py

              # torch-model-archiver does not allow specifying the output path, but always writes to "${model_name}.<format>"
              expected_model_archive_path="${model_name}.mar"
              mv "$expected_model_archive_path" "$output_model_archive_path"

            - {inputPath: Model}
            - {inputValue: Model name}
            - {inputValue: Model version}
            - {outputPath: Model archive}
  - name: Tensorflow
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/f3a9769d35a057c31a498e0667cae2e4a830c5b0/components/tensorflow/Create_fully_connected_network/component.yaml
      digest: c97df1a0fe9b4d58300ffdf97dd63555cef1246ec93f9c68a0667dc8a99f1e1c
      text: |
        name: Create fully connected tensorflow network
        description: Creates fully-connected network in Tensorflow SavedModel format
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Create_fully_connected_network/component.yaml'}
        inputs:
        - {name: layer_sizes, type: JsonArray}
        - {name: activation_name, type: String, default: relu, optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: model, type: TensorflowSavedModel}
        implementation:
          container:
            image: tensorflow/tensorflow:2.7.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def create_fully_connected_tensorflow_network(
                  layer_sizes,
                  model_path,
                  activation_name = "relu",
                  random_seed = 0,
              ):
                  """Creates fully-connected network in Tensorflow SavedModel format"""
                  import tensorflow as tf
                  tf.random.set_seed(seed=random_seed)

                  if len(layer_sizes) < 2:
                      raise ValueError(f"Fully-connected network requires at least two layer sizes (input and output). Got {layer_sizes}.")

                  model = tf.keras.models.Sequential()
                  model.add(tf.keras.Input(shape=(layer_sizes[0],)))
                  for layer_size in layer_sizes[1:-1]:
                      model.add(tf.keras.layers.Dense(units=layer_size, activation=activation_name))
                  # The last layer is left without activation
                  model.add(tf.keras.layers.Dense(units=layer_sizes[-1]))

                  # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:
                  #tf.saved_model.save(model, model_path)
                  # ValueError: Unable to create a Keras model from this SavedModel.
                  # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.
                  # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.
                  # See https://github.com/keras-team/keras/issues/16451
                  tf.keras.models.save_model(model, model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Create fully connected tensorflow network', description='Creates fully-connected network in Tensorflow SavedModel format')
              _parser.add_argument("--layer-sizes", dest="layer_sizes", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--activation-name", dest="activation_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = create_fully_connected_tensorflow_network(**_parsed_args)
            args:
            - --layer-sizes
            - {inputValue: layer_sizes}
            - if:
                cond: {isPresent: activation_name}
                then:
                - --activation-name
                - {inputValue: activation_name}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/c504a4010348c50eaaf6d4337586ccc008f4dcef/components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml
      digest: 42ae60c889034dbad74815653e95b4f7d576b5f47f803173e8679c7b54984609
      text: |
        name: Train model using Keras on CSV
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml'}
        inputs:
        - {name: training_data, type: CSV}
        - {name: model, type: TensorflowSavedModel}
        - {name: label_column_name, type: String}
        - {name: loss_function_name, type: String, default: mean_squared_error, optional: true}
        - {name: number_of_epochs, type: Integer, default: '1', optional: true}
        - {name: learning_rate, type: Float, default: '0.1', optional: true}
        - {name: optimizer_name, type: String, default: Adadelta, optional: true}
        - {name: optimizer_parameters, type: JsonObject, optional: true}
        - {name: batch_size, type: Integer, default: '32', optional: true}
        - {name: metric_names, type: JsonArray, optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: trained_model, type: TensorflowSavedModel}
        implementation:
          container:
            image: tensorflow/tensorflow:2.8.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_model_using_Keras_on_CSV(
                  training_data_path,
                  model_path,
                  trained_model_path,
                  label_column_name,
                  loss_function_name = "mean_squared_error",
                  number_of_epochs = 1,
                  learning_rate = 0.1,
                  optimizer_name = "Adadelta",
                  optimizer_parameters = None,
                  batch_size = 32,
                  metric_names = None,
                  random_seed = 0,
              ):
                  import tensorflow as tf
                  tf.random.set_seed(seed=random_seed)

                  # Loading model using Keras. Model loaded using TensorFlow does not have .fit.
                  #model = tf.saved_model.load(export_dir=model_path)
                  keras_model = tf.keras.models.load_model(filepath=model_path)

                  optimizer_parameters = optimizer_parameters or {}
                  optimizer_parameters["learning_rate"] = learning_rate
                  optimizer_config = {
                      "class_name": optimizer_name,
                      "config": optimizer_parameters,
                  }
                  optimizer = tf.keras.optimizers.get(optimizer_config)
                  loss = tf.keras.losses.get(loss_function_name)

                  training_dataset = tf.data.experimental.make_csv_dataset(
                      file_pattern=training_data_path,
                      batch_size=batch_size,
                      label_name=label_column_name,
                      header=True,
                      # Need to specify num_epochs=1 otherwise the training becomes infinite
                      num_epochs=1,
                      shuffle=True,
                      shuffle_seed=random_seed,
                      ignore_errors=True,
                  )
                  def stack_feature_batches(features_batch, labels_batch):
                      # Need to stack individual feature columns to create a single feature tensor
                      # Need to cast all column tensor types to float to prevent error:
                      # TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int32, float32, float32, int32, int32] that don't all match.
                      list_of_feature_batches = list(tf.cast(x=feature_batch, dtype=tf.float32) for feature_batch in features_batch.values())
                      return tf.stack(list_of_feature_batches, axis=-1), labels_batch

                  training_dataset = training_dataset.map(stack_feature_batches)

                  # Need to compile the model to prevent error:
                  # ValueError: No gradients provided for any variable: [..., ...].
                  keras_model.compile(
                      optimizer=optimizer,
                      loss=loss,
                      metrics=metric_names,
                  )
                  keras_model.fit(
                      training_dataset,
                      epochs=number_of_epochs,
                  )

                  # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:
                  #tf.saved_model.save(keras_model, trained_model_path)
                  # ValueError: Unable to create a Keras model from this SavedModel.
                  # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.
                  # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.
                  # See https://github.com/keras-team/keras/issues/16451
                  tf.keras.models.save_model(keras_model, trained_model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train model using Keras on CSV', description='')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function-name", dest="loss_function_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-epochs", dest="number_of_epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-name", dest="optimizer_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-parameters", dest="optimizer_parameters", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--metric-names", dest="metric_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--trained-model", dest="trained_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_model_using_Keras_on_CSV(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --model
            - {inputPath: model}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: loss_function_name}
                then:
                - --loss-function-name
                - {inputValue: loss_function_name}
            - if:
                cond: {isPresent: number_of_epochs}
                then:
                - --number-of-epochs
                - {inputValue: number_of_epochs}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: optimizer_name}
                then:
                - --optimizer-name
                - {inputValue: optimizer_name}
            - if:
                cond: {isPresent: optimizer_parameters}
                then:
                - --optimizer-parameters
                - {inputValue: optimizer_parameters}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - if:
                cond: {isPresent: metric_names}
                then:
                - --metric-names
                - {inputValue: metric_names}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --trained-model
            - {outputPath: trained_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/tensorflow/Train_model_using_Keras/on_ApacheParquet/component.yaml
      digest: 5f886b115e8c5e35c65e9f520b449b7cbcbdebb92114eeef2b7745497f6f6be1
      text: |
        name: Train model using Keras on ApacheParquet
        description: Trains TensorFlow model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Train_model_using_Keras/on_ApacheParquet/component.yaml'}
        inputs:
        - {name: training_data, type: ApacheParquet, description: Tabular dataset for training.}
        - {name: model, type: TensorflowSavedModel, description: Model in TensorFlow format.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label.}
        - {name: loss_function_name, type: String, description: Name of the loss function.,
          default: mean_squared_error, optional: true}
        - {name: number_of_epochs, type: Integer, description: Number of training epochs.,
          default: '1', optional: true}
        - {name: learning_rate, type: Float, description: Learning rate of the optimizer.,
          default: '0.1', optional: true}
        - {name: optimizer_name, type: String, description: Name of the optimizer., default: Adadelta,
          optional: true}
        - {name: optimizer_parameters, type: JsonObject, description: Optimizer parameters
            in dictionary form., optional: true}
        - {name: batch_size, type: Integer, description: Number of training samples to use
            in each batch., default: '32', optional: true}
        - {name: metric_names, type: JsonArray, description: A list of metrics to evaluate
            during the training., optional: true}
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: trained_model, type: TensorflowSavedModel, description: Trained model in
            TensorFlow format.}
        implementation:
          container:
            image: tensorflow/tensorflow:2.8.0
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'tensorflow-io==0.25.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
              --quiet --no-warn-script-location 'tensorflow-io==0.25.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_model_using_Keras_on_ApacheParquet(
                  training_data_path,
                  model_path,
                  trained_model_path,
                  label_column_name,
                  loss_function_name = "mean_squared_error",
                  number_of_epochs = 1,
                  learning_rate = 0.1,
                  optimizer_name = "Adadelta",
                  optimizer_parameters = None,
                  batch_size = 32,
                  metric_names = None,
                  random_seed = 0,
              ):
                  """Trains TensorFlow model.

                  Args:
                      training_data_path: Tabular dataset for training.
                      model_path: Model in TensorFlow format.
                      trained_model_path: Trained model in TensorFlow format.
                      label_column_name: Name of the table column to use as label.
                      loss_function_name: Name of the loss function.
                      number_of_epochs: Number of training epochs.
                      learning_rate: Learning rate of the optimizer.
                      optimizer_name: Name of the optimizer.
                      optimizer_parameters: Optimizer parameters in dictionary form.
                      batch_size: Number of training samples to use in each batch.
                      metric_names: A list of metrics to evaluate during the training.
                      random_seed: Controls the seed of the random processes.
                  """
                  import tensorflow as tf
                  import tensorflow_io as tfio

                  tf.random.set_seed(seed=random_seed)

                  FEATURES_COLUMN_NAME = "features"
                  SHUFFLE_BUFFER_SIZE = 10000

                  # Loading model using Keras. Model loaded using TensorFlow does not have .fit.
                  # model = tf.saved_model.load(export_dir=model_path)
                  keras_model = tf.keras.models.load_model(filepath=model_path)

                  optimizer_parameters = optimizer_parameters or {}
                  optimizer_parameters["learning_rate"] = learning_rate
                  optimizer_config = {
                      "class_name": optimizer_name,
                      "config": optimizer_parameters,
                  }
                  optimizer = tf.keras.optimizers.get(optimizer_config)
                  loss = tf.keras.losses.get(loss_function_name)

                  def stack_feature_batches(columns_batch_dict):
                      label_batch = columns_batch_dict.pop(label_column_name.encode())
                      if FEATURES_COLUMN_NAME in columns_batch_dict:
                          features_batch = columns_batch_dict[FEATURES_COLUMN_NAME]
                      else:
                          # Need to stack individual feature columns to create a single feature tensor
                          # Need to cast all column tensor types to float
                          list_of_feature_batches = list(
                              tf.cast(x=feature_batch, dtype=tf.float32)
                              for feature_batch in columns_batch_dict.values()
                          )
                          features_batch = tf.stack(list_of_feature_batches, axis=-1)
                      return features_batch, label_batch

                  # ! parquet::ParquetException is thrown if the Parquet dataset has nulls values
                  training_dataset = tfio.IODataset.from_parquet(filename=training_data_path)

                  training_dataset = (
                      training_dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE, seed=random_seed)
                      .repeat(count=number_of_epochs)
                      .batch(
                          batch_size=batch_size,
                          num_parallel_calls=tf.data.AUTOTUNE,
                          deterministic=True,
                      )
                      .map(
                          map_func=stack_feature_batches,
                          num_parallel_calls=tf.data.AUTOTUNE,
                          deterministic=True,
                      )
                  )

                  # Need to compile the model to prevent error:
                  # ValueError: No gradients provided for any variable: [..., ...].
                  keras_model.compile(
                      optimizer=optimizer,
                      loss=loss,
                      metrics=metric_names,
                  )
                  keras_model.fit(
                      training_dataset,
                      epochs=number_of_epochs,
                  )

                  # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:
                  # tf.saved_model.save(keras_model, trained_model_path)
                  # ValueError: Unable to create a Keras model from this SavedModel.
                  # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.
                  # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.
                  # See https://github.com/keras-team/keras/issues/16451
                  tf.keras.models.save_model(keras_model, trained_model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train model using Keras on ApacheParquet', description='Trains TensorFlow model.')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function-name", dest="loss_function_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-epochs", dest="number_of_epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-name", dest="optimizer_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-parameters", dest="optimizer_parameters", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--metric-names", dest="metric_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--trained-model", dest="trained_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_model_using_Keras_on_ApacheParquet(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --model
            - {inputPath: model}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: loss_function_name}
                then:
                - --loss-function-name
                - {inputValue: loss_function_name}
            - if:
                cond: {isPresent: number_of_epochs}
                then:
                - --number-of-epochs
                - {inputValue: number_of_epochs}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: optimizer_name}
                then:
                - --optimizer-name
                - {inputValue: optimizer_name}
            - if:
                cond: {isPresent: optimizer_parameters}
                then:
                - --optimizer-parameters
                - {inputValue: optimizer_parameters}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - if:
                cond: {isPresent: metric_names}
                then:
                - --metric-names
                - {inputValue: metric_names}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --trained-model
            - {outputPath: trained_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/tensorflow/Predict/on_ApacheParquet/component.yaml
      digest: 7e760242a9c9ce013a367b866d3d1d7411986367bcfa59d847d853d3219f7fe8
      text: |
        name: Predict with TensorFlow model on ApacheParquet data
        description: Makes predictions using TensorFlow model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Predict/on_ApacheParquet/component.yaml'}
        inputs:
        - {name: dataset, type: ApacheParquet, description: Tabular dataset for prediction.}
        - {name: model, type: TensorflowSavedModel, description: Trained model in TensorFlow
            format.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label., optional: true}
        - {name: batch_size, type: Integer, description: Number of samples to use in each
            batch., default: '1000', optional: true}
        outputs:
        - {name: predictions, description: Predictions in multiline text format.}
        implementation:
          container:
            image: tensorflow/tensorflow:2.9.1
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'tensorflow-io==0.26.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
              --quiet --no-warn-script-location 'tensorflow-io==0.26.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def predict_with_TensorFlow_model_on_ApacheParquet_data(
                  dataset_path,
                  model_path,
                  predictions_path,
                  label_column_name = None,
                  batch_size = 1000,
              ):
                  """Makes predictions using TensorFlow model.

                  Args:
                      dataset_path: Tabular dataset for prediction.
                      model_path: Trained model in TensorFlow format.
                      predictions_path: Predictions in multiline text format.
                      label_column_name: Name of the table column to use as label.
                      batch_size: Number of samples to use in each batch.
                  """
                  import numpy
                  import tensorflow as tf
                  import tensorflow_io as tfio

                  FEATURES_COLUMN_NAME = "features"

                  model = tf.saved_model.load(export_dir=model_path)

                  def stack_feature_batches_and_drop_labels(columns_batch_dict):
                      if label_column_name:
                          # batch dict keys have bytes type
                          columns_batch_dict.pop(label_column_name.encode())

                      if FEATURES_COLUMN_NAME in columns_batch_dict:
                          features_batch = columns_batch_dict[FEATURES_COLUMN_NAME]
                      else:
                          # Need to stack individual feature columns to create a single feature tensor
                          # Need to cast all column tensor types to float
                          list_of_feature_batches = list(
                              tf.cast(x=feature_batch, dtype=tf.float32)
                              for feature_batch in columns_batch_dict.values()
                          )
                          features_batch = tf.stack(list_of_feature_batches, axis=-1)
                      return features_batch

                  # ! parquet::ParquetException is thrown if the Parquet dataset has nulls values
                  dataset = tfio.IODataset.from_parquet(filename=dataset_path)

                  dataset = dataset.batch(
                      batch_size=batch_size,
                      num_parallel_calls=tf.data.AUTOTUNE,
                      deterministic=True,
                  ).map(
                      map_func=stack_feature_batches_and_drop_labels,
                      num_parallel_calls=tf.data.AUTOTUNE,
                      deterministic=True,
                  )

                  with open(predictions_path, "w") as predictions_file:
                      for features_batch in dataset:
                          predictions_tensor = model(features_batch)
                          numpy.savetxt(predictions_file, predictions_tensor.numpy())

              import argparse
              _parser = argparse.ArgumentParser(prog='Predict with TensorFlow model on ApacheParquet data', description='Makes predictions using TensorFlow model.')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = predict_with_TensorFlow_model_on_ApacheParquet_data(**_parsed_args)
            args:
            - --dataset
            - {inputPath: dataset}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - --predictions
            - {outputPath: predictions}
  - name: CatBoost
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_regression/from_CSV/component.yaml
      digest: ac04356d7b08e9d3e90279befd4df446832641517efb34dea75354312347295a
      text: |
        name: Catboost train regression
        description: |-
          Train a CatBoost classifier model.

              Args:
                  training_data_path: Path for the training data in CSV format.
                  model_path: Output path for the trained model in binary CatBoostModel format.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column: Column containing the label data.

                  loss_function: The metric to use in training and also selector of the machine learning
                      problem to solve. Default = 'RMSE'. Possible values:
                      'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'
                  num_iterations: Number of trees to add to the ensemble.
                  learning_rate: Step size shrinkage used in update to prevents overfitting.
                      Default value is selected automatically for binary classification with other parameters set to default.
                      In all other cases default is 0.03.
                  depth: Depth of a tree. All trees are the same depth. Default = 6
                  random_seed: Random number seed. Default = 0

                  cat_features: A list of Categorical features (indices or names).
                  additional_training_options: A dictionary with additional options to pass to CatBoostRegressor

              Outputs:
                  model: Trained model in binary CatBoostModel format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: training_data, type: CSV}
        - {name: starting_model, type: CatBoostModel, optional: true}
        - {name: label_column, type: Integer, default: '0', optional: true}
        - {name: loss_function, type: String, default: RMSE, optional: true}
        - {name: num_iterations, type: Integer, default: '500', optional: true}
        - {name: learning_rate, type: Float, optional: true}
        - {name: depth, type: Integer, default: '6', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        - {name: cat_features, type: JsonArray, optional: true}
        - {name: additional_training_options, type: JsonObject, default: '{}', optional: true}
        outputs:
        - {name: model, type: CatBoostModel}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_regression/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_train_regression(
                  training_data_path,
                  model_path,
                  starting_model_path = None,
                  label_column = 0,

                  loss_function = 'RMSE',
                  num_iterations = 500,
                  learning_rate = None,
                  depth = 6,
                  random_seed = 0,

                  cat_features = None,

                  additional_training_options = {},
              ):
                  '''Train a CatBoost classifier model.

                  Args:
                      training_data_path: Path for the training data in CSV format.
                      model_path: Output path for the trained model in binary CatBoostModel format.
                      starting_model_path: Path for the existing trained model to start from.
                      label_column: Column containing the label data.

                      loss_function: The metric to use in training and also selector of the machine learning
                          problem to solve. Default = 'RMSE'. Possible values:
                          'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'
                      num_iterations: Number of trees to add to the ensemble.
                      learning_rate: Step size shrinkage used in update to prevents overfitting.
                          Default value is selected automatically for binary classification with other parameters set to default.
                          In all other cases default is 0.03.
                      depth: Depth of a tree. All trees are the same depth. Default = 6
                      random_seed: Random number seed. Default = 0

                      cat_features: A list of Categorical features (indices or names).
                      additional_training_options: A dictionary with additional options to pass to CatBoostRegressor

                  Outputs:
                      model: Trained model in binary CatBoostModel format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile
                  from pathlib import Path

                  from catboost import CatBoostRegressor, Pool

                  column_descriptions = {label_column: 'Label'}
                  column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                  with open(column_description_path, 'w') as column_description_file:
                      for idx, kind in column_descriptions.items():
                          column_description_file.write('{}\t{}\n'.format(idx, kind))

                  train_data = Pool(
                      training_data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoostRegressor(
                      iterations=num_iterations,
                      depth=depth,
                      learning_rate=learning_rate,
                      loss_function=loss_function,
                      random_seed=random_seed,
                      verbose=True,
                      **additional_training_options,
                  )

                  model.fit(
                      train_data,
                      cat_features=cat_features,
                      init_model=starting_model_path,
                      #verbose=False,
                      #plot=True,
                  )
                  Path(model_path).parent.mkdir(parents=True, exist_ok=True)
                  model.save_model(model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost train regression', description="Train a CatBoost classifier model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary CatBoostModel format.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n\n        loss_function: The metric to use in training and also selector of the machine learning\n            problem to solve. Default = 'RMSE'. Possible values:\n            'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'\n        num_iterations: Number of trees to add to the ensemble.\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\n            Default value is selected automatically for binary classification with other parameters set to default.\n            In all other cases default is 0.03.\n        depth: Depth of a tree. All trees are the same depth. Default = 6\n        random_seed: Random number seed. Default = 0\n\n        cat_features: A list of Categorical features (indices or names).\n        additional_training_options: A dictionary with additional options to pass to CatBoostRegressor\n\n    Outputs:\n        model: Trained model in binary CatBoostModel format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function", dest="loss_function", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--depth", dest="depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--cat-features", dest="cat_features", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--additional-training-options", dest="additional_training_options", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_train_regression(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - if:
                cond: {isPresent: loss_function}
                then:
                - --loss-function
                - {inputValue: loss_function}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: depth}
                then:
                - --depth
                - {inputValue: depth}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - if:
                cond: {isPresent: cat_features}
                then:
                - --cat-features
                - {inputValue: cat_features}
            - if:
                cond: {isPresent: additional_training_options}
                then:
                - --additional-training-options
                - {inputValue: additional_training_options}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_classifier/from_CSV/component.yaml
      digest: f4ef9b474377248847c6487a827aa7b14d092a8d031c368168f2857a8ff11cad
      text: |
        name: Catboost train classifier
        description: |-
          Train a CatBoost classifier model.

              Args:
                  training_data_path: Path for the training data in CSV format.
                  model_path: Output path for the trained model in binary CatBoostModel format.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column: Column containing the label data.

                  loss_function: The metric to use in training and also selector of the machine learning
                      problem to solve. Default = 'Logloss'
                  num_iterations: Number of trees to add to the ensemble.
                  learning_rate: Step size shrinkage used in update to prevents overfitting.
                      Default value is selected automatically for binary classification with other parameters set to default.
                      In all other cases default is 0.03.
                  depth: Depth of a tree. All trees are the same depth. Default = 6
                  random_seed: Random number seed. Default = 0

                  cat_features: A list of Categorical features (indices or names).
                  text_features: A list of Text features (indices or names).
                  additional_training_options: A dictionary with additional options to pass to CatBoostClassifier

              Outputs:
                  model: Trained model in binary CatBoostModel format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: training_data, type: CSV}
        - {name: starting_model, type: CatBoostModel, optional: true}
        - {name: label_column, type: Integer, default: '0', optional: true}
        - {name: loss_function, type: String, default: Logloss, optional: true}
        - {name: num_iterations, type: Integer, default: '500', optional: true}
        - {name: learning_rate, type: Float, optional: true}
        - {name: depth, type: Integer, default: '6', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        - {name: cat_features, type: JsonArray, optional: true}
        - {name: text_features, type: JsonArray, optional: true}
        - {name: additional_training_options, type: JsonObject, default: '{}', optional: true}
        outputs:
        - {name: model, type: CatBoostModel}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_classifier/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_train_classifier(
                  training_data_path,
                  model_path,
                  starting_model_path = None,
                  label_column = 0,

                  loss_function = 'Logloss',
                  num_iterations = 500,
                  learning_rate = None,
                  depth = 6,
                  random_seed = 0,

                  cat_features = None,
                  text_features = None,

                  additional_training_options = {},
              ):
                  '''Train a CatBoost classifier model.

                  Args:
                      training_data_path: Path for the training data in CSV format.
                      model_path: Output path for the trained model in binary CatBoostModel format.
                      starting_model_path: Path for the existing trained model to start from.
                      label_column: Column containing the label data.

                      loss_function: The metric to use in training and also selector of the machine learning
                          problem to solve. Default = 'Logloss'
                      num_iterations: Number of trees to add to the ensemble.
                      learning_rate: Step size shrinkage used in update to prevents overfitting.
                          Default value is selected automatically for binary classification with other parameters set to default.
                          In all other cases default is 0.03.
                      depth: Depth of a tree. All trees are the same depth. Default = 6
                      random_seed: Random number seed. Default = 0

                      cat_features: A list of Categorical features (indices or names).
                      text_features: A list of Text features (indices or names).
                      additional_training_options: A dictionary with additional options to pass to CatBoostClassifier

                  Outputs:
                      model: Trained model in binary CatBoostModel format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile
                  from pathlib import Path

                  from catboost import CatBoostClassifier, Pool

                  column_descriptions = {label_column: 'Label'}
                  column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                  with open(column_description_path, 'w') as column_description_file:
                      for idx, kind in column_descriptions.items():
                          column_description_file.write('{}\t{}\n'.format(idx, kind))

                  train_data = Pool(
                      training_data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoostClassifier(
                      iterations=num_iterations,
                      depth=depth,
                      learning_rate=learning_rate,
                      loss_function=loss_function,
                      random_seed=random_seed,
                      verbose=True,
                      **additional_training_options,
                  )

                  model.fit(
                      train_data,
                      cat_features=cat_features,
                      text_features=text_features,
                      init_model=starting_model_path,
                      #verbose=False,
                      #plot=True,
                  )
                  Path(model_path).parent.mkdir(parents=True, exist_ok=True)
                  model.save_model(model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost train classifier', description="Train a CatBoost classifier model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary CatBoostModel format.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n\n        loss_function: The metric to use in training and also selector of the machine learning\n            problem to solve. Default = 'Logloss'\n        num_iterations: Number of trees to add to the ensemble.\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\n            Default value is selected automatically for binary classification with other parameters set to default.\n            In all other cases default is 0.03.\n        depth: Depth of a tree. All trees are the same depth. Default = 6\n        random_seed: Random number seed. Default = 0\n\n        cat_features: A list of Categorical features (indices or names).\n        text_features: A list of Text features (indices or names).\n        additional_training_options: A dictionary with additional options to pass to CatBoostClassifier\n\n    Outputs:\n        model: Trained model in binary CatBoostModel format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function", dest="loss_function", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--depth", dest="depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--cat-features", dest="cat_features", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--text-features", dest="text_features", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--additional-training-options", dest="additional_training_options", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_train_classifier(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - if:
                cond: {isPresent: loss_function}
                then:
                - --loss-function
                - {inputValue: loss_function}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: depth}
                then:
                - --depth
                - {inputValue: depth}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - if:
                cond: {isPresent: cat_features}
                then:
                - --cat-features
                - {inputValue: cat_features}
            - if:
                cond: {isPresent: text_features}
                then:
                - --text-features
                - {inputValue: text_features}
            - if:
                cond: {isPresent: additional_training_options}
                then:
                - --additional-training-options
                - {inputValue: additional_training_options}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_values/from_CSV/component.yaml
      digest: 64f4651cd80d1bdbd524ff0cd2272916ebd3855f7b1f71d8abf474ccb0ec3dfb
      text: |
        name: Catboost predict values
        description: |-
          Predict values with a CatBoost model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: data, type: CSV}
        - {name: model, type: CatBoostModel}
        - {name: label_column, type: Integer, optional: true}
        outputs:
        - {name: predictions}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_values/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_predict_values(
                  data_path,
                  model_path,
                  predictions_path,

                  label_column = None,
              ):
                  '''Predict values with a CatBoost model.

                  Args:
                      data_path: Path for the data in CSV format.
                      model_path: Path for the trained model in binary CatBoostModel format.
                      label_column: Column containing the label data.
                      predictions_path: Output path for the predictions.

                  Outputs:
                      predictions: Predictions in text format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile

                  from catboost import CatBoost, Pool
                  import numpy

                  if label_column:
                      column_descriptions = {label_column: 'Label'}
                      column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                      with open(column_description_path, 'w') as column_description_file:
                          for idx, kind in column_descriptions.items():
                              column_description_file.write('{}\t{}\n'.format(idx, kind))
                  else:
                      column_description_path = None

                  eval_data = Pool(
                      data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoost()
                  model.load_model(model_path)

                  predictions = model.predict(eval_data, prediction_type='RawFormulaVal')
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost predict values', description='Predict values with a CatBoost model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_predict_values(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_classes/from_CSV/component.yaml
      digest: 6e45f6a3b7a7805aa3c99a9d6c1bbf7434c365ad6f23de930014902bf19dc78f
      text: |
        name: Catboost predict classes
        description: |-
          Predict classes using the CatBoost classifier model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Class predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: data, type: CSV}
        - {name: model, type: CatBoostModel}
        - {name: label_column, type: Integer, optional: true}
        outputs:
        - {name: predictions}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_classes/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.22' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_predict_classes(
                  data_path,
                  model_path,
                  predictions_path,

                  label_column = None,
              ):
                  '''Predict classes using the CatBoost classifier model.

                  Args:
                      data_path: Path for the data in CSV format.
                      model_path: Path for the trained model in binary CatBoostModel format.
                      label_column: Column containing the label data.
                      predictions_path: Output path for the predictions.

                  Outputs:
                      predictions: Class predictions in text format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile

                  from catboost import CatBoostClassifier, Pool
                  import numpy

                  if label_column:
                      column_descriptions = {label_column: 'Label'}
                      column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                      with open(column_description_path, 'w') as column_description_file:
                          for idx, kind in column_descriptions.items():
                              column_description_file.write('{}\t{}\n'.format(idx, kind))
                  else:
                      column_description_path = None

                  eval_data = Pool(
                      data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoostClassifier()
                  model.load_model(model_path)

                  predictions = model.predict(eval_data)
                  numpy.savetxt(predictions_path, predictions, fmt='%s')

              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost predict classes', description='Predict classes using the CatBoost classifier model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Class predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_predict_classes(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml
      digest: 9074952dc787000001b2f75291a287a1cc968648c711a8f648605fd0053999bb
      text: |
        name: Catboost predict class probabilities
        description: |-
          Predict class probabilities with a CatBoost model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: data, type: CSV}
        - {name: model, type: CatBoostModel}
        - {name: label_column, type: Integer, optional: true}
        outputs:
        - {name: predictions}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_predict_class_probabilities(
                  data_path,
                  model_path,
                  predictions_path,

                  label_column = None,
              ):
                  '''Predict class probabilities with a CatBoost model.

                  Args:
                      data_path: Path for the data in CSV format.
                      model_path: Path for the trained model in binary CatBoostModel format.
                      label_column: Column containing the label data.
                      predictions_path: Output path for the predictions.

                  Outputs:
                      predictions: Predictions in text format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile

                  from catboost import CatBoost, Pool
                  import numpy

                  if label_column:
                      column_descriptions = {label_column: 'Label'}
                      column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                      with open(column_description_path, 'w') as column_description_file:
                          for idx, kind in column_descriptions.items():
                              column_description_file.write('{}\t{}\n'.format(idx, kind))
                  else:
                      column_description_path = None

                  eval_data = Pool(
                      data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoost()
                  model.load_model(model_path)

                  predictions = model.predict(eval_data, prediction_type='Probability')
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost predict class probabilities', description='Predict class probabilities with a CatBoost model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_predict_class_probabilities(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml
      digest: f56b73b1e1e6b9e8745a4dfb49423ec33194adde2aac0b9d20d4c7bf77731907
      text: |
        name: Convert CatBoostModel to ONNX
        description: |-
          Convert CatBoost model to ONNX format.

              Args:
                  model_path: Path of a trained model in binary CatBoost model format.
                  converted_model_path: Output path for the converted model.

              Outputs:
                  converted_model: Model in ONNX format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: model, type: CatBoostModel}
        outputs:
        - {name: converted_model, type: ONNX}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.22' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def convert_CatBoostModel_to_ONNX(
                  model_path,
                  converted_model_path,
              ):
                  '''Convert CatBoost model to ONNX format.

                  Args:
                      model_path: Path of a trained model in binary CatBoost model format.
                      converted_model_path: Output path for the converted model.

                  Outputs:
                      converted_model: Model in ONNX format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  from catboost import CatBoost

                  model = CatBoost()
                  model.load_model(model_path)
                  model.save_model(converted_model_path, format="onnx")

              import argparse
              _parser = argparse.ArgumentParser(prog='Convert CatBoostModel to ONNX', description='Convert CatBoost model to ONNX format.\n\n    Args:\n        model_path: Path of a trained model in binary CatBoost model format.\n        converted_model_path: Output path for the converted model.\n\n    Outputs:\n        converted_model: Model in ONNX format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = convert_CatBoostModel_to_ONNX(**_parsed_args)
            args:
            - --model
            - {inputPath: model}
            - --converted-model
            - {outputPath: converted_model}
  - name: Vowpal_Wabbit
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Vowpal_Wabbit/Create_JSON_dataset/from_CSV/component.yaml
      digest: 2e5d07f6df3fa21f7d0438136526d952a0c726302971c2e00ac42fe26856e674
      text: |
        name: Create Vowpal Wabbit JSON dataset from CSV
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Create_JSON_dataset/from_CSV/component.yaml'}
        inputs:
        - {name: dataset, type: CSV}
        - {name: label_column_name, type: String, optional: true}
        outputs:
        - {name: converted_dataset, type: VowpalWabbitJsonDataset}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def create_Vowpal_Wabbit_JSON_dataset_from_CSV(
                  dataset_path,
                  converted_dataset_path,
                  label_column_name = None,
              ):
                  import json
                  import pandas

                  df = pandas.read_csv(dataset_path).convert_dtypes()

                  if label_column_name:
                      label_series = df[label_column_name]
                      features_df = df.drop(columns=[label_column_name])
                      label_values_list = label_series.to_list()
                      feature_records_list = features_df.to_dict("records")

                      with open(converted_dataset_path, "w") as f:
                          for features, label in zip(feature_records_list, label_values_list):
                              non_nan_features = {
                                  k: v for k, v in features.items() if v == v and v is not None
                              }
                              vw_record = {
                                  "_label": label,
                              }
                              vw_record.update(non_nan_features)
                              vw_record_line = json.dumps(vw_record)
                              f.write(vw_record_line + "\n")
                  else:
                      features_df = df
                      feature_records_list = features_df.to_dict("records")

                      with open(converted_dataset_path, "w") as f:
                          for features in feature_records_list:
                              non_nan_features = {
                                  k: v for k, v in features.items() if v == v and v is not None
                              }
                              vw_record = non_nan_features
                              vw_record_line = json.dumps(vw_record)
                              f.write(vw_record_line + "\n")

              import argparse
              _parser = argparse.ArgumentParser(prog='Create Vowpal Wabbit JSON dataset from CSV', description='')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--converted-dataset", dest="converted_dataset_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = create_Vowpal_Wabbit_JSON_dataset_from_CSV(**_parsed_args)
            args:
            - --dataset
            - {inputPath: dataset}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - --converted-dataset
            - {outputPath: converted_dataset}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/a2a629e776d5fa0204ce71370cab23282d3e4278/components/ML_frameworks/Vowpal_Wabbit/Train_regression_model/from_VowpalWabbitJsonDataset/component.yaml
      digest: 601b68212b611b2b798767f4fcf3e77de662688b772daaae0ad3eb55426f56bc
      text: |
        name: Train regression model using Vowpal Wabbit on VowpalWabbitDataset
        metadata:
          annotations:
            author: "Alexey Volkov <alexey.volkov@ark-kun.com>"
            canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Train_regression_model/from_VowpalWabbitJsonDataset/component.yaml"
        inputs:
        - {name: Dataset, type: VowpalWabbitJsonDataset}
        - {name: Initial model, type: VowpalWabbitRegressorModel, optional: true}
        - {name: Number of passes, type: Integer, default: "1"}
        - {name: Loss function, type: String, default: "squared", description: "Supported values: squared, hinge, logistic, quantile, poisson. See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Loss-functions"}
        outputs:
        - {name: Model, type: VowpalWabbitRegressorModel}
        - {name: Readable model, type: VowpalWabbitReadableHashRegressorModel}
        implementation:
          container:
            image: vowpalwabbit/vw-rel-alpine:9.0.1
            # See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-line-arguments
            command:
              - sh
              - -exc
              - |
                # Creating directories for the outputs
                mkdir -p "$(dirname "$4")" # Model
                mkdir -p "$(dirname "$6")" # Readable model
                "$0" "$@"
              - ./vw
              - --data
              - {inputPath: Dataset}
              - --final_regressor
              - {outputPath: Model}
              - --invert_hash
              - {outputPath: Readable model}
              - --passes
              - {inputValue: Number of passes}
              - --loss_function
              - {inputValue: Loss function}
              # Enable JSON parsing
              - --json
              - if:
                  cond: {isPresent: Initial model}
                  then:
                    - --initial_regressor
                    - {inputPath: Initial model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/a2a629e776d5fa0204ce71370cab23282d3e4278/components/ML_frameworks/Vowpal_Wabbit/Predict/from_VowpalWabbitJsonDataset/component.yaml
      digest: f3eecb52c5f4d31d89090d17217e05f881600f8c7a18fc8de11eea1bf9d5fdf8
      text: |
        name: Predict using Vowpal Wabbit model on VowpalWabbitJsonDataset
        metadata:
          annotations:
            author: "Alexey Volkov <alexey.volkov@ark-kun.com>"
            canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Predict/from_VowpalWabbitJsonDataset/component.yaml"
        inputs:
        - {name: Dataset, type: VowpalWabbitJsonDataset}
        - {name: Model, type: VowpalWabbitRegressorModel}
        outputs:
        - {name: Predictions}
        implementation:
          container:
            image: vowpalwabbit/vw-rel-alpine:9.0.1
            # See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-line-arguments
            command:
              - sh
              - -exc
              - |
                # Creating directories for the outputs
                mkdir -p "$(dirname "$6")" # Predictions
                "$0" "$@"
              - ./vw
              - --data
              - {inputPath: Dataset}
              - --initial_regressor
              - {inputPath: Model}
              - --predictions
              - {outputPath: Predictions}
              # Ignore label information and just test
              - --testonly
              # Enable JSON parsing
              - --json
- name: Converters
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_CSV/component.yaml
    digest: 01be9da9b7e2c1d63a72090cb1c47abf9a581b037c4a477584fbfdf1dcf85c86
    text: |
      name: Convert csv to apache parquet
      description: |-
        Converts CSV table to Apache Parquet.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: CSV}
      outputs:
      - {name: output_data, type: ApacheParquet}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_CSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
            --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_csv_to_apache_parquet(
                data_path,
                output_data_path,
            ):
                '''Converts CSV table to Apache Parquet.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import csv, parquet

                table = csv.read_csv(data_path)
                parquet.write_table(table, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert csv to apache parquet', description='Converts CSV table to Apache Parquet.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_csv_to_apache_parquet(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_TSV/component.yaml
    digest: 50b66cb4e967d5fd1ea3ca0e9de8956b1f313c9f3abb271ca426b050d4573eac
    text: |
      name: Convert tsv to apache parquet
      description: |-
        Converts TSV table to Apache Parquet.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: TSV}
      outputs:
      - {name: output_data, type: ApacheParquet}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_TSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
            --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_tsv_to_apache_parquet(
                data_path,
                output_data_path,
            ):
                '''Converts TSV table to Apache Parquet.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import csv, parquet

                table = csv.read_csv(data_path, parse_options=csv.ParseOptions(delimiter='\t'))
                parquet.write_table(table, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert tsv to apache parquet', description='Converts TSV table to Apache Parquet.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_tsv_to_apache_parquet(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml
    digest: 9ad7aee6ca2fb82841c30cf2995b75c063f19b1f9c935bda1a9ef3d8ac1efed6
    text: |
      name: Convert apache arrow feather to apache parquet
      description: |-
        Converts Apache Arrow Feather to Apache Parquet.

            [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheArrowFeather}
      outputs:
      - {name: output_data, type: ApacheParquet}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
            --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_arrow_feather_to_apache_parquet(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Arrow Feather to Apache Parquet.

                [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import feather, parquet

                table = feather.read_table(data_path)
                parquet.write_table(table, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache arrow feather to apache parquet', description='Converts Apache Arrow Feather to Apache Parquet.\n\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_apache_arrow_feather_to_apache_parquet(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_CSV/component.yaml
    digest: fc9d4d0537d59e61dae838cbce8bbc58a338ea5e87eb504f941ef35965e08cc9
    text: |
      name: Convert apache parquet to csv
      description: |-
        Converts Apache Parquet to CSV.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheParquet}
      outputs:
      - {name: output_data, type: CSV}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_CSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'
            --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_parquet_to_csv(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Parquet to CSV.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import parquet

                data_frame = parquet.read_pandas(data_path).to_pandas()
                data_frame.to_csv(
                    output_data_path,
                    index=False,
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache parquet to csv', description='Converts Apache Parquet to CSV.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_apache_parquet_to_csv(**_parsed_args)
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_TSV/component.yaml
    digest: 7812764bbc6b9bdd4d71e406dea9575943c084ba7799cafd7e378085d76d4c42
    text: |
      name: Convert apache parquet to tsv
      description: |-
        Converts Apache Parquet to TSV.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheParquet}
      outputs:
      - {name: output_data, type: TSV}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_TSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'
            --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_parquet_to_tsv(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Parquet to TSV.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import parquet

                data_frame = parquet.read_pandas(data_path).to_pandas()
                data_frame.to_csv(
                    output_data_path,
                    index=False,
                    sep='\t',
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache parquet to tsv', description='Converts Apache Parquet to TSV.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_apache_parquet_to_tsv(**_parsed_args)
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml
    digest: 53a6051a8cac9ce5eb5fe0950c40c48b5ec78ecddbe76bbafdeffbc586f280a6
    text: |
      name: Convert apache parquet to apache arrow feather
      description: |-
        Converts Apache Parquet to Apache Arrow Feather.

            [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheParquet}
      outputs:
      - {name: output_data, type: ApacheArrowFeather}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'
            --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_parquet_to_apache_arrow_feather(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Parquet to Apache Arrow Feather.

                [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import feather, parquet

                data_frame = parquet.read_pandas(data_path).to_pandas()
                feather.write_feather(data_frame, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache parquet to apache arrow feather', description='Converts Apache Parquet to Apache Arrow Feather.\n\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_apache_parquet_to_apache_arrow_feather(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml
    digest: f82823ba99ab6fc6696e37685a00918aa94a9186b0a1979206b86ca3818099a4
    text: |
      name: Convert to XGBoostJsonModel from XGBoostModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostModel}
      outputs:
      - {name: converted_model, type: XGBoostJsonModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'xgboost==1.5.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_XGBoostJsonModel_from_XGBoostModel(
                model_path,
                converted_model_path,
            ):
                import os
                import xgboost

                model = xgboost.Booster(model_file=model_path)

                # The file path needs to have .json extension so that the model is saved in the JSON format.
                tmp_converted_model_path = converted_model_path + ".json"
                model.save_model(tmp_converted_model_path)
                os.rename(tmp_converted_model_path, converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to XGBoostJsonModel from XGBoostModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_XGBoostJsonModel_from_XGBoostModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml
    digest: b45de01b867736c16b2a68f53aa808c02f8e0caa2bffe2172aad7a20cdd17063
    text: |
      name: Convert to XGBoostModel from XGBoostJsonModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostJsonModel}
      outputs:
      - {name: converted_model, type: XGBoostModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'xgboost==1.5.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_XGBoostModel_from_XGBoostJsonModel(
                model_path,
                converted_model_path,
            ):
                import os
                import shutil
                import tempfile
                import xgboost

                # The file path needs to have .json extension so that the model is loaded as JSON format.
                with tempfile.NamedTemporaryFile(suffix=".json") as tmp_model_file:
                    tmp_model_path = tmp_model_file.name
                    shutil.copy(model_path, tmp_model_path)
                    model = xgboost.Booster(model_file=tmp_model_path)

                tmp_converted_model_path = converted_model_path + ".bst"
                model.save_model(tmp_converted_model_path)
                os.rename(tmp_converted_model_path, converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to XGBoostModel from XGBoostJsonModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_XGBoostModel_from_XGBoostJsonModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/4619f84eebc54230153ba48b0e67ac8446dc31c6/components/_converters/OnnxModel/from_XGBoostJsonModel/component.yaml
    digest: 5e02e5fa2f12181a6b9588b152e9ee78d4952fea7f3d5740dfdbd15b3b2c0ee9
    text: |
      name: Convert to OnnxModel from XGBoostJsonModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_XGBoostJsonModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostJsonModel}
      - {name: model_graph_name, type: String, optional: true}
      - {name: doc_string, type: String, default: '', optional: true}
      - {name: target_opset, type: Integer, optional: true}
      outputs:
      - {name: converted_model, type: OnnxModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.2' 'onnx==1.11.0' 'onnxmltools==1.10.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
            python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.5.2' 'onnx==1.11.0'
            'onnxmltools==1.10.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_OnnxModel_from_XGBoostJsonModel(
                model_path,
                converted_model_path,
                model_graph_name = None,
                doc_string = "",
                target_opset = None,
            ):
                import xgboost
                import onnx
                import onnxmltools

                # The file path needs to have .json extension so that the model is loaded as JSON format.
                import os
                import shutil
                import tempfile
                with tempfile.NamedTemporaryFile(suffix=".json") as tmp_model_file:
                    tmp_model_path = tmp_model_file.name
                    shutil.copy(model_path, tmp_model_path)
                    model = xgboost.Booster(model_file=tmp_model_path)

                # Workaround for https://github.com/onnx/onnxmltools/issues/499
                # Although I'm not sure this formula is correct given https://github.com/dmlc/xgboost/pull/6569
                model.best_ntree_limit = model.num_boosted_rounds()

                converted_model = onnxmltools.convert_xgboost(
                    model=model,
                    name=model_graph_name,
                    initial_types=[
                        (
                            "input",
                            onnxmltools.convert.common.data_types.FloatTensorType(
                                shape=[None, model.num_features()]
                            ),
                        )
                    ],
                    doc_string=doc_string,
                    target_opset=target_opset,
                )
                onnx.save_model(proto=converted_model, f=converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from XGBoostJsonModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-graph-name", dest="model_graph_name", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--doc-string", dest="doc_string", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--target-opset", dest="target_opset", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_OnnxModel_from_XGBoostJsonModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: model_graph_name}
              then:
              - --model-graph-name
              - {inputValue: model_graph_name}
          - if:
              cond: {isPresent: doc_string}
              then:
              - --doc-string
              - {inputValue: doc_string}
          - if:
              cond: {isPresent: target_opset}
              then:
              - --target-opset
              - {inputValue: target_opset}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/ae9d04e833d973d785809e7734021b06bcfea9bc/components/_converters/OnnxModel/from_XGBoostModel/component.yaml
    digest: 3cdc24652d1ab9b436dbe935d291cd03a1d6e14c039f8fc6e67e25ff6f80b0e5
    text: |
      name: Convert to OnnxModel from XGBoostModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_XGBoostModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostModel}
      - {name: model_graph_name, type: String, optional: true}
      - {name: doc_string, type: String, default: '', optional: true}
      - {name: target_opset, type: Integer, optional: true}
      outputs:
      - {name: converted_model, type: OnnxModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.2' 'onnx==1.11.0' 'onnxmltools==1.10.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
            python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.5.2' 'onnx==1.11.0'
            'onnxmltools==1.10.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_OnnxModel_from_XGBoostModel(
                model_path,
                converted_model_path,
                model_graph_name = None,
                doc_string = "",
                target_opset = None,
            ):
                import xgboost
                import onnx
                import onnxmltools

                model = xgboost.Booster(model_file=model_path)

                # Workaround for https://github.com/onnx/onnxmltools/issues/499
                # Although I'm not sure this formula is correct given https://github.com/dmlc/xgboost/pull/6569
                model.best_ntree_limit = model.num_boosted_rounds()

                converted_model = onnxmltools.convert_xgboost(
                    model=model,
                    name=model_graph_name,
                    initial_types=[
                        (
                            "input",
                            onnxmltools.convert.common.data_types.FloatTensorType(
                                shape=[None, model.num_features()]
                            ),
                        )
                    ],
                    doc_string=doc_string,
                    target_opset=target_opset,
                )
                onnx.save_model(proto=converted_model, f=converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from XGBoostModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-graph-name", dest="model_graph_name", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--doc-string", dest="doc_string", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--target-opset", dest="target_opset", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_OnnxModel_from_XGBoostModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: model_graph_name}
              then:
              - --model-graph-name
              - {inputValue: model_graph_name}
          - if:
              cond: {isPresent: doc_string}
              then:
              - --doc-string
              - {inputValue: doc_string}
          - if:
              cond: {isPresent: target_opset}
              then:
              - --target-opset
              - {inputValue: target_opset}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml
    digest: d44f6b94387edfc6560bf452c5662de431d2e1141b670e7ab9e921eba25396a8
    text: |
      name: To ONNX from Tensorflow SavedModel
      inputs:
      - {name: Model, type: TensorflowSavedModel}
      outputs:
      - {name: Model, type: OnnxModel}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml'
      implementation:
        container:
          image: tensorflow/tensorflow:2.3.0
          command:
          - sh
          - -exc
          - python3 -m pip install tf2onnx==1.6.3 && "$0" "$@"
          - python3
          - -m
          - tf2onnx.convert
          - --saved-model
          - {inputPath: Model}
          - --output
          - {outputPath: Model}
          - --fold_const
          - --verbose
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/2fc275072568cd0cdf73743ab49aa90928303f2c/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml
    digest: 93f892f4f2bdace722e473cae22141715b6328bdc0a508cd6878f050449ff92d
    text: |
      name: Convert to TensorflowSavedModel from OnnxModel
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml'
      inputs:
      - {name: model, type: OnnxModel}
      outputs:
      - {name: converted_model, type: TensorflowSavedModel}
      implementation:
        container:
          image: tensorflow/tensorflow:2.8.0
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'onnx-tf==1.9.0' 'onnx==1.11.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
            pip install --quiet --no-warn-script-location 'onnx-tf==1.9.0' 'onnx==1.11.0'
            --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_TensorflowSavedModel_from_OnnxModel(
                model_path,
                converted_model_path,
            ):
                import onnx
                import onnx_tf

                onnx_model = onnx.load(model_path)
                tf_rep = onnx_tf.backend.prepare(onnx_model)
                tf_rep.export_graph(converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to TensorflowSavedModel from OnnxModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_TensorflowSavedModel_from_OnnxModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/f7f79decd10955b8408541f6f695a614538a6901/components/_converters/ScikitLearnPickleModel/to_OnnxModel/component.yaml
    digest: 1899985bd972ba5849be119fc841633e940f293f2314ae52d5cdd9ace2fda15f
    text: |
      name: Convert to OnnxModel from ScikitLearnPickleModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ScikitLearnPickleModel/to_OnnxModel/component.yaml'}
      inputs:
      - {name: model, type: ScikitLearnPickleModel}
      - {name: doc_string, type: String, default: '', optional: true}
      - {name: target_opset, type: Integer, optional: true}
      outputs:
      - {name: converted_model, type: OnnxModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'skl2onnx==1.11' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'skl2onnx==1.11' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_OnnxModel_from_ScikitLearnPickleModel(
                model_path,
                converted_model_path,
                doc_string = "",
                target_opset = None,
            ):
                import onnx
                import pickle
                import skl2onnx

                with open(model_path, "rb") as model_file:
                    model = pickle.load(model_file)

                # Funny hack to infer the model input shape
                # Just try passing arrays of different size to the model.predict method and check what works, lol.
                def get_input_output_shapes(model):
                    for input_length_candidate in range(100000):
                        try:
                            prediction = model.predict(X=[[0.0] * input_length_candidate])
                            input_length = input_length_candidate
                            output_shape = prediction.shape[1:]
                            return (input_length, output_shape)
                        except:
                            pass
                    return None

                input_length, _ = get_input_output_shapes(model)

                # Setting model name is not necessary, but why not.
                model_type = type(model)
                model_type_name = model_type.__module__ + "." + model_type.__name__

                onnx_model = skl2onnx.convert_sklearn(
                    model=model,
                    initial_types=[
                        ("input", skl2onnx.common.data_types.FloatTensorType([None, input_length]))
                    ],
                    name=model_type_name,
                    verbose=1,
                    # TODO: Include the original model hash digest so that the model can be traced.
                    doc_string=doc_string,
                    target_opset=target_opset,
                )
                print(onnx_model)
                onnx.save_model(
                    proto=onnx_model, f=converted_model_path,
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from ScikitLearnPickleModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--doc-string", dest="doc_string", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--target-opset", dest="target_opset", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_OnnxModel_from_ScikitLearnPickleModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: doc_string}
              then:
              - --doc-string
              - {inputValue: doc_string}
          - if:
              cond: {isPresent: target_opset}
              then:
              - --target-opset
              - {inputValue: target_opset}
          - --converted-model
          - {outputPath: converted_model}
