annotations: {}
folders:
- name: Quick start
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/57f780b15922061e59833541b71f3d099e710177/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml
    digest: 42030acab16b71bbc3ad018563b5aedb94d373e72f7cb2b322bc27dbbee75e1b
    text: |
      name: Chicago Taxi Trips dataset
      description: |
        City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew

        The input parameters configure the SQL query to the database.
        The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.
        Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml'
      inputs:
      - {name: Where, type: String, default: 'trip_start_timestamp>="1900-01-01" AND trip_start_timestamp<"2100-01-01"'}
      - {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}
      - {name: Select, type: String, default: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total'}
      - {name: Format, type: String, default: 'csv', description: 'Output data format. Supports csv,tsv,cml,rdf,json'}
      outputs:
      - {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}
      implementation:
        container:
          # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
          image: alpine/curl:8.14.1
          command:
          - sh
          - -c
          - |
            set -e -x -o pipefail
            output_path="$0"
            select="$1"
            where="$2"
            limit="$3"
            format="$4"
            mkdir -p "$(dirname "$output_path")"
            curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}" \
                --data-urlencode '$limit='"${limit}" \
                --data-urlencode '$where='"${where}" \
                --data-urlencode '$select='"${select}" \
                | sed -E 's/"([^",\]*)"/\1/g' > "$output_path"  # Removing unneeded quotes around all numbers
          - {outputPath: Table}
          - {inputValue: Select}
          - {inputValue: Where}
          - {inputValue: Limit}
          - {inputValue: Format}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/component.yaml
    digest: 5b8bec6716337d7bd8ecafd6dd46bc44527783bc05950cfe03a60206b43c7654
    text: |
      name: Train XGBoost model on CSV
      description: Trains an XGBoost model.
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'}
      inputs:
      - {name: training_data, type: CSV, description: Training data in CSV format.}
      - {name: label_column_name, type: String, description: Name of the column containing
          the label data.}
      - {name: starting_model, type: XGBoostModel, description: Existing trained model to
          start from (in the binary XGBoost format)., optional: true}
      - {name: num_iterations, type: Integer, description: Number of boosting iterations.,
        default: '10', optional: true}
      - name: objective
        type: String
        description: |-
          The learning task and the corresponding learning objective.
          See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
          The most common values are:
          "reg:squarederror" - Regression with squared loss (default).
          "reg:logistic" - Logistic regression.
          "binary:logistic" - Logistic regression for binary classification, output probability.
          "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
          "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
          "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
        default: reg:squarederror
        optional: true
      - {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,
          `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`
          uses linear functions.', default: gbtree, optional: true}
      - {name: learning_rate, type: Float, description: 'Step size shrinkage used in update
          to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}
      - name: min_split_loss
        type: Float
        description: |-
          Minimum loss reduction required to make a further partition on a leaf node of the tree.
          The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
        default: '0'
        optional: true
      - name: max_depth
        type: Integer
        description: |-
          Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
          0 indicates no limit on depth. Range: [0,Inf].
        default: '6'
        optional: true
      - {name: booster_params, type: JsonObject, description: 'Parameters for the booster.
          See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}
      outputs:
      - {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost
          format.}
      - {name: model_config, type: XGBoostModelConfig, description: The internal parameter
          configuration of Booster as a JSON string.}
      implementation:
        container:
          image: python:3.10
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
            --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def train_XGBoost_model_on_CSV(
                training_data_path,
                model_path,
                model_config_path,
                label_column_name,
                starting_model_path = None,
                num_iterations = 10,
                # Booster parameters
                objective = "reg:squarederror",
                booster = "gbtree",
                learning_rate = 0.3,
                min_split_loss = 0,
                max_depth = 6,
                booster_params = None,
            ):
                """Trains an XGBoost model.

                Args:
                    training_data_path: Training data in CSV format.
                    model_path: Trained model in the binary XGBoost format.
                    model_config_path: The internal parameter configuration of Booster as a JSON string.
                    starting_model_path: Existing trained model to start from (in the binary XGBoost format).
                    label_column_name: Name of the column containing the label data.
                    num_iterations: Number of boosting iterations.
                    booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                    objective: The learning task and the corresponding learning objective.
                        See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                        The most common values are:
                        "reg:squarederror" - Regression with squared loss (default).
                        "reg:logistic" - Logistic regression.
                        "binary:logistic" - Logistic regression for binary classification, output probability.
                        "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                        "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                        "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
                    booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.
                    learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].
                    min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.
                        The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
                    max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                        0 indicates no limit on depth. Range: [0,Inf].

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                """
                import pandas
                import xgboost

                df = pandas.read_csv(
                    training_data_path,
                ).convert_dtypes()
                print("Training data information:")
                df.info(verbose=True)
                # Converting column types that XGBoost does not support
                for column_name, dtype in df.dtypes.items():
                    if dtype in ["string", "object"]:
                        print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                        df[column_name] = df[column_name].astype("category")
                        print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                    # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                    if pandas.api.types.is_float_dtype(dtype):
                        # Converting from "Float64" to "float64"
                        df[column_name] = df[column_name].astype(dtype.name.lower())
                print()
                print("Final training data information:")
                df.info(verbose=True)

                training_data = xgboost.DMatrix(
                    data=df.drop(columns=[label_column_name]),
                    label=df[[label_column_name]],
                    enable_categorical=True,
                )

                booster_params = booster_params or {}
                booster_params.setdefault("objective", objective)
                booster_params.setdefault("booster", booster)
                booster_params.setdefault("learning_rate", learning_rate)
                booster_params.setdefault("min_split_loss", min_split_loss)
                booster_params.setdefault("max_depth", max_depth)

                starting_model = None
                if starting_model_path:
                    starting_model = xgboost.Booster(model_file=starting_model_path)

                print()
                print("Training the model:")
                model = xgboost.train(
                    params=booster_params,
                    dtrain=training_data,
                    num_boost_round=num_iterations,
                    xgb_model=starting_model,
                    evals=[(training_data, "training_data")],
                )

                # Saving the model in binary format
                model.save_model(model_path)

                model_config_str = model.save_config()
                with open(model_config_path, "w") as model_config_file:
                    model_config_file.write(model_config_str)

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Train XGBoost model on CSV', description='Trains an XGBoost model.')
            _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = train_XGBoost_model_on_CSV(**_parsed_args)
          args:
          - --training-data
          - {inputPath: training_data}
          - --label-column-name
          - {inputValue: label_column_name}
          - if:
              cond: {isPresent: starting_model}
              then:
              - --starting-model
              - {inputPath: starting_model}
          - if:
              cond: {isPresent: num_iterations}
              then:
              - --num-iterations
              - {inputValue: num_iterations}
          - if:
              cond: {isPresent: objective}
              then:
              - --objective
              - {inputValue: objective}
          - if:
              cond: {isPresent: booster}
              then:
              - --booster
              - {inputValue: booster}
          - if:
              cond: {isPresent: learning_rate}
              then:
              - --learning-rate
              - {inputValue: learning_rate}
          - if:
              cond: {isPresent: min_split_loss}
              then:
              - --min-split-loss
              - {inputValue: min_split_loss}
          - if:
              cond: {isPresent: max_depth}
              then:
              - --max-depth
              - {inputValue: max_depth}
          - if:
              cond: {isPresent: booster_params}
              then:
              - --booster-params
              - {inputValue: booster_params}
          - --model
          - {outputPath: model}
          - --model-config
          - {outputPath: model_config}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/component.yaml
    digest: 6fd7196d2061e6f49ec98459c90f4b1e4e63bca21170c70b23f7679921ce01d7
    text: |
      name: Xgboost predict on CSV
      description: Makes predictions using a trained XGBoost model.
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'}
      inputs:
      - {name: data, type: CSV, description: Feature data in Apache Parquet format.}
      - {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}
      - {name: label_column_name, type: String, description: Optional. Name of the column
          containing the label data that is excluded during the prediction., optional: true}
      outputs:
      - {name: predictions, description: Model predictions.}
      implementation:
        container:
          image: python:3.10
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
            --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def xgboost_predict_on_CSV(
                data_path,
                model_path,
                predictions_path,
                label_column_name = None,
            ):
                """Makes predictions using a trained XGBoost model.

                Args:
                    data_path: Feature data in Apache Parquet format.
                    model_path: Trained model in binary XGBoost format.
                    predictions_path: Model predictions.
                    label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                """
                from pathlib import Path

                import numpy
                import pandas
                import xgboost

                df = pandas.read_csv(
                    data_path,
                ).convert_dtypes()
                print("Evaluation data information:")
                df.info(verbose=True)
                # Converting column types that XGBoost does not support
                for column_name, dtype in df.dtypes.items():
                    if dtype in ["string", "object"]:
                        print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                        df[column_name] = df[column_name].astype("category")
                        print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                    # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                    if pandas.api.types.is_float_dtype(dtype):
                        # Converting from "Float64" to "float64"
                        df[column_name] = df[column_name].astype(dtype.name.lower())
                print("Final evaluation data information:")
                df.info(verbose=True)

                if label_column_name is not None:
                    df = df.drop(columns=[label_column_name])

                testing_data = xgboost.DMatrix(
                    data=df,
                    enable_categorical=True,
                )

                model = xgboost.Booster(model_file=model_path)

                predictions = model.predict(testing_data)

                Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                numpy.savetxt(predictions_path, predictions)

            import argparse
            _parser = argparse.ArgumentParser(prog='Xgboost predict on CSV', description='Makes predictions using a trained XGBoost model.')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = xgboost_predict_on_CSV(**_parsed_args)
          args:
          - --data
          - {inputPath: data}
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: label_column_name}
              then:
              - --label-column-name
              - {inputValue: label_column_name}
          - --predictions
          - {outputPath: predictions}
- name: Basics
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/c36d9a4893c3caae0d166d9e783af28fcd3cdfe4/components/basics/Format_date_time/component.yaml
    digest: 7784069b3bd1301674cfb4bbf7170b7db91ffd289138eb2d8838e3368158b70c
    text: |
      name: Format date and time
      inputs:
        - {name: Date, type: DateTime}
        - {name: Format, type: String, default: "%Y-%m-%d %H:%M:%S.%N", description: "Format string for date and time. See [man date](https://linux.die.net/man/1/date)."}
      outputs:
        - {name: Formatted date, type: String}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/basics/Format_date_time/component.yaml'
      implementation:
        container:
          image: ubuntu
          command:
            - sh
            - -exc
            - |
              date="$0"
              format="$1"
              output_path="$2"
              mkdir -p "$(dirname "$output_path")"

              date -d "$date" +"$format" >"$output_path"
            - {inputValue: Date}
            - {inputValue: Format}
            - {outputPath: Formatted date}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/37d98d43ad3193cf3516c134899f272d9643117c/components/basics/Calculate_hash/component.yaml
    digest: 6afc1b9d9c845fcdf0e9820aa97c9544c0f8b1ec2b7c1cf481975231711f6503
    text: |
      name: Calculate data hash
      inputs:
      - {name: Data}
      - {name: Hash algorithm, type: String, default: SHA256, description: "Hash algorithm to use. Supported values are MD5, SHA1, SHA256, SHA512, SHA3"}
      outputs:
      - {name: Hash}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/basics/Calculate_hash/component.yaml'
      implementation:
        container:
          image: alpine
          command:
          - sh
          - -exc
          - |
            data_path="$0"
            hash_algorithm="$1"
            hash_path="$2"
            mkdir -p "$(dirname "$hash_path")"

            hash_algorithm=$(echo "$hash_algorithm" | tr '[:upper:]' '[:lower:]')
            case "$hash_algorithm" in
                md5|sha1|sha256|sha512|sha3)  hash_program="${hash_algorithm}sum";;
                *)  echo "Unsupported hash algorithm $hash_algorithm"; exit 1;;
            esac

            if [ -d "$data_path" ]; then
                # Calculating hash for directory
                cd "$data_path"
                find . -type f -print0 |
                    sort -z |
                    xargs -0 "$hash_program" |
                    "$hash_program" |
                    cut -d ' ' -f 1 > "$hash_path"
            else
                # Calculating hash for file
                "$hash_program" "$data_path" |
                    cut -d ' ' -f 1 > "$hash_path"
            fi
          - {inputPath: Data}
          - {inputValue: Hash algorithm}
          - {outputPath: Hash}
  folders:
  - name: File system
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/605cefc151797369752b51530a91cc02e7f913f8/components/filesystem/create_directory/component.yaml
      digest: 6fa449465ecf656c02d07622acb5df045c3863acc15c607e1817f4e9f8078134
      text: |
        name: Create directory from files (5)
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/filesystem/create_directory/component.yaml"
        inputs:
          - { name: File 1 }
          - { name: File 1 name, type: String, default: "1" }
          - { name: File 2, optional: true }
          - { name: File 2 name, type: String, default: "2" }
          - { name: File 3, optional: true }
          - { name: File 3 name, type: String, default: "3" }
          - { name: File 4, optional: true }
          - { name: File 4 name, type: String, default: "4" }
          - { name: File 5, optional: true }
          - { name: File 5 name, type: String, default: "5" }
        outputs:
          - { name: Directory, type: Directory }
        implementation:
          container:
            image: alpine
            command:
              - sh
              - -ec
              - |
                output_path="$1"
                shift
                mkdir -p "$output_path"
                while [ "$#" -gt 0 ]; do
                  input_path="$1"
                  file_name="$2"
                  shift 2
                  cp -r "$input_path" "$output_path/$file_name"
                done
              - -c  # stabilize $0 vs $1
              - { outputPath: Directory }
              - { inputPath: File 1 }
              - { inputValue: File 1 name }
              - if:
                  cond: { isPresent: File 2 }
                  then:
                    - { inputPath: File 2 }
                    - { inputValue: File 2 name }
              - if:
                  cond: { isPresent: File 3 }
                  then:
                    - { inputPath: File 3 }
                    - { inputValue: File 3 name }
              - if:
                  cond: { isPresent: File 4 }
                  then:
                    - { inputPath: File 4 }
                    - { inputValue: File 4 name }
              - if:
                  cond: { isPresent: File 5 }
                  then:
                    - { inputPath: File 5 }
                    - { inputValue: File 5 name }
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/filesystem/get_file/component.yaml
      digest: 4415166532fa69baa33378631802fab630f51ea1677ce6408cef946672f4a9d2
      text: |
        name: Get file
        description: Get file from directory.
        inputs:
        - {name: Directory, type: Directory}
        - {name: Subpath, type: String}
        outputs:
        - {name: File}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/filesystem/get_file/component.yaml'
        implementation:
          container:
            image: alpine
            command:
            - sh
            - -ex
            - -c
            - |
              mkdir -p "$(dirname "$2")"
              cp -r "$0/$1" "$2"
            - inputPath: Directory
            - inputValue: Subpath
            - outputPath: File
- name: Datasets
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/57f780b15922061e59833541b71f3d099e710177/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml
    digest: 42030acab16b71bbc3ad018563b5aedb94d373e72f7cb2b322bc27dbbee75e1b
    text: |
      name: Chicago Taxi Trips dataset
      description: |
        City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew

        The input parameters configure the SQL query to the database.
        The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.
        Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml'
      inputs:
      - {name: Where, type: String, default: 'trip_start_timestamp>="1900-01-01" AND trip_start_timestamp<"2100-01-01"'}
      - {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}
      - {name: Select, type: String, default: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total'}
      - {name: Format, type: String, default: 'csv', description: 'Output data format. Supports csv,tsv,cml,rdf,json'}
      outputs:
      - {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}
      implementation:
        container:
          # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
          image: alpine/curl:8.14.1
          command:
          - sh
          - -c
          - |
            set -e -x -o pipefail
            output_path="$0"
            select="$1"
            where="$2"
            limit="$3"
            format="$4"
            mkdir -p "$(dirname "$output_path")"
            curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}" \
                --data-urlencode '$limit='"${limit}" \
                --data-urlencode '$where='"${where}" \
                --data-urlencode '$select='"${select}" \
                | sed -E 's/"([^",\]*)"/\1/g' > "$output_path"  # Removing unneeded quotes around all numbers
          - {outputPath: Table}
          - {inputValue: Select}
          - {inputValue: Where}
          - {inputValue: Limit}
          - {inputValue: Format}
- name: Data manipulation
  folders:
  - name: Parquet
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Select_columns/in_ApacheParquet_format/component.yaml
      digest: 7cf49b426385086fbb58b2d7c900637439f9dfff0df5661591cb6840b125d57e
      text: |
        name: Select columns using Pandas on ApacheParquet data
        description: Selects columns from a data table.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Select_columns/in_ApacheParquet_format/component.yaml'}
        inputs:
        - {name: table, type: ApacheParquet, description: Input data table.}
        - {name: column_names, type: JsonArray, description: Names of the columns to select
            from the table.}
        outputs:
        - {name: transformed_table, type: ApacheParquet, description: Transformed data table
            that only has the chosen columns.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def select_columns_using_Pandas_on_ApacheParquet_data(
                  table_path,
                  transformed_table_path,
                  column_names,
              ):
                  """Selects columns from a data table.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table that only has the chosen columns.
                      column_names: Names of the columns to select from the table.
                  """
                  import pandas

                  df = pandas.read_parquet(table_path)
                  df = df[column_names]
                  df.to_parquet(transformed_table_path, index=False)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Select columns using Pandas on ApacheParquet data', description='Selects columns from a data table.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = select_columns_using_Pandas_on_ApacheParquet_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-names
            - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Fill_all_missing_values/in_ApacheParquet_format/component.yaml
      digest: 0fd11307bd634351c521d4ae5d32f547e0a77304b17e2eab046e092debb3b2b4
      text: |
        name: Fill all missing values using Pandas on ApacheParquet data
        description: Fills the missing column items with the specified replacement value.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Fill_all_missing_values/in_ApacheParquet_format/component.yaml'}
        inputs:
        - {name: table, type: ApacheParquet, description: Input data table.}
        - {name: replacement_value, type: String, description: The value to use when replacing
            the missing items., default: '0', optional: true}
        - {name: column_names, type: JsonArray, description: Names of the columns where to
            perform the replacement., optional: true}
        outputs:
        - {name: transformed_table, type: ApacheParquet, description: Transformed data table
            where missing values are filed.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def fill_all_missing_values_using_Pandas_on_ApacheParquet_data(
                  table_path,
                  transformed_table_path,
                  replacement_value = "0",
                  column_names = None,
              ):
                  """Fills the missing column items with the specified replacement value.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table where missing values are filed.
                      replacement_value: The value to use when replacing the missing items.
                      column_names: Names of the columns where to perform the replacement.
                  """
                  import pandas

                  df = pandas.read_parquet(path=table_path)
                  for column_name in column_names or df.columns:
                      column = df[column_name]
                      # The `.astype` method does not work correctly on booleans
                      # So we need to special-case them
                      if pandas.api.types.is_bool_dtype(column.dtype):
                          if replacement_value.lower() in ("true", "1"):
                              converted_replacement_value = True
                          elif replacement_value.lower() in ("false", "0"):
                              converted_replacement_value = False
                          else:
                              raise ValueError(
                                  f"Cannot convert value '{replacement_value}' to boolean for column {column_name}."
                              )
                      else:
                          # Using Pandas to convert the replacement_value to column.dtype.
                          converted_replacement_value = pandas.Series(
                              replacement_value, dtype=column.dtype
                          ).tolist()[0]

                      print(
                          f"Filling missing values in column '{column_name}' with '{converted_replacement_value}'"
                      )
                      column.fillna(value=converted_replacement_value)

                  df.to_parquet(
                      path=transformed_table_path,
                      index=False,
                  )

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Fill all missing values using Pandas on ApacheParquet data', description='Fills the missing column items with the specified replacement value.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--replacement-value", dest="replacement_value", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = fill_all_missing_values_using_Pandas_on_ApacheParquet_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: replacement_value}
                then:
                - --replacement-value
                - {inputValue: replacement_value}
            - if:
                cond: {isPresent: column_names}
                then:
                - --column-names
                - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Binarize_column/in_ApacheParquet_format/component.yaml
      digest: 3a8d435090075b9064e62974a07baf316063b248b4c175d9e1f42bdf1bdd3439
      text: |
        name: Binarize column using Pandas on ApacheParquet data
        description: Transforms a table column into a binary class column using a predicate.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Binarize_column/in_ApacheParquet_format/component.yaml'}
        inputs:
        - {name: table, type: ApacheParquet, description: Input data table.}
        - {name: column_name, type: String, description: Name of the column to transform to
            binary class.}
        - {name: predicate, type: String, description: Expression that determines whether
            the column value is mapped to class 0 (false) or class 1 (true)., default: '>
            0', optional: true}
        - {name: new_column_name, type: String, description: Name for the new class column.
            Equals column_name by default., optional: true}
        - name: keep_original_column
          type: Boolean
          description: Whether to keep the original column (column_name) in the table.
          default: "False"
          optional: true
        outputs:
        - {name: transformed_table, type: ApacheParquet, description: Transformed data table
            with the binary class column.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def binarize_column_using_Pandas_on_ApacheParquet_data(
                  table_path,
                  transformed_table_path,
                  column_name,
                  predicate = "> 0",
                  new_column_name = None,
                  keep_original_column = False,
              ):
                  """Transforms a table column into a binary class column using a predicate.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table with the binary class column.
                      column_name: Name of the column to transform to binary class.
                      predicate: Expression that determines whether the column value is mapped to class 0 (false) or class 1 (true).
                      new_column_name: Name for the new class column. Equals column_name by default.
                      keep_original_column: Whether to keep the original column (column_name) in the table.
                  """
                  import pandas

                  df = pandas.read_parquet(path=table_path)
                  original_series = df[column_name]

                  # Dynamically executing the predicate code
                  # Variable namespace for code execution
                  namespace = dict(x=original_series)
                  # I though that there should be no space before `predicate` so that "dot" predicate methods like ".between(min, max)" work.
                  # However Python allows spaces before dot: `df .isna()`.
                  # So having a space is not a problem
                  transform_code = f"""new_series_boolean = x {predicate}"""
                  # Note: exec() takes no keyword arguments
                  # exec(__source=transform_code, __globals=namespace)
                  exec(transform_code, namespace)
                  new_series_boolean = namespace["new_series_boolean"]

                  # There are multiple ways to convert boolean column to integer.
                  # .apply(int) might be faster. https://stackoverflow.com/a/49804868/1497385
                  # TODO: Do a proper benchmark.
                  new_series = new_series_boolean.apply(int)
                  # new_series = new_series_boolean.astype(int)
                  # new_series = new_series_boolean.replace({False: 0, True: 1})

                  if new_column_name:
                      df.insert(loc=0, column=new_column_name, value=new_series)
                      if not keep_original_column:
                          df = df.drop(columns=[column_name])
                  else:
                      df[column_name] = new_series

                  df.to_parquet(path=transformed_table_path)

              def _deserialize_bool(s) -> bool:
                  from distutils.util import strtobool
                  return strtobool(s) == 1

              import argparse
              _parser = argparse.ArgumentParser(prog='Binarize column using Pandas on ApacheParquet data', description='Transforms a table column into a binary class column using a predicate.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-name", dest="column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--predicate", dest="predicate", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--new-column-name", dest="new_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--keep-original-column", dest="keep_original_column", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = binarize_column_using_Pandas_on_ApacheParquet_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-name
            - {inputValue: column_name}
            - if:
                cond: {isPresent: predicate}
                then:
                - --predicate
                - {inputValue: predicate}
            - if:
                cond: {isPresent: new_column_name}
                then:
                - --new-column-name
                - {inputValue: new_column_name}
            - if:
                cond: {isPresent: keep_original_column}
                then:
                - --keep-original-column
                - {inputValue: keep_original_column}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml
      digest: 45cad5c51cbebcc10c6c552709ba367e3f516f98a33dfaadd776737d69375885
      text: |
        name: Pandas Transform DataFrame in ApacheParquet format
        description: |-
          Transform DataFrame loaded from an ApacheParquet file.

              Inputs:
                  table: DataFrame to transform.
                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                      The DataFrame variable is called "df".
                      Examples:
                      - `df['prod'] = df['X'] * df['Y']`
                      - `df = df[['X', 'prod']]`
                      - `df.insert(0, "is_positive", df["X"] > 0)`

              Outputs:
                  transformed_table: Transformed DataFrame.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: table, type: ApacheParquet}
        - {name: transform_code, type: PythonCode}
        outputs:
        - {name: transformed_table, type: ApacheParquet}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.0.4' 'pyarrow==0.14.1' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' 'pyarrow==0.14.1' 'numpy<2'
              --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def Pandas_Transform_DataFrame_in_ApacheParquet_format(
                  table_path,
                  transformed_table_path,
                  transform_code,
              ):
                  '''Transform DataFrame loaded from an ApacheParquet file.

                  Inputs:
                      table: DataFrame to transform.
                      transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                          The DataFrame variable is called "df".
                          Examples:
                          - `df['prod'] = df['X'] * df['Y']`
                          - `df = df[['X', 'prod']]`
                          - `df.insert(0, "is_positive", df["X"] > 0)`

                  Outputs:
                      transformed_table: Transformed DataFrame.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import pandas

                  df = pandas.read_parquet(table_path)
                  # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                  namespace = locals()
                  exec(transform_code, namespace)
                  namespace['df'].to_parquet(transformed_table_path)

              import argparse
              _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in ApacheParquet format', description='Transform DataFrame loaded from an ApacheParquet file.\n\n    Inputs:\n        table: DataFrame to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed DataFrame.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = Pandas_Transform_DataFrame_in_ApacheParquet_format(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --transform-code
            - {inputValue: transform_code}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml
      digest: ca3834a0b29b0647f0e48f9d45419bb7ae05c6f4cd6fc98f4d99a28979915284
      text: |
        name: Split table into folds
        description: |-
          Splits the data table into the specified number of folds.

              The data is split into the specified number of folds k (default: 5).
              Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
              Each training subsample has (k-1)/k fraction of samples.
              The train_i subsample is produced by excluding test_i subsample form all samples.

              Inputs:
                  table: The data to split by rows
                  number_of_folds: Number of folds to split data into
                  random_seed: Random seed for reproducible splitting

              Outputs:
                  train_i: The i-th training subsample
                  test_i: The i-th testing subsample

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'
        inputs:
        - {name: table, type: CSV}
        - {name: number_of_folds, type: Integer, default: '5', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: train_1, type: CSV}
        - {name: train_2, type: CSV}
        - {name: train_3, type: CSV}
        - {name: train_4, type: CSV}
        - {name: train_5, type: CSV}
        - {name: test_1, type: CSV}
        - {name: test_2, type: CSV}
        - {name: test_3, type: CSV}
        - {name: test_4, type: CSV}
        - {name: test_5, type: CSV}
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def split_table_into_folds(
                  table_path,

                  train_1_path,
                  train_2_path,
                  train_3_path,
                  train_4_path,
                  train_5_path,

                  test_1_path,
                  test_2_path,
                  test_3_path,
                  test_4_path,
                  test_5_path,

                  number_of_folds = 5,
                  random_seed = 0,
              ):
                  """Splits the data table into the specified number of folds.

                  The data is split into the specified number of folds k (default: 5).
                  Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
                  Each training subsample has (k-1)/k fraction of samples.
                  The train_i subsample is produced by excluding test_i subsample form all samples.

                  Inputs:
                      table: The data to split by rows
                      number_of_folds: Number of folds to split data into
                      random_seed: Random seed for reproducible splitting

                  Outputs:
                      train_i: The i-th training subsample
                      test_i: The i-th testing subsample

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>

                  """
                  import pandas
                  from sklearn import model_selection

                  max_number_of_folds = 5

                  if number_of_folds < 1 or number_of_folds > max_number_of_folds:
                      raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )
                  splitter = model_selection.KFold(
                      n_splits=number_of_folds,
                      shuffle=True,
                      random_state=random_seed,
                  )
                  folds = list(splitter.split(df))

                  fold_paths = [
                      (train_1_path, test_1_path),
                      (train_2_path, test_2_path),
                      (train_3_path, test_3_path),
                      (train_4_path, test_4_path),
                      (train_5_path, test_5_path),
                  ]

                  for i in range(max_number_of_folds):
                      (train_path, test_path) = fold_paths[i]
                      if i < len(folds):
                          (train_indices, test_indices) = folds[i]
                          train_fold = df.iloc[train_indices]
                          test_fold = df.iloc[test_indices]
                      else:
                          train_fold = df.iloc[0:0]
                          test_fold = df.iloc[0:0]
                      train_fold.to_csv(train_path, index=False)
                      test_fold.to_csv(test_path, index=False)

              import argparse
              _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-folds", dest="number_of_folds", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--train-1", dest="train_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-2", dest="train_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-3", dest="train_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-4", dest="train_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-5", dest="train_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-1", dest="test_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-2", dest="test_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-3", dest="test_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-4", dest="test_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-5", dest="test_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = split_table_into_folds(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: number_of_folds}
                then:
                - --number-of-folds
                - {inputValue: number_of_folds}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --train-1
            - {outputPath: train_1}
            - --train-2
            - {outputPath: train_2}
            - --train-3
            - {outputPath: train_3}
            - --train-4
            - {outputPath: train_4}
            - --train-5
            - {outputPath: train_5}
            - --test-1
            - {outputPath: test_1}
            - --test-2
            - {outputPath: test_2}
            - --test-3
            - {outputPath: test_3}
            - --test-4
            - {outputPath: test_4}
            - --test-5
            - {outputPath: test_5}
  - name: CSV
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Select_columns/in_CSV_format/component.yaml
      digest: 7160345cf18c12fa2a5230c1ed5ceef9f2cc40b8b72d39995c72f99755622bec
      text: |
        name: Select columns using Pandas on CSV data
        description: Selects columns from a data table.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Select_columns/in_CSV_format/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: column_names, type: JsonArray, description: Names of the columns to select
            from the table.}
        outputs:
        - {name: transformed_table, type: CSV, description: Transformed data table that only
            has the chosen columns.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.2' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.2' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def select_columns_using_Pandas_on_CSV_data(
                  table_path,
                  transformed_table_path,
                  column_names,
              ):
                  """Selects columns from a data table.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table that only has the chosen columns.
                      column_names: Names of the columns to select from the table.
                  """
                  import pandas

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )
                  df = df[column_names]
                  df.to_csv(transformed_table_path, index=False)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Select columns using Pandas on CSV data', description='Selects columns from a data table.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = select_columns_using_Pandas_on_CSV_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-names
            - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml
      digest: c873de1f3a95bb7fb17efbbc5a72c4a13a1ef581dc162054707441ddce2c06d2
      text: |
        name: Fill all missing values using Pandas on CSV data
        description: Fills the missing column items with the specified replacement value.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: replacement_value, type: String, description: The value to use when replacing
            the missing items., default: '0', optional: true}
        - {name: column_names, type: JsonArray, description: Names of the columns where to
            perform the replacement., optional: true}
        outputs:
        - {name: transformed_table, type: CSV, description: Transformed data table where missing
            values are filed.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.1' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.1' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def fill_all_missing_values_using_Pandas_on_CSV_data(
                  table_path,
                  transformed_table_path,
                  replacement_value = "0",
                  column_names = None,
              ):
                  """Fills the missing column items with the specified replacement value.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table where missing values are filed.
                      replacement_value: The value to use when replacing the missing items.
                      column_names: Names of the columns where to perform the replacement.
                  """
                  import pandas

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )

                  for column_name in column_names or df.columns:
                      df[column_name] = df[column_name].fillna(value=replacement_value)

                  df.to_csv(
                      transformed_table_path, index=False,
                  )

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Fill all missing values using Pandas on CSV data', description='Fills the missing column items with the specified replacement value.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--replacement-value", dest="replacement_value", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = fill_all_missing_values_using_Pandas_on_CSV_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: replacement_value}
                then:
                - --replacement-value
                - {inputValue: replacement_value}
            - if:
                cond: {isPresent: column_names}
                then:
                - --column-names
                - {inputValue: column_names}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Binarize_column/in_CSV_format/component.yaml
      digest: 98e0babb5e2ac915e5c8969b76f61d8e7b78637aef66f7fb508a8f3f35d63cf1
      text: |
        name: Binarize column using Pandas on CSV data
        description: Transforms a table column into a binary class column using a predicate.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Binarize_column/in_CSV_format/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: column_name, type: String, description: Name of the column to transform to
            binary class.}
        - {name: predicate, type: String, description: Expression that determines whether
            the column value is mapped to class 0 (false) or class 1 (true)., default: '>
            0', optional: true}
        - {name: new_column_name, type: String, description: Name for the new class column.
            Equals column_name by default., optional: true}
        - name: keep_original_column
          type: Boolean
          description: Whether to keep the original column (column_name) in the table.
          default: "False"
          optional: true
        outputs:
        - {name: transformed_table, type: CSV, description: Transformed data table with the
            binary class column.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def binarize_column_using_Pandas_on_CSV_data(
                  table_path,
                  transformed_table_path,
                  column_name,
                  predicate = "> 0",
                  new_column_name = None,
                  keep_original_column = False,
              ):
                  """Transforms a table column into a binary class column using a predicate.

                  Args:
                      table_path: Input data table.
                      transformed_table_path: Transformed data table with the binary class column.
                      column_name: Name of the column to transform to binary class.
                      predicate: Expression that determines whether the column value is mapped to class 0 (false) or class 1 (true).
                      new_column_name: Name for the new class column. Equals column_name by default.
                      keep_original_column: Whether to keep the original column (column_name) in the table.
                  """
                  import pandas

                  df = pandas.read_csv(table_path).convert_dtypes()
                  original_series = df[column_name]

                  # Dynamically executing the predicate code
                  # Variable namespace for code execution
                  namespace = dict(x=original_series)
                  # I though that there should be no space before `predicate` so that "dot" predicate methods like ".between(min, max)" work.
                  # However Python allows spaces before dot: `df .isna()`.
                  # So having a space is not a problem
                  transform_code = f"""new_series_boolean = x {predicate}"""
                  # Note: exec() takes no keyword arguments
                  # exec(__source=transform_code, __globals=namespace)
                  exec(transform_code, namespace)
                  new_series_boolean = namespace["new_series_boolean"]

                  # There are multiple ways to convert boolean column to integer.
                  # .apply(int) might be faster. https://stackoverflow.com/a/49804868/1497385
                  # TODO: Do a proper benchmark.
                  new_series = new_series_boolean.apply(int)
                  # new_series = new_series_boolean.astype(int)
                  # new_series = new_series_boolean.replace({False: 0, True: 1})

                  if new_column_name:
                      df.insert(loc=0, column=new_column_name, value=new_series)
                      if not keep_original_column:
                          df = df.drop(columns=[column_name])
                  else:
                      df[column_name] = new_series

                  df.to_csv(transformed_table_path, index=False)

              def _deserialize_bool(s) -> bool:
                  from distutils.util import strtobool
                  return strtobool(s) == 1

              import argparse
              _parser = argparse.ArgumentParser(prog='Binarize column using Pandas on CSV data', description='Transforms a table column into a binary class column using a predicate.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--column-name", dest="column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--predicate", dest="predicate", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--new-column-name", dest="new_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--keep-original-column", dest="keep_original_column", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = binarize_column_using_Pandas_on_CSV_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --column-name
            - {inputValue: column_name}
            - if:
                cond: {isPresent: predicate}
                then:
                - --predicate
                - {inputValue: predicate}
            - if:
                cond: {isPresent: new_column_name}
                then:
                - --new-column-name
                - {inputValue: new_column_name}
            - if:
                cond: {isPresent: keep_original_column}
                then:
                - --keep-original-column
                - {inputValue: keep_original_column}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml
      digest: 6cf3a8e4fedec9f5e92ade9616b06c0717cc7f7622f2da8c6c646d2bed17fa00
      text: |
        name: Pandas Transform DataFrame in CSV format
        description: |-
          Transform DataFrame loaded from a CSV file.

              Inputs:
                  table: Table to transform.
                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                      The DataFrame variable is called "df".
                      Examples:
                      - `df['prod'] = df['X'] * df['Y']`
                      - `df = df[['X', 'prod']]`
                      - `df.insert(0, "is_positive", df["X"] > 0)`

              Outputs:
                  transformed_table: Transformed table.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: table, type: CSV}
        - {name: transform_code, type: PythonCode}
        outputs:
        - {name: transformed_table, type: CSV}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def Pandas_Transform_DataFrame_in_CSV_format(
                  table_path,
                  transformed_table_path,
                  transform_code,
              ):
                  '''Transform DataFrame loaded from a CSV file.

                  Inputs:
                      table: Table to transform.
                      transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                          The DataFrame variable is called "df".
                          Examples:
                          - `df['prod'] = df['X'] * df['Y']`
                          - `df = df[['X', 'prod']]`
                          - `df.insert(0, "is_positive", df["X"] > 0)`

                  Outputs:
                      transformed_table: Transformed table.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import pandas

                  df = pandas.read_csv(
                      table_path,
                  ).convert_dtypes()
                  # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                  namespace = locals()
                  exec(transform_code, namespace)
                  namespace['df'].to_csv(
                      transformed_table_path,
                      index=False,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --transform-code
            - {inputValue: transform_code}
            - --transformed-table
            - {outputPath: transformed_table}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml
      digest: ca3834a0b29b0647f0e48f9d45419bb7ae05c6f4cd6fc98f4d99a28979915284
      text: |
        name: Split table into folds
        description: |-
          Splits the data table into the specified number of folds.

              The data is split into the specified number of folds k (default: 5).
              Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
              Each training subsample has (k-1)/k fraction of samples.
              The train_i subsample is produced by excluding test_i subsample form all samples.

              Inputs:
                  table: The data to split by rows
                  number_of_folds: Number of folds to split data into
                  random_seed: Random seed for reproducible splitting

              Outputs:
                  train_i: The i-th training subsample
                  test_i: The i-th testing subsample

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'
        inputs:
        - {name: table, type: CSV}
        - {name: number_of_folds, type: Integer, default: '5', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: train_1, type: CSV}
        - {name: train_2, type: CSV}
        - {name: train_3, type: CSV}
        - {name: train_4, type: CSV}
        - {name: train_5, type: CSV}
        - {name: test_1, type: CSV}
        - {name: test_2, type: CSV}
        - {name: test_3, type: CSV}
        - {name: test_4, type: CSV}
        - {name: test_5, type: CSV}
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def split_table_into_folds(
                  table_path,

                  train_1_path,
                  train_2_path,
                  train_3_path,
                  train_4_path,
                  train_5_path,

                  test_1_path,
                  test_2_path,
                  test_3_path,
                  test_4_path,
                  test_5_path,

                  number_of_folds = 5,
                  random_seed = 0,
              ):
                  """Splits the data table into the specified number of folds.

                  The data is split into the specified number of folds k (default: 5).
                  Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
                  Each training subsample has (k-1)/k fraction of samples.
                  The train_i subsample is produced by excluding test_i subsample form all samples.

                  Inputs:
                      table: The data to split by rows
                      number_of_folds: Number of folds to split data into
                      random_seed: Random seed for reproducible splitting

                  Outputs:
                      train_i: The i-th training subsample
                      test_i: The i-th testing subsample

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>

                  """
                  import pandas
                  from sklearn import model_selection

                  max_number_of_folds = 5

                  if number_of_folds < 1 or number_of_folds > max_number_of_folds:
                      raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))

                  df = pandas.read_csv(
                      table_path,
                      dtype="string",
                  )
                  splitter = model_selection.KFold(
                      n_splits=number_of_folds,
                      shuffle=True,
                      random_state=random_seed,
                  )
                  folds = list(splitter.split(df))

                  fold_paths = [
                      (train_1_path, test_1_path),
                      (train_2_path, test_2_path),
                      (train_3_path, test_3_path),
                      (train_4_path, test_4_path),
                      (train_5_path, test_5_path),
                  ]

                  for i in range(max_number_of_folds):
                      (train_path, test_path) = fold_paths[i]
                      if i < len(folds):
                          (train_indices, test_indices) = folds[i]
                          train_fold = df.iloc[train_indices]
                          test_fold = df.iloc[test_indices]
                      else:
                          train_fold = df.iloc[0:0]
                          test_fold = df.iloc[0:0]
                      train_fold.to_csv(train_path, index=False)
                      test_fold.to_csv(test_path, index=False)

              import argparse
              _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-folds", dest="number_of_folds", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--train-1", dest="train_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-2", dest="train_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-3", dest="train_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-4", dest="train_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-5", dest="train_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-1", dest="test_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-2", dest="test_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-3", dest="test_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-4", dest="test_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--test-5", dest="test_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = split_table_into_folds(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - if:
                cond: {isPresent: number_of_folds}
                then:
                - --number-of-folds
                - {inputValue: number_of_folds}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --train-1
            - {outputPath: train_1}
            - --train-2
            - {outputPath: train_2}
            - --train-3
            - {outputPath: train_3}
            - --train-4
            - {outputPath: train_4}
            - --train-5
            - {outputPath: train_5}
            - --test-1
            - {outputPath: test_1}
            - --test-2
            - {outputPath: test_2}
            - --test-3
            - {outputPath: test_3}
            - --test-4
            - {outputPath: test_4}
            - --test-5
            - {outputPath: test_5}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml
      digest: 1d1b505ec8a538347e8c0db84a92f76ebae55c5ef85ff3e91bc1d7eb7a86dc06
      text: |
        name: Split rows into subsets
        description: Splits the data table according to the split fractions.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml'}
        inputs:
        - {name: table, type: CSV, description: Input data table.}
        - {name: fraction_1, type: Float, description: 'The proportion of the lines to put
            into the 1st split. Range: [0, 1]'}
        - name: fraction_2
          type: Float
          description: |-
            The proportion of the lines to put into the 2nd split. Range: [0, 1]
            If fraction_2 is not specified, then fraction_2 = 1 - fraction_1.
            The remaining lines go to the 3rd split (if any).
          optional: true
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: split_1, type: CSV, description: Subset of the data table.}
        - {name: split_2, type: CSV, description: Subset of the data table.}
        - {name: split_3, type: CSV, description: Subset of the data table.}
        - {name: split_1_count, type: Integer}
        - {name: split_2_count, type: Integer}
        - {name: split_3_count, type: Integer}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def split_rows_into_subsets(
                  table_path,
                  split_1_path,
                  split_2_path,
                  split_3_path,
                  fraction_1,
                  fraction_2 = None,
                  random_seed = 0,
              ):
                  """Splits the data table according to the split fractions.

                  Args:
                      table_path: Input data table.
                      split_1_path: Subset of the data table.
                      split_2_path: Subset of the data table.
                      split_3_path: Subset of the data table.
                      fraction_1: The proportion of the lines to put into the 1st split. Range: [0, 1]
                      fraction_2: The proportion of the lines to put into the 2nd split. Range: [0, 1]
                          If fraction_2 is not specified, then fraction_2 = 1 - fraction_1.
                          The remaining lines go to the 3rd split (if any).
                      random_seed: Controls the seed of the random processes.
                  """
                  import random

                  random.seed(random_seed)

                  SHUFFLE_BUFFER_SIZE = 10000

                  num_splits = 3

                  if fraction_1 < 0 or fraction_1 > 1:
                      raise ValueError("fraction_1 must be in between 0 and 1.")

                  if fraction_2 is None:
                      fraction_2 = 1 - fraction_1
                  if fraction_2 < 0 or fraction_2 > 1:
                      raise ValueError("fraction_2 must be in between 0 and 1.")

                  fraction_3 = 1 - fraction_1 - fraction_2

                  fractions = [
                      fraction_1,
                      fraction_2,
                      fraction_3,
                  ]

                  assert sum(fractions) == 1

                  written_line_counts = [0] * num_splits

                  output_files = [
                      open(split_1_path, "wb"),
                      open(split_2_path, "wb"),
                      open(split_3_path, "wb"),
                  ]

                  with open(table_path, "rb") as input_file:
                      # Writing the headers
                      header_line = input_file.readline()
                      for output_file in output_files:
                          output_file.write(header_line)

                      while True:
                          line_buffer = []
                          for i in range(SHUFFLE_BUFFER_SIZE):
                              line = input_file.readline()
                              if not line:
                                  break
                              line_buffer.append(line)

                          # We need to exactly partition the lines between the output files
                          # To overcome possible systematic bias, we could calculate the total numbers
                          # of lines written to each file and take that into account.
                          num_read_lines = len(line_buffer)
                          number_of_lines_for_files = [0] * num_splits
                          # List that will have the index of the destination file for each line
                          file_index_for_line = []
                          remaining_lines = num_read_lines
                          remaining_fraction = 1
                          for i in range(num_splits):
                              number_of_lines_for_file = (
                                  round(remaining_lines * (fractions[i] / remaining_fraction))
                                  if remaining_fraction > 0
                                  else 0
                              )
                              number_of_lines_for_files[i] = number_of_lines_for_file
                              remaining_lines -= number_of_lines_for_file
                              remaining_fraction -= fractions[i]
                              file_index_for_line.extend([i] * number_of_lines_for_file)

                          assert remaining_lines == 0, f"{remaining_lines}"
                          assert len(file_index_for_line) == num_read_lines

                          random.shuffle(file_index_for_line)

                          for i in range(num_read_lines):
                              output_files[file_index_for_line[i]].write(line_buffer[i])
                              written_line_counts[file_index_for_line[i]] += 1

                          # Exit if the file ended before we were able to fully fill the buffer
                          if len(line_buffer) != SHUFFLE_BUFFER_SIZE:
                              break

                  for output_file in output_files:
                      output_file.close()

                  return written_line_counts

              def _serialize_int(int_value: int) -> str:
                  if isinstance(int_value, str):
                      return int_value
                  if not isinstance(int_value, int):
                      raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                  return str(int_value)

              import argparse
              _parser = argparse.ArgumentParser(prog='Split rows into subsets', description='Splits the data table according to the split fractions.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--fraction-1", dest="fraction_1", type=float, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--fraction-2", dest="fraction_2", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--split-1", dest="split_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--split-2", dest="split_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--split-3", dest="split_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = split_rows_into_subsets(**_parsed_args)

              _output_serializers = [
                  _serialize_int,
                  _serialize_int,
                  _serialize_int,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - --table
            - {inputPath: table}
            - --fraction-1
            - {inputValue: fraction_1}
            - if:
                cond: {isPresent: fraction_2}
                then:
                - --fraction-2
                - {inputValue: fraction_2}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --split-1
            - {outputPath: split_1}
            - --split-2
            - {outputPath: split_2}
            - --split-3
            - {outputPath: split_3}
            - '----output-paths'
            - {outputPath: split_1_count}
            - {outputPath: split_2_count}
            - {outputPath: split_3_count}
  - name: JSON lines
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_JsonLines_format/component.yaml
      digest: a16b7f91eaf2eecf351c2775d4d33c960457257a90025ac058313946a3c7fc44
      text: |
        name: Transform using Pandas DataFrame on JsonLines data
        description: Transform DataFrame loaded from an ApacheParquet file.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_JsonLines_format/component.yaml'}
        inputs:
        - {name: table, type: JsonLines, description: DataFrame to transform.}
        - name: transform_code
          type: PythonCode
          description: |-
            Transformation code. Code is written in Python and can consist of multiple lines.
            The DataFrame variable is called "df".
            Examples:
            - `df['prod'] = df['X'] * df['Y']`
            - `df = df[['X', 'prod']]`
            - `df.insert(0, "is_positive", df["X"] > 0)`
        outputs:
        - {name: transformed_table, type: JsonLines, description: Transformed DataFrame.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def transform_using_Pandas_DataFrame_on_JsonLines_data(
                  table_path,
                  transformed_table_path,
                  transform_code,
              ):
                  """Transform DataFrame loaded from an ApacheParquet file.

                  Args:
                      table_path: DataFrame to transform.
                      transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                          The DataFrame variable is called "df".
                          Examples:
                          - `df['prod'] = df['X'] * df['Y']`
                          - `df = df[['X', 'prod']]`
                          - `df.insert(0, "is_positive", df["X"] > 0)`
                      transformed_table_path: Transformed DataFrame.
                  """
                  import pandas

                  df = pandas.read_json(path_or_buf=table_path, lines=True)
                  # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                  namespace = dict(df=df)
                  exec(transform_code, namespace)
                  df = namespace["df"]
                  df.to_json(path_or_buf=transformed_table_path, orient="records", lines=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Transform using Pandas DataFrame on JsonLines data', description='Transform DataFrame loaded from an ApacheParquet file.')
              _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = transform_using_Pandas_DataFrame_on_JsonLines_data(**_parsed_args)
            args:
            - --table
            - {inputPath: table}
            - --transform-code
            - {inputValue: transform_code}
            - --transformed-table
            - {outputPath: transformed_table}
  - name: JSON
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_index/component.yaml
      digest: c7b09bd3bf9cf9e42e22bedcc4822c32c31f81f92e45c019f9cca626961cf2a2
      text: |
        name: Get element by index from JSON
        inputs:
        - {name: Json}
        - {name: Index, type: Integer}
        outputs:
        - {name: Output}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_index/component.yaml'
        implementation:
          container:
            image: stedolan/jq:latest
            command:
            - sh
            - -exc
            - |
              input_path=$0
              output_path=$1
              index=$2
              mkdir -p "$(dirname "$output_path")"
              < "$input_path" jq --raw-output --join-output .["$index"] > "$output_path"
            - {inputPath: Json}
            - {outputPath: Output}
            - {inputValue: Index}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_key/component.yaml
      digest: 2790d7a0b9983e3b9ddca1fb02b243a7b3e0280198cd859438d622ad1ca8d3b0
      text: |
        name: Get element by key from JSON
        inputs:
        - {name: Json}
        - {name: Key, type: String}
        outputs:
        - {name: Output}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_key/component.yaml'
        implementation:
          container:
            image: stedolan/jq:latest
            command:
            - sh
            - -exc
            - |
              input_path=$0
              output_path=$1
              key=$2
              mkdir -p "$(dirname "$output_path")"
              < "$input_path" jq --raw-output --join-output '.["'"$key"'"]' > "$output_path"
            - {inputPath: Json}
            - {outputPath: Output}
            - {inputValue: Key}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Query/component.yaml
      digest: 879a8ddf73f5bec05643901f88b024f82c83cf1443536b98a624331a89d8a9f5
      text: |
        name: Query JSON using JQ
        inputs:
        - {name: Json}
        - {name: Query, type: String}
        - {name: Options, type: String, default: '--raw-output'}
        outputs:
        - {name: Output}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Query/component.yaml'
        implementation:
          container:
            image: stedolan/jq:latest
            command:
            - sh
            - -exc
            - |
              input_path=$0
              output_path=$1
              query=$2
              options=$3
              mkdir -p "$(dirname "$output_path")"
              < "$input_path" jq $options "$query" > "$output_path"
            - {inputPath: Json}
            - {outputPath: Output}
            - {inputValue: Query}
            - {inputValue: Options}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_dict/component.yaml
      digest: a3d12264c5d5cd52900d2a8d16c96630954de1762c0000103c999a0d48396a7a
      text: |
        name: Build dict
        description: Creates a JSON object from multiple key and value pairs.
        inputs:
        - {name: key_1, type: String, optional: true}
        - {name: value_1, type: JsonObject, optional: true}
        - {name: key_2, type: String, optional: true}
        - {name: value_2, type: JsonObject, optional: true}
        - {name: key_3, type: String, optional: true}
        - {name: value_3, type: JsonObject, optional: true}
        - {name: key_4, type: String, optional: true}
        - {name: value_4, type: JsonObject, optional: true}
        - {name: key_5, type: String, optional: true}
        - {name: value_5, type: JsonObject, optional: true}
        outputs:
        - {name: Output, type: JsonObject}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_dict/component.yaml'
        implementation:
          container:
            image: python:3.8
            command:
            - python3
            - -u
            - -c
            - |
              def build_dict(
                  key_1 = None,
                  value_1 = None,
                  key_2 = None,
                  value_2 = None,
                  key_3 = None,
                  value_3 = None,
                  key_4 = None,
                  value_4 = None,
                  key_5 = None,
                  value_5 = None,
              ):
                  """Creates a JSON object from multiple key and value pairs.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = dict([
                      (key_1, value_1),
                      (key_2, value_2),
                      (key_3, value_3),
                      (key_4, value_4),
                      (key_5, value_5),
                  ])
                  if None in result:
                      del result[None]
                  return result

              import json
              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build dict', description='Creates a JSON object from multiple key and value pairs.')
              _parser.add_argument("--key-1", dest="key_1", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-1", dest="value_1", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-2", dest="key_2", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-2", dest="value_2", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-3", dest="key_3", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-3", dest="value_3", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-4", dest="key_4", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-4", dest="value_4", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--key-5", dest="key_5", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--value-5", dest="value_5", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_dict(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: key_1}
                then:
                - --key-1
                - {inputValue: key_1}
            - if:
                cond: {isPresent: value_1}
                then:
                - --value-1
                - {inputValue: value_1}
            - if:
                cond: {isPresent: key_2}
                then:
                - --key-2
                - {inputValue: key_2}
            - if:
                cond: {isPresent: value_2}
                then:
                - --value-2
                - {inputValue: value_2}
            - if:
                cond: {isPresent: key_3}
                then:
                - --key-3
                - {inputValue: key_3}
            - if:
                cond: {isPresent: value_3}
                then:
                - --value-3
                - {inputValue: value_3}
            - if:
                cond: {isPresent: key_4}
                then:
                - --key-4
                - {inputValue: key_4}
            - if:
                cond: {isPresent: value_4}
                then:
                - --value-4
                - {inputValue: value_4}
            - if:
                cond: {isPresent: key_5}
                then:
                - --key-5
                - {inputValue: key_5}
            - if:
                cond: {isPresent: value_5}
                then:
                - --value-5
                - {inputValue: value_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_list/component.yaml
      digest: 24b33123e1cb13712aec208e81e6c1dc2d6d643c8a1fe323581e8858c3c8993a
      text: |
        name: Build list
        description: Creates a JSON array from multiple items.
        inputs:
        - {name: item_1, type: JsonObject, optional: true}
        - {name: item_2, type: JsonObject, optional: true}
        - {name: item_3, type: JsonObject, optional: true}
        - {name: item_4, type: JsonObject, optional: true}
        - {name: item_5, type: JsonObject, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list/component.yaml'
        implementation:
          container:
            image: python:3.8
            command:
            - python3
            - -u
            - -c
            - |
              def build_list(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple items.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              import json
              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list', description='Creates a JSON array from multiple items.')
              _parser.add_argument("--item-1", dest="item_1", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/aecac18d4023c73c561d7f21192253e9593b9932/components/json/Build_list_of_strings/component.yaml
      digest: 76963afe607da688f1334e8cff2017da891f92609855d6103ccbab627c6eb2ef
      text: |
        name: Build list of strings
        description: Creates a JSON array from multiple strings.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_strings/component.yaml'}
        inputs:
        - {name: item_1, type: String, optional: true}
        - {name: item_2, type: String, optional: true}
        - {name: item_3, type: String, optional: true}
        - {name: item_4, type: String, optional: true}
        - {name: item_5, type: String, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def build_list_of_strings(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple strings.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list of strings', description='Creates a JSON array from multiple strings.')
              _parser.add_argument("--item-1", dest="item_1", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list_of_strings(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_integers/component.yaml
      digest: 965f5a4f2f0a34f07419ab5d60cc3c3505ab2d7579baa0dd36d9a32ea97a7ffa
      text: |
        name: Build list of integers
        description: Creates a JSON array from multiple integer numbers.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_integers/component.yaml'}
        inputs:
        - {name: item_1, type: Integer, optional: true}
        - {name: item_2, type: Integer, optional: true}
        - {name: item_3, type: Integer, optional: true}
        - {name: item_4, type: Integer, optional: true}
        - {name: item_5, type: Integer, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        implementation:
          container:
            image: python:3.8
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def build_list_of_integers(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple integer numbers.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list of integers', description='Creates a JSON array from multiple integer numbers.')
              _parser.add_argument("--item-1", dest="item_1", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list_of_integers(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_floats/component.yaml
      digest: b3ad1eeb79066b286809fe4a1b078aee08bbadd07d977b5327880e854bec1ff2
      text: |
        name: Build list of floats
        description: Creates a JSON array from multiple floating-point numbers.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_floats/component.yaml'}
        inputs:
        - {name: item_1, type: Float, optional: true}
        - {name: item_2, type: Float, optional: true}
        - {name: item_3, type: Float, optional: true}
        - {name: item_4, type: Float, optional: true}
        - {name: item_5, type: Float, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        implementation:
          container:
            image: python:3.8
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def build_list_of_floats(
                  item_1 = None,
                  item_2 = None,
                  item_3 = None,
                  item_4 = None,
                  item_5 = None,
              ):
                  """Creates a JSON array from multiple floating-point numbers.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for item in [item_1, item_2, item_3, item_4, item_5]:
                      if item is not None:
                          result.append(item)
                  return result

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Build list of floats', description='Creates a JSON array from multiple floating-point numbers.')
              _parser.add_argument("--item-1", dest="item_1", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-2", dest="item_2", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-3", dest="item_3", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-4", dest="item_4", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--item-5", dest="item_5", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = build_list_of_floats(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: item_1}
                then:
                - --item-1
                - {inputValue: item_1}
            - if:
                cond: {isPresent: item_2}
                then:
                - --item-2
                - {inputValue: item_2}
            - if:
                cond: {isPresent: item_3}
                then:
                - --item-3
                - {inputValue: item_3}
            - if:
                cond: {isPresent: item_4}
                then:
                - --item-4
                - {inputValue: item_4}
            - if:
                cond: {isPresent: item_5}
                then:
                - --item-5
                - {inputValue: item_5}
            - '----output-paths'
            - {outputPath: Output}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Combine_lists/component.yaml
      digest: f2b024570631dd504413a7836297d02067423ff9d64b62ebc76be1a943488f4a
      text: |
        name: Combine lists
        description: Combines multiple JSON arrays into one.
        inputs:
        - {name: list_1, type: JsonArray, optional: true}
        - {name: list_2, type: JsonArray, optional: true}
        - {name: list_3, type: JsonArray, optional: true}
        - {name: list_4, type: JsonArray, optional: true}
        - {name: list_5, type: JsonArray, optional: true}
        outputs:
        - {name: Output, type: JsonArray}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Combine_lists/component.yaml'
        implementation:
          container:
            image: python:3.8
            command:
            - python3
            - -u
            - -c
            - |
              def combine_lists(
                  list_1 = None,
                  list_2 = None,
                  list_3 = None,
                  list_4 = None,
                  list_5 = None,
              ):
                  """Combines multiple JSON arrays into one.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  result = []
                  for list in [list_1, list_2, list_3, list_4, list_5]:
                      if list is not None:
                          result.extend(list)
                  return result

              import json
              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Combine lists', description='Combines multiple JSON arrays into one.')
              _parser.add_argument("--list-1", dest="list_1", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-2", dest="list_2", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-3", dest="list_3", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-4", dest="list_4", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--list-5", dest="list_5", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = combine_lists(**_parsed_args)

              _outputs = [_outputs]

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - if:
                cond: {isPresent: list_1}
                then:
                - --list-1
                - {inputValue: list_1}
            - if:
                cond: {isPresent: list_2}
                then:
                - --list-2
                - {inputValue: list_2}
            - if:
                cond: {isPresent: list_3}
                then:
                - --list-3
                - {inputValue: list_3}
            - if:
                cond: {isPresent: list_4}
                then:
                - --list-4
                - {inputValue: list_4}
            - if:
                cond: {isPresent: list_5}
                then:
                - --list-5
                - {inputValue: list_5}
            - '----output-paths'
            - {outputPath: Output}
- name: Upload/Download
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/f328b3f4842838b6578b701d975b22f7908aca48/components/web/Download/component.yaml
    digest: 44c3cf1caa233e1d9057a25181ce622359643fc9345140b84556b00c8042ffb4
    text: |
      name: Download data
      inputs:
      - {name: Url, type: URI}
      - {name: curl options, type: string, default: '--location', description: 'Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html'}
      outputs:
      - {name: Data}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml'
      implementation:
        container:
          # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
          image: alpine/curl:8.14.1
          command:
          - sh
          - -exc
          - |
            url="$0"
            output_path="$1"
            curl_options="$2"

            mkdir -p "$(dirname "$output_path")"
            curl --get "$url" --output "$output_path" $curl_options
          - inputValue: Url
          - outputPath: Data
          - inputValue: curl options
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml
    digest: 30c424ac6156c478aa0c3027b470baf9cb7dbbf90aebcabde7469bfbd02a512e
    text: |
      name: Download from GCS
      inputs:
      - {name: GCS path, type: URI}
      outputs:
      - {name: Data}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download/component.yaml'
      implementation:
          container:
              image: google/cloud-sdk
              command:
              - bash # Pattern comparison only works in Bash
              - -ex
              - -c
              - |
                  if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                      gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                  fi

                  uri="$0"
                  output_path="$1"

                  # Checking whether the URI points to a single blob, a directory or a URI pattern
                  # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI
                  if [[ "$uri" != */ ]] && (gsutil ls "$uri" | grep --fixed-strings --line-regexp "$uri"); then
                      mkdir -p "$(dirname "$output_path")"
                      gsutil -m cp -r "$uri" "$output_path"
                  else
                      mkdir -p "$output_path" # When source path is a directory, gsutil requires the destination to also be a directory
                      gsutil -m rsync -r "$uri" "$output_path" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.
                  fi
              - inputValue: GCS path
              - outputPath: Data
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml
    digest: 074b5c68679117f5aeb41e36ee75b8c349ec88631ba552cbf6991bd5942d9192
    text: |-
      name: Upload to GCS with unique name
      description: Upload to GCS with unique URI suffix
      inputs:
      - {name: Data}
      - {name: GCS path prefix, type: URI}
      outputs:
      - {name: GCS path, type: String}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_unique_uri/component.yaml'
      implementation:
          container:
              image: google/cloud-sdk
              command:
              - sh
              - -ex
              - -c
              - |
                  data_path="$0"
                  url_prefix="$1"
                  output_path="$2"
                  random_string=$(< dev/urandom tr -dc A-Za-z0-9 | head -c 64)
                  uri="${url_prefix}${random_string}"
                  if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                      gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                  fi
                  gsutil cp -r "$data_path" "$uri"
                  mkdir -p "$(dirname "$output_path")"
                  printf "%s" "$uri" > "$output_path"
              - inputPath: Data
              - {inputValue: GCS path prefix}
              - outputPath: GCS path
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml
    digest: e3cfa607ab9e2e0312ef4268dad157edf30c3d4ba5059b2e295fe047258aa31d
    text: |
      name: Upload to GCS
      inputs:
      - {name: Data}
      - {name: GCS path, type: URI}
      outputs:
      - {name: GCS path, type: String}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'
      implementation:
          container:
              image: google/cloud-sdk
              command:
              - sh
              - -ex
              - -c
              - |
                  if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                      gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                  fi
                  gsutil cp -r "$0" "$1"
                  mkdir -p "$(dirname "$2")"
                  printf "%s" "$1" > "$2"
              - inputPath: Data
              - inputValue: GCS path
              - outputPath: GCS path
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/cca2d8569d01b527df10c629258be04d52eacc43/components/Download_and_upload/IPFS/Download/component.yaml
    digest: 6fedde78a50fcece87df20eeba865d6f4a2c2513d514ff1386efe2e0d1445421
    text: |
      name: Download data from IPFS
      description: |
        Download data from IPFS using ipget.
        See https://ipfs.io/
        See https://github.com/ipfs/ipget
      inputs:
      - {name: Path, type: String, description: "IPFS Path. Example: /ipfs/QmWATWQ7fVPP2EFGu71UkfnqhYXDYH566qy47CnJDgvs8u"}
      outputs:
      - {name: Data}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/Download_and_upload/IPFS/Download/component.yaml"
      implementation:
        container:
          image: alpine
          command:
          - sh
          - -exc
          - |
            path=$0
            output_data_path=$1
            mkdir -p "$(dirname "$output_data_path")"

            wget https://dist.ipfs.io/ipget/v0.7.0/ipget_v0.7.0_linux-amd64.tar.gz
            tar -xzf ipget_v0.7.0_linux-amd64.tar.gz
            cd ipget

            # Fixing IPFS on Alpine issue:
            # https://discuss.ipfs.io/t/why-go-ipfs-cant-run-in-alphine-linux/6625
            mkdir /lib64 && ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2

            ./ipget --output "$output_data_path" "$path"
          - {inputValue: Path}
          - {outputPath: Data}
- name: ML frameworks
  folders:
  - name: Scikit learn
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml
      digest: 57ecdf4c3d54c5e6a032ba60667fea7af140d3568552bf5ea090510af5821a90
      text: |
        name: Train linear regression model using scikit learn from CSV
        description: Trains linear regression model using Scikit-learn.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml'}
        inputs:
        - {name: dataset, type: CSV, description: Tabular dataset for training.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label.}
        outputs:
        - {name: model, type: ScikitLearnPickleModel, description: Trained model in Scikit-learn
            pickle format.}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_linear_regression_model_using_scikit_learn_from_CSV(
                  dataset_path,
                  model_path,
                  label_column_name,
              ):
                  """Trains linear regression model using Scikit-learn.

                  See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression

                  Args:
                      dataset_path: Tabular dataset for training.
                      model_path: Trained model in Scikit-learn pickle format.
                      label_column_name: Name of the table column to use as label.
                  """
                  import pandas
                  import pickle
                  from sklearn import linear_model

                  df = pandas.read_csv(dataset_path)
                  model = linear_model.LinearRegression()
                  model.fit(
                      X=df.drop(columns=label_column_name),
                      y=df[label_column_name],
                  )

                  with open(model_path, "wb") as f:
                      pickle.dump(model, f)

              import argparse
              _parser = argparse.ArgumentParser(prog='Train linear regression model using scikit learn from CSV', description='Trains linear regression model using Scikit-learn.')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_linear_regression_model_using_scikit_learn_from_CSV(**_parsed_args)
            args:
            - --dataset
            - {inputPath: dataset}
            - --label-column-name
            - {inputValue: label_column_name}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml
      digest: 25410cf204a3a5d4274906541d5f0140c2257b719c0b98e07ab6e85b066e84e4
      text: |
        name: Train logistic regression model using scikit learn from CSV
        description: Trains logistic regression model using Scikit-learn.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml'}
        inputs:
        - {name: dataset, type: CSV, description: Tabular dataset for training.}
        - {name: label_column_name, type: String, description: Name of the data column to
            use as label.}
        - name: penalty
          type: String
          description: |-
            Used to specify the norm used in the penalization.
            Possible values: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'
            The 'newton-cg',
            'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is
            only supported by the 'saga' solver. If 'none' (not supported by the
            liblinear solver), no regularization is applied.
          default: l2
          optional: true
        - name: solver
          type: String
          description: |-
            Algorithm to use in the optimization problem.
            Possible values: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'

            - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
            'saga' are faster for large ones.
            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
            handle multinomial loss; 'liblinear' is limited to one-versus-rest
            schemes.
            - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty
            - 'liblinear' and 'saga' also handle L1 penalty
            - 'saga' also supports 'elasticnet' penalty
            - 'liblinear' does not support setting ``penalty='none'``

            Note that 'sag' and 'saga' fast convergence is only guaranteed on
            features with approximately the same scale. You can
            preprocess the data with a scaler from sklearn.preprocessing.
          default: lbfgs
          optional: true
        - {name: max_iterations, type: Integer, description: Maximum number of iterations
            taken for the solvers to converge., default: '100', optional: true}
        - name: multi_class_mode
          type: String
          description: |-
            Possible values: {'auto', 'ovr', 'multinomial'}, default='auto'
            If the option chosen is 'ovr', then a binary problem is fit for each
            label. For 'multinomial' the loss minimised is the multinomial loss fit
            across the entire probability distribution, *even when the data is
            binary*. 'multinomial' is unavailable when solver='liblinear'.
            'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
            and otherwise selects 'multinomial'.
          default: auto
          optional: true
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: model, type: ScikitLearnPickleModel, description: Trained model in Scikit-learn
            pickle format.}
        - {name: model_parameters, type: JsonObject}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_logistic_regression_model_using_scikit_learn_from_CSV(
                  dataset_path,
                  model_path,
                  label_column_name,
                  penalty = "l2", # l1, l2, elasticnet, none
                  solver = "lbfgs", # newton-cg, lbfgs, liblinear, sag, saga
                  max_iterations = 100,
                  multi_class_mode = "auto", # auto, ovr, multinomial
                  random_seed = 0,
              ):
                  """Trains logistic regression model using Scikit-learn.

                  See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

                  Args:
                      dataset_path: Tabular dataset for training.
                      model_path: Trained model in Scikit-learn pickle format.
                      label_column_name: Name of the data column to use as label.
                      penalty: Used to specify the norm used in the penalization.
                          Possible values: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'
                          The 'newton-cg',
                          'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is
                          only supported by the 'saga' solver. If 'none' (not supported by the
                          liblinear solver), no regularization is applied.
                      solver: Algorithm to use in the optimization problem.
                          Possible values: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'

                          - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
                          'saga' are faster for large ones.
                          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
                          handle multinomial loss; 'liblinear' is limited to one-versus-rest
                          schemes.
                          - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty
                          - 'liblinear' and 'saga' also handle L1 penalty
                          - 'saga' also supports 'elasticnet' penalty
                          - 'liblinear' does not support setting ``penalty='none'``

                          Note that 'sag' and 'saga' fast convergence is only guaranteed on
                          features with approximately the same scale. You can
                          preprocess the data with a scaler from sklearn.preprocessing.
                      max_iterations: Maximum number of iterations taken for the solvers to converge.
                      multi_class_mode: Possible values: {'auto', 'ovr', 'multinomial'}, default='auto'
                          If the option chosen is 'ovr', then a binary problem is fit for each
                          label. For 'multinomial' the loss minimised is the multinomial loss fit
                          across the entire probability distribution, *even when the data is
                          binary*. 'multinomial' is unavailable when solver='liblinear'.
                          'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
                          and otherwise selects 'multinomial'.
                      random_seed: Controls the seed of the random processes.
                  """
                  import json
                  import pandas
                  import pickle
                  from sklearn import linear_model

                  df = pandas.read_csv(dataset_path)
                  model = linear_model.LogisticRegression(
                      penalty=penalty,
                      #dual=False,
                      #tol=1e-4,
                      #C=1.0,
                      #fit_intercept=True,
                      #intercept_scaling=1,
                      #class_weight=None,
                      random_state=random_seed,
                      solver=solver,
                      max_iter=max_iterations,
                      multi_class=multi_class_mode,
                      #l1_ratio=None,
                      verbose=1,
                  )

                  model_parameters = model.get_params()
                  model_parameters_json = json.dumps(model_parameters, indent=2)
                  print("Model parameters:")
                  print(model_parameters_json)
                  print()

                  model.fit(
                      X=df.drop(columns=label_column_name),
                      y=df[label_column_name],
                  )

                  with open(model_path, "wb") as f:
                      pickle.dump(model, f)

                  return (model_parameters_json,)

              def _serialize_json(obj) -> str:
                  if isinstance(obj, str):
                      return obj
                  import json
                  def default_serializer(obj):
                      if hasattr(obj, 'to_struct'):
                          return obj.to_struct()
                      else:
                          raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                  return json.dumps(obj, default=default_serializer, sort_keys=True)

              import argparse
              _parser = argparse.ArgumentParser(prog='Train logistic regression model using scikit learn from CSV', description='Trains logistic regression model using Scikit-learn.')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--penalty", dest="penalty", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--solver", dest="solver", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--max-iterations", dest="max_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--multi-class-mode", dest="multi_class_mode", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
              _parsed_args = vars(_parser.parse_args())
              _output_files = _parsed_args.pop("_output_paths", [])

              _outputs = train_logistic_regression_model_using_scikit_learn_from_CSV(**_parsed_args)

              _output_serializers = [
                  _serialize_json,

              ]

              import os
              for idx, output_file in enumerate(_output_files):
                  try:
                      os.makedirs(os.path.dirname(output_file))
                  except OSError:
                      pass
                  with open(output_file, 'w') as f:
                      f.write(_output_serializers[idx](_outputs[idx]))
            args:
            - --dataset
            - {inputPath: dataset}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: penalty}
                then:
                - --penalty
                - {inputValue: penalty}
            - if:
                cond: {isPresent: solver}
                then:
                - --solver
                - {inputValue: solver}
            - if:
                cond: {isPresent: max_iterations}
                then:
                - --max-iterations
                - {inputValue: max_iterations}
            - if:
                cond: {isPresent: multi_class_mode}
                then:
                - --multi-class-mode
                - {inputValue: multi_class_mode}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --model
            - {outputPath: model}
            - '----output-paths'
            - {outputPath: model_parameters}
  - name: XGBoost
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/component.yaml
      digest: 5b8bec6716337d7bd8ecafd6dd46bc44527783bc05950cfe03a60206b43c7654
      text: |
        name: Train XGBoost model on CSV
        description: Trains an XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'}
        inputs:
        - {name: training_data, type: CSV, description: Training data in CSV format.}
        - {name: label_column_name, type: String, description: Name of the column containing
            the label data.}
        - {name: starting_model, type: XGBoostModel, description: Existing trained model to
            start from (in the binary XGBoost format)., optional: true}
        - {name: num_iterations, type: Integer, description: Number of boosting iterations.,
          default: '10', optional: true}
        - name: objective
          type: String
          description: |-
            The learning task and the corresponding learning objective.
            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
            The most common values are:
            "reg:squarederror" - Regression with squared loss (default).
            "reg:logistic" - Logistic regression.
            "binary:logistic" - Logistic regression for binary classification, output probability.
            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
          default: reg:squarederror
          optional: true
        - {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,
            `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`
            uses linear functions.', default: gbtree, optional: true}
        - {name: learning_rate, type: Float, description: 'Step size shrinkage used in update
            to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}
        - name: min_split_loss
          type: Float
          description: |-
            Minimum loss reduction required to make a further partition on a leaf node of the tree.
            The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
          default: '0'
          optional: true
        - name: max_depth
          type: Integer
          description: |-
            Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
            0 indicates no limit on depth. Range: [0,Inf].
          default: '6'
          optional: true
        - {name: booster_params, type: JsonObject, description: 'Parameters for the booster.
            See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}
        outputs:
        - {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost
            format.}
        - {name: model_config, type: XGBoostModelConfig, description: The internal parameter
            configuration of Booster as a JSON string.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_XGBoost_model_on_CSV(
                  training_data_path,
                  model_path,
                  model_config_path,
                  label_column_name,
                  starting_model_path = None,
                  num_iterations = 10,
                  # Booster parameters
                  objective = "reg:squarederror",
                  booster = "gbtree",
                  learning_rate = 0.3,
                  min_split_loss = 0,
                  max_depth = 6,
                  booster_params = None,
              ):
                  """Trains an XGBoost model.

                  Args:
                      training_data_path: Training data in CSV format.
                      model_path: Trained model in the binary XGBoost format.
                      model_config_path: The internal parameter configuration of Booster as a JSON string.
                      starting_model_path: Existing trained model to start from (in the binary XGBoost format).
                      label_column_name: Name of the column containing the label data.
                      num_iterations: Number of boosting iterations.
                      booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                      objective: The learning task and the corresponding learning objective.
                          See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                          The most common values are:
                          "reg:squarederror" - Regression with squared loss (default).
                          "reg:logistic" - Logistic regression.
                          "binary:logistic" - Logistic regression for binary classification, output probability.
                          "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                          "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                          "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
                      booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.
                      learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].
                      min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.
                          The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
                      max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                          0 indicates no limit on depth. Range: [0,Inf].

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  import pandas
                  import xgboost

                  df = pandas.read_csv(
                      training_data_path,
                  ).convert_dtypes()
                  print("Training data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print()
                  print("Final training data information:")
                  df.info(verbose=True)

                  training_data = xgboost.DMatrix(
                      data=df.drop(columns=[label_column_name]),
                      label=df[[label_column_name]],
                      enable_categorical=True,
                  )

                  booster_params = booster_params or {}
                  booster_params.setdefault("objective", objective)
                  booster_params.setdefault("booster", booster)
                  booster_params.setdefault("learning_rate", learning_rate)
                  booster_params.setdefault("min_split_loss", min_split_loss)
                  booster_params.setdefault("max_depth", max_depth)

                  starting_model = None
                  if starting_model_path:
                      starting_model = xgboost.Booster(model_file=starting_model_path)

                  print()
                  print("Training the model:")
                  model = xgboost.train(
                      params=booster_params,
                      dtrain=training_data,
                      num_boost_round=num_iterations,
                      xgb_model=starting_model,
                      evals=[(training_data, "training_data")],
                  )

                  # Saving the model in binary format
                  model.save_model(model_path)

                  model_config_str = model.save_config()
                  with open(model_config_path, "w") as model_config_file:
                      model_config_file.write(model_config_str)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train XGBoost model on CSV', description='Trains an XGBoost model.')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_XGBoost_model_on_CSV(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: objective}
                then:
                - --objective
                - {inputValue: objective}
            - if:
                cond: {isPresent: booster}
                then:
                - --booster
                - {inputValue: booster}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: min_split_loss}
                then:
                - --min-split-loss
                - {inputValue: min_split_loss}
            - if:
                cond: {isPresent: max_depth}
                then:
                - --max-depth
                - {inputValue: max_depth}
            - if:
                cond: {isPresent: booster_params}
                then:
                - --booster-params
                - {inputValue: booster_params}
            - --model
            - {outputPath: model}
            - --model-config
            - {outputPath: model_config}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/component.yaml
      digest: 6fd7196d2061e6f49ec98459c90f4b1e4e63bca21170c70b23f7679921ce01d7
      text: |
        name: Xgboost predict on CSV
        description: Makes predictions using a trained XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'}
        inputs:
        - {name: data, type: CSV, description: Feature data in Apache Parquet format.}
        - {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}
        - {name: label_column_name, type: String, description: Optional. Name of the column
            containing the label data that is excluded during the prediction., optional: true}
        outputs:
        - {name: predictions, description: Model predictions.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
              -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def xgboost_predict_on_CSV(
                  data_path,
                  model_path,
                  predictions_path,
                  label_column_name = None,
              ):
                  """Makes predictions using a trained XGBoost model.

                  Args:
                      data_path: Feature data in Apache Parquet format.
                      model_path: Trained model in binary XGBoost format.
                      predictions_path: Model predictions.
                      label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  from pathlib import Path

                  import numpy
                  import pandas
                  import xgboost

                  df = pandas.read_csv(
                      data_path,
                  ).convert_dtypes()
                  print("Evaluation data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print("Final evaluation data information:")
                  df.info(verbose=True)

                  if label_column_name is not None:
                      df = df.drop(columns=[label_column_name])

                  testing_data = xgboost.DMatrix(
                      data=df,
                      enable_categorical=True,
                  )

                  model = xgboost.Booster(model_file=model_path)

                  predictions = model.predict(testing_data)

                  Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Xgboost predict on CSV', description='Makes predictions using a trained XGBoost model.')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = xgboost_predict_on_CSV(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/from_ApacheParquet/component.yaml
      digest: eb6a6f2e21760995351eda950d9a350bf339410afb327eceab7df0618cceb656
      text: |
        name: Train XGBoost model on ApacheParquet
        description: Trains an XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/from_ApacheParquet/component.yaml'}
        inputs:
        - {name: training_data, type: ApacheParquet, description: Training data in the Apache
            Parquet format.}
        - {name: label_column_name, type: String, description: Name of the column containing
            the label data.}
        - {name: starting_model, type: XGBoostModel, description: Existing trained model to
            start from (in the binary XGBoost format)., optional: true}
        - {name: num_iterations, type: Integer, description: Number of boosting iterations.,
          default: '10', optional: true}
        - name: objective
          type: String
          description: |-
            The learning task and the corresponding learning objective.
            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
            The most common values are:
            "reg:squarederror" - Regression with squared loss (default).
            "reg:logistic" - Logistic regression.
            "binary:logistic" - Logistic regression for binary classification, output probability.
            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
          default: reg:squarederror
          optional: true
        - {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,
            `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`
            uses linear functions.', default: gbtree, optional: true}
        - {name: learning_rate, type: Float, description: 'Step size shrinkage used in update
            to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}
        - name: min_split_loss
          type: Float
          description: |-
            Minimum loss reduction required to make a further partition on a leaf node of the tree.
            The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
          default: '0'
          optional: true
        - name: max_depth
          type: Integer
          description: |-
            Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
            0 indicates no limit on depth. Range: [0,Inf].
          default: '6'
          optional: true
        - {name: booster_params, type: JsonObject, description: 'Parameters for the booster.
            See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}
        outputs:
        - {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost
            format.}
        - {name: model_config, type: XGBoostModelConfig, description: The internal parameter
            configuration of Booster as a JSON string.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1
              python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              'pyarrow==9.0.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_XGBoost_model_on_ApacheParquet(
                  training_data_path,
                  model_path,
                  model_config_path,
                  label_column_name,
                  starting_model_path = None,
                  num_iterations = 10,
                  # Booster parameters
                  objective = "reg:squarederror",
                  booster = "gbtree",
                  learning_rate = 0.3,
                  min_split_loss = 0,
                  max_depth = 6,
                  booster_params = None,
              ):
                  """Trains an XGBoost model.

                  Args:
                      training_data_path: Training data in the Apache Parquet format.
                      model_path: Trained model in the binary XGBoost format.
                      model_config_path: The internal parameter configuration of Booster as a JSON string.
                      starting_model_path: Existing trained model to start from (in the binary XGBoost format).
                      label_column_name: Name of the column containing the label data.
                      num_iterations: Number of boosting iterations.
                      booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                      objective: The learning task and the corresponding learning objective.
                          See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                          The most common values are:
                          "reg:squarederror" - Regression with squared loss (default).
                          "reg:logistic" - Logistic regression.
                          "binary:logistic" - Logistic regression for binary classification, output probability.
                          "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                          "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                          "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
                      booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.
                      learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].
                      min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.
                          The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
                      max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                          0 indicates no limit on depth. Range: [0,Inf].

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  import pandas
                  import xgboost

                  # Loading data
                  df = pandas.read_parquet(training_data_path)
                  print("Training data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print()
                  print("Final training data information:")
                  df.info(verbose=True)

                  training_data = xgboost.DMatrix(
                      data=df.drop(columns=[label_column_name]),
                      label=df[[label_column_name]],
                      enable_categorical=True,
                  )
                  # Training
                  booster_params = booster_params or {}
                  booster_params.setdefault("objective", objective)
                  booster_params.setdefault("booster", booster)
                  booster_params.setdefault("learning_rate", learning_rate)
                  booster_params.setdefault("min_split_loss", min_split_loss)
                  booster_params.setdefault("max_depth", max_depth)

                  starting_model = None
                  if starting_model_path:
                      starting_model = xgboost.Booster(model_file=starting_model_path)

                  print()
                  print("Training the model:")
                  model = xgboost.train(
                      params=booster_params,
                      dtrain=training_data,
                      num_boost_round=num_iterations,
                      xgb_model=starting_model,
                      evals=[(training_data, "training_data")],
                  )

                  # Saving the model in binary format
                  model.save_model(model_path)

                  model_config_str = model.save_config()
                  with open(model_config_path, "w") as model_config_file:
                      model_config_file.write(model_config_str)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train XGBoost model on ApacheParquet', description='Trains an XGBoost model.')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_XGBoost_model_on_ApacheParquet(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: objective}
                then:
                - --objective
                - {inputValue: objective}
            - if:
                cond: {isPresent: booster}
                then:
                - --booster
                - {inputValue: booster}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: min_split_loss}
                then:
                - --min-split-loss
                - {inputValue: min_split_loss}
            - if:
                cond: {isPresent: max_depth}
                then:
                - --max-depth
                - {inputValue: max_depth}
            - if:
                cond: {isPresent: booster_params}
                then:
                - --booster-params
                - {inputValue: booster_params}
            - --model
            - {outputPath: model}
            - --model-config
            - {outputPath: model_config}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/from_ApacheParquet/component.yaml
      digest: 012fca583d79e22d2121f00251b52e2632052de8bc3d8a6666e881e18fb3ee59
      text: |
        name: Xgboost predict on ApacheParquet
        description: Makes predictions using a trained XGBoost model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/from_ApacheParquet/component.yaml'}
        inputs:
        - {name: data, type: ApacheParquet, description: Feature data in Apache Parquet format.}
        - {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}
        - {name: label_column_name, type: String, description: Optional. Name of the column
            containing the label data that is excluded during the prediction., optional: true}
        outputs:
        - {name: predictions, description: Model predictions.}
        implementation:
          container:
            image: python:3.10
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'xgboost==1.6.1' 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1
              python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
              'pyarrow==9.0.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def xgboost_predict_on_ApacheParquet(
                  data_path,
                  model_path,
                  predictions_path,
                  label_column_name = None,
              ):
                  """Makes predictions using a trained XGBoost model.

                  Args:
                      data_path: Feature data in Apache Parquet format.
                      model_path: Trained model in binary XGBoost format.
                      predictions_path: Model predictions.
                      label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  """
                  from pathlib import Path

                  import numpy
                  import pandas
                  import xgboost

                  # Loading data
                  df = pandas.read_parquet(data_path)
                  print("Evaluation data information:")
                  df.info(verbose=True)
                  # Converting column types that XGBoost does not support
                  for column_name, dtype in df.dtypes.items():
                      if dtype in ["string", "object"]:
                          print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                          df[column_name] = df[column_name].astype("category")
                          print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                      # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                      if pandas.api.types.is_float_dtype(dtype):
                          # Converting from "Float64" to "float64"
                          df[column_name] = df[column_name].astype(dtype.name.lower())
                  print("Final evaluation data information:")
                  df.info(verbose=True)

                  if label_column_name:
                      df = df.drop(columns=[label_column_name])

                  evaluation_data = xgboost.DMatrix(
                      data=df,
                      enable_categorical=True,
                  )

                  # Training
                  model = xgboost.Booster(model_file=model_path)

                  predictions = model.predict(evaluation_data)

                  Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Xgboost predict on ApacheParquet', description='Makes predictions using a trained XGBoost model.')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = xgboost_predict_on_ApacheParquet(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - --predictions
            - {outputPath: predictions}
  - name: PyTorch
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Create_fully_connected_network/component.yaml
      digest: 6074655c1dddbafcd7a4d618ba1d2489fe3f53444399f1450a23c4acc69280c5
      text: |
        name: Create fully connected pytorch network
        description: Creates fully-connected network in PyTorch ScriptModule format
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_fully_connected_network/component.yaml'
        inputs:
        - {name: layer_sizes, type: JsonArray}
        - {name: activation_name, type: String, default: relu, optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: network, type: PyTorchScriptModule}
        implementation:
          container:
            image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def create_fully_connected_pytorch_network(
                  layer_sizes,
                  network_path,
                  activation_name = 'relu',
                  random_seed = 0,
              ):
                  '''Creates fully-connected network in PyTorch ScriptModule format'''
                  import torch
                  torch.manual_seed(random_seed)

                  activation = getattr(torch, activation_name, None) or getattr(torch.nn.functional, activation_name, None)
                  if not activation:
                      raise ValueError(f'Activation "{activation_name}" was not found.')

                  class ActivationLayer(torch.nn.Module):
                      def forward(self, input):
                          return activation(input)

                  layers = []
                  for layer_idx in range(len(layer_sizes) - 1):
                      layer = torch.nn.Linear(layer_sizes[layer_idx], layer_sizes[layer_idx + 1])
                      layers.append(layer)
                      if layer_idx < len(layer_sizes) - 2:
                          layers.append(ActivationLayer())

                  network = torch.nn.Sequential(*layers)
                  script_module = torch.jit.script(network)
                  print(script_module)
                  script_module.save(network_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Create fully connected pytorch network', description='Creates fully-connected network in PyTorch ScriptModule format')
              _parser.add_argument("--layer-sizes", dest="layer_sizes", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--activation-name", dest="activation_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--network", dest="network_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = create_fully_connected_pytorch_network(**_parsed_args)
            args:
            - --layer-sizes
            - {inputValue: layer_sizes}
            - if:
                cond: {isPresent: activation_name}
                then:
                - --activation-name
                - {inputValue: activation_name}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --network
            - {outputPath: network}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml
      digest: 3c84cd7eddeda2d59ca84d37ea416bd78427aea8eeccd85005e958c203914b67
      text: |
        name: Train pytorch model from csv
        description: Trains PyTorch model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml'}
        inputs:
        - {name: model, type: PyTorchScriptModule, description: Model in PyTorch format.}
        - {name: training_data, type: CSV, description: Tabular dataset for training.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label.}
        - {name: loss_function_name, type: String, description: Name of the loss function.,
          default: mse_loss, optional: true}
        - {name: number_of_epochs, type: Integer, description: Number of training epochs.,
          default: '1', optional: true}
        - {name: learning_rate, type: Float, description: Learning rate of the optimizer.,
          default: '0.1', optional: true}
        - {name: optimizer_name, type: String, description: Name of the optimizer., default: Adadelta,
          optional: true}
        - {name: optimizer_parameters, type: JsonObject, description: Optimizer parameters
            in dictionary form., optional: true}
        - {name: batch_size, type: Integer, description: Number of training samples to use
            in each batch., default: '32', optional: true}
        - {name: batch_log_interval, type: Integer, description: Print training summary after
            every N batches., default: '100', optional: true}
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: trained_model, type: PyTorchScriptModule, description: Trained model in PyTorch
            format.}
        implementation:
          container:
            image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_pytorch_model_from_csv(
                  model_path,
                  training_data_path,
                  trained_model_path,
                  label_column_name,
                  loss_function_name = 'mse_loss',
                  number_of_epochs = 1,
                  learning_rate = 0.1,
                  optimizer_name = 'Adadelta',
                  optimizer_parameters = None,
                  batch_size = 32,
                  batch_log_interval = 100,
                  random_seed = 0,
              ):
                  """Trains PyTorch model.

                  Args:
                      model_path: Model in PyTorch format.
                      training_data_path: Tabular dataset for training.
                      trained_model_path: Trained model in PyTorch format.
                      label_column_name: Name of the table column to use as label.
                      loss_function_name: Name of the loss function.
                      number_of_epochs: Number of training epochs.
                      learning_rate: Learning rate of the optimizer.
                      optimizer_name: Name of the optimizer.
                      optimizer_parameters: Optimizer parameters in dictionary form.
                      batch_size: Number of training samples to use in each batch.
                      batch_log_interval: Print training summary after every N batches.
                      random_seed: Controls the seed of the random processes.
                  """
                  import pandas
                  import torch

                  torch.manual_seed(random_seed)

                  use_cuda = torch.cuda.is_available()
                  device = torch.device("cuda" if use_cuda else "cpu")

                  model = torch.jit.load(model_path)
                  model.to(device)
                  model.train()

                  optimizer_class = getattr(torch.optim, optimizer_name, None)
                  if not optimizer_class:
                      raise ValueError(f'Optimizer "{optimizer_name}" was not found.')

                  optimizer_parameters = optimizer_parameters or {}
                  optimizer_parameters['lr'] = learning_rate
                  optimizer = optimizer_class(model.parameters(), **optimizer_parameters)

                  loss_function = getattr(torch, loss_function_name, None) or getattr(torch.nn, loss_function_name, None) or getattr(torch.nn.functional, loss_function_name, None)
                  if not loss_function:
                      raise ValueError(f'Loss function "{loss_function_name}" was not found.')

                  class CsvDataset(torch.utils.data.Dataset):

                      def __init__(self, file_path, label_column_name, drop_nan_columns_or_rows = 'columns'):
                          dataframe = pandas.read_csv(file_path).convert_dtypes()
                          # Preventing error: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object
                          if drop_nan_columns_or_rows == 'columns':
                              non_nan_data = dataframe.dropna(axis='columns')
                              removed_columns = set(dataframe.columns) - set(non_nan_data.columns)
                              if removed_columns:
                                  print('Skipping columns with NaNs: ' + str(removed_columns))
                              dataframe = non_nan_data
                          if drop_nan_columns_or_rows == 'rows':
                              non_nan_data = dataframe.dropna(axis='index')
                              number_of_removed_rows = len(dataframe) - len(non_nan_data)
                              if number_of_removed_rows:
                                  print(f'Skipped {number_of_removed_rows} rows with NaNs.')
                              dataframe = non_nan_data
                          numerical_data = dataframe.select_dtypes(include='number')
                          non_numerical_data = dataframe.select_dtypes(exclude='number')
                          if not non_numerical_data.empty:
                              print('Skipping non-number columns:')
                              print(non_numerical_data.dtypes)
                          self._dataframe = dataframe
                          self.labels = numerical_data[[label_column_name]]
                          self.features = numerical_data.drop(columns=[label_column_name])

                      def __len__(self):
                          return len(self._dataframe)

                      def __getitem__(self, index):
                          return [self.features.loc[index].to_numpy(dtype='float32'), self.labels.loc[index].to_numpy(dtype='float32')]

                  dataset = CsvDataset(
                      file_path=training_data_path,
                      label_column_name=label_column_name,
                  )
                  train_loader = torch.utils.data.DataLoader(
                      dataset=dataset,
                      batch_size=batch_size,
                      shuffle=True,
                  )

                  last_full_batch_loss = None
                  for epoch in range(1, number_of_epochs + 1):
                      for batch_idx, (data, target) in enumerate(train_loader):
                          data, target = data.to(device), target.to(device)
                          optimizer.zero_grad()
                          output = model(data)
                          loss = loss_function(output, target)
                          loss.backward()
                          optimizer.step()
                          if len(data) == batch_size:
                              last_full_batch_loss = loss.item()
                          if batch_idx % batch_log_interval == 0:
                              print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                                  epoch, batch_idx * len(data), len(train_loader.dataset),
                                  100. * batch_idx / len(train_loader), loss.item()))
                      print(f'Training epoch {epoch} completed. Last full batch loss: {last_full_batch_loss:.6f}')

                  # print(optimizer.state_dict())
                  model.save(trained_model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train pytorch model from csv', description='Trains PyTorch model.')
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function-name", dest="loss_function_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-epochs", dest="number_of_epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-name", dest="optimizer_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-parameters", dest="optimizer_parameters", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-log-interval", dest="batch_log_interval", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--trained-model", dest="trained_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_pytorch_model_from_csv(**_parsed_args)
            args:
            - --model
            - {inputPath: model}
            - --training-data
            - {inputPath: training_data}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: loss_function_name}
                then:
                - --loss-function-name
                - {inputValue: loss_function_name}
            - if:
                cond: {isPresent: number_of_epochs}
                then:
                - --number-of-epochs
                - {inputValue: number_of_epochs}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: optimizer_name}
                then:
                - --optimizer-name
                - {inputValue: optimizer_name}
            - if:
                cond: {isPresent: optimizer_parameters}
                then:
                - --optimizer-parameters
                - {inputValue: optimizer_parameters}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - if:
                cond: {isPresent: batch_log_interval}
                then:
                - --batch-log-interval
                - {inputValue: batch_log_interval}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --trained-model
            - {outputPath: trained_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml
      digest: 7edca2cd8212000a52dedf6c60aeb808fd022818ccb0ee64ddc85259b84e6186
      text: |
        name: Convert to onnx from pytorch script module
        description: Creates fully-connected network in PyTorch ScriptModule format
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml'
        inputs:
        - {name: model, type: PyTorchScriptModule}
        - {name: list_of_input_shapes, type: JsonArray}
        outputs:
        - {name: converted_model, type: OnnxModel}
        implementation:
          container:
            image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def convert_to_onnx_from_pytorch_script_module(
                  model_path,
                  converted_model_path,
                  list_of_input_shapes,
              ):
                  '''Creates fully-connected network in PyTorch ScriptModule format'''
                  import torch
                  model = torch.jit.load(model_path)
                  example_inputs = [
                      torch.ones(*input_shape)
                      for input_shape in list_of_input_shapes
                  ]
                  example_outputs = model.forward(*example_inputs)
                  torch.onnx.export(
                      model=model,
                      args=example_inputs,
                      f=converted_model_path,
                      verbose=True,
                      training=torch.onnx.TrainingMode.EVAL,
                      example_outputs=example_outputs,
                  )

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Convert to onnx from pytorch script module', description='Creates fully-connected network in PyTorch ScriptModule format')
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--list-of-input-shapes", dest="list_of_input_shapes", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = convert_to_onnx_from_pytorch_script_module(**_parsed_args)
            args:
            - --model
            - {inputPath: model}
            - --list-of-input-shapes
            - {inputValue: list_of_input_shapes}
            - --converted-model
            - {outputPath: converted_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/46d51383e6554b7f3ab4fd8cf614d8c2b422fb22/components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml
      digest: 8298b5ee1b0f0879f893add4cf352c8dec7cf9e21bb9db134c91a2d046cdb0ec
      text: |
        name: Create PyTorch Model Archive with base handler
        inputs:
        - {name: Model, type: PyTorchScriptModule}
        - {name: Model name, type: String, default: model}
        - {name: Model version, type: String, default: "1.0"}
        outputs:
        - {name: Model archive, type: PyTorchModelArchive}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml'
        implementation:
          container:
            image: pytorch/torchserve:0.6.0-cpu
            command:
            - bash
            - -exc
            - |
              model_path=$0
              model_name=$1
              model_version=$2
              output_model_archive_path=$3

              mkdir -p "$(dirname "$output_model_archive_path")"

              # TODO: Use the built-in base_handler once my fix is merged: https://github.com/pytorch/serve/pull/1682
              echo '
              from ts.torch_handler import base_handler
              class BaseHandler(base_handler.BaseHandler):
                  pass
              ' > base_handler.py  # torch-model-archiver needs the handler to have .py extension
              torch-model-archiver --model-name "$model_name" --version "$model_version" --serialized-file "$model_path" --handler base_handler.py

              # torch-model-archiver does not allow specifying the output path, but always writes to "${model_name}.<format>"
              expected_model_archive_path="${model_name}.mar"
              mv "$expected_model_archive_path" "$output_model_archive_path"

            - {inputPath: Model}
            - {inputValue: Model name}
            - {inputValue: Model version}
            - {outputPath: Model archive}
  - name: Tensorflow
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/f3a9769d35a057c31a498e0667cae2e4a830c5b0/components/tensorflow/Create_fully_connected_network/component.yaml
      digest: c97df1a0fe9b4d58300ffdf97dd63555cef1246ec93f9c68a0667dc8a99f1e1c
      text: |
        name: Create fully connected tensorflow network
        description: Creates fully-connected network in Tensorflow SavedModel format
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Create_fully_connected_network/component.yaml'}
        inputs:
        - {name: layer_sizes, type: JsonArray}
        - {name: activation_name, type: String, default: relu, optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: model, type: TensorflowSavedModel}
        implementation:
          container:
            image: tensorflow/tensorflow:2.7.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def create_fully_connected_tensorflow_network(
                  layer_sizes,
                  model_path,
                  activation_name = "relu",
                  random_seed = 0,
              ):
                  """Creates fully-connected network in Tensorflow SavedModel format"""
                  import tensorflow as tf
                  tf.random.set_seed(seed=random_seed)

                  if len(layer_sizes) < 2:
                      raise ValueError(f"Fully-connected network requires at least two layer sizes (input and output). Got {layer_sizes}.")

                  model = tf.keras.models.Sequential()
                  model.add(tf.keras.Input(shape=(layer_sizes[0],)))
                  for layer_size in layer_sizes[1:-1]:
                      model.add(tf.keras.layers.Dense(units=layer_size, activation=activation_name))
                  # The last layer is left without activation
                  model.add(tf.keras.layers.Dense(units=layer_sizes[-1]))

                  # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:
                  #tf.saved_model.save(model, model_path)
                  # ValueError: Unable to create a Keras model from this SavedModel.
                  # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.
                  # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.
                  # See https://github.com/keras-team/keras/issues/16451
                  tf.keras.models.save_model(model, model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Create fully connected tensorflow network', description='Creates fully-connected network in Tensorflow SavedModel format')
              _parser.add_argument("--layer-sizes", dest="layer_sizes", type=json.loads, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--activation-name", dest="activation_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = create_fully_connected_tensorflow_network(**_parsed_args)
            args:
            - --layer-sizes
            - {inputValue: layer_sizes}
            - if:
                cond: {isPresent: activation_name}
                then:
                - --activation-name
                - {inputValue: activation_name}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/c504a4010348c50eaaf6d4337586ccc008f4dcef/components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml
      digest: 42ae60c889034dbad74815653e95b4f7d576b5f47f803173e8679c7b54984609
      text: |
        name: Train model using Keras on CSV
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml'}
        inputs:
        - {name: training_data, type: CSV}
        - {name: model, type: TensorflowSavedModel}
        - {name: label_column_name, type: String}
        - {name: loss_function_name, type: String, default: mean_squared_error, optional: true}
        - {name: number_of_epochs, type: Integer, default: '1', optional: true}
        - {name: learning_rate, type: Float, default: '0.1', optional: true}
        - {name: optimizer_name, type: String, default: Adadelta, optional: true}
        - {name: optimizer_parameters, type: JsonObject, optional: true}
        - {name: batch_size, type: Integer, default: '32', optional: true}
        - {name: metric_names, type: JsonArray, optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        outputs:
        - {name: trained_model, type: TensorflowSavedModel}
        implementation:
          container:
            image: tensorflow/tensorflow:2.8.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_model_using_Keras_on_CSV(
                  training_data_path,
                  model_path,
                  trained_model_path,
                  label_column_name,
                  loss_function_name = "mean_squared_error",
                  number_of_epochs = 1,
                  learning_rate = 0.1,
                  optimizer_name = "Adadelta",
                  optimizer_parameters = None,
                  batch_size = 32,
                  metric_names = None,
                  random_seed = 0,
              ):
                  import tensorflow as tf
                  tf.random.set_seed(seed=random_seed)

                  # Loading model using Keras. Model loaded using TensorFlow does not have .fit.
                  #model = tf.saved_model.load(export_dir=model_path)
                  keras_model = tf.keras.models.load_model(filepath=model_path)

                  optimizer_parameters = optimizer_parameters or {}
                  optimizer_parameters["learning_rate"] = learning_rate
                  optimizer_config = {
                      "class_name": optimizer_name,
                      "config": optimizer_parameters,
                  }
                  optimizer = tf.keras.optimizers.get(optimizer_config)
                  loss = tf.keras.losses.get(loss_function_name)

                  training_dataset = tf.data.experimental.make_csv_dataset(
                      file_pattern=training_data_path,
                      batch_size=batch_size,
                      label_name=label_column_name,
                      header=True,
                      # Need to specify num_epochs=1 otherwise the training becomes infinite
                      num_epochs=1,
                      shuffle=True,
                      shuffle_seed=random_seed,
                      ignore_errors=True,
                  )
                  def stack_feature_batches(features_batch, labels_batch):
                      # Need to stack individual feature columns to create a single feature tensor
                      # Need to cast all column tensor types to float to prevent error:
                      # TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int32, float32, float32, int32, int32] that don't all match.
                      list_of_feature_batches = list(tf.cast(x=feature_batch, dtype=tf.float32) for feature_batch in features_batch.values())
                      return tf.stack(list_of_feature_batches, axis=-1), labels_batch

                  training_dataset = training_dataset.map(stack_feature_batches)

                  # Need to compile the model to prevent error:
                  # ValueError: No gradients provided for any variable: [..., ...].
                  keras_model.compile(
                      optimizer=optimizer,
                      loss=loss,
                      metrics=metric_names,
                  )
                  keras_model.fit(
                      training_dataset,
                      epochs=number_of_epochs,
                  )

                  # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:
                  #tf.saved_model.save(keras_model, trained_model_path)
                  # ValueError: Unable to create a Keras model from this SavedModel.
                  # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.
                  # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.
                  # See https://github.com/keras-team/keras/issues/16451
                  tf.keras.models.save_model(keras_model, trained_model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train model using Keras on CSV', description='')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function-name", dest="loss_function_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-epochs", dest="number_of_epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-name", dest="optimizer_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-parameters", dest="optimizer_parameters", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--metric-names", dest="metric_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--trained-model", dest="trained_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_model_using_Keras_on_CSV(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --model
            - {inputPath: model}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: loss_function_name}
                then:
                - --loss-function-name
                - {inputValue: loss_function_name}
            - if:
                cond: {isPresent: number_of_epochs}
                then:
                - --number-of-epochs
                - {inputValue: number_of_epochs}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: optimizer_name}
                then:
                - --optimizer-name
                - {inputValue: optimizer_name}
            - if:
                cond: {isPresent: optimizer_parameters}
                then:
                - --optimizer-parameters
                - {inputValue: optimizer_parameters}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - if:
                cond: {isPresent: metric_names}
                then:
                - --metric-names
                - {inputValue: metric_names}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --trained-model
            - {outputPath: trained_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/tensorflow/Train_model_using_Keras/on_ApacheParquet/component.yaml
      digest: 5f886b115e8c5e35c65e9f520b449b7cbcbdebb92114eeef2b7745497f6f6be1
      text: |
        name: Train model using Keras on ApacheParquet
        description: Trains TensorFlow model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Train_model_using_Keras/on_ApacheParquet/component.yaml'}
        inputs:
        - {name: training_data, type: ApacheParquet, description: Tabular dataset for training.}
        - {name: model, type: TensorflowSavedModel, description: Model in TensorFlow format.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label.}
        - {name: loss_function_name, type: String, description: Name of the loss function.,
          default: mean_squared_error, optional: true}
        - {name: number_of_epochs, type: Integer, description: Number of training epochs.,
          default: '1', optional: true}
        - {name: learning_rate, type: Float, description: Learning rate of the optimizer.,
          default: '0.1', optional: true}
        - {name: optimizer_name, type: String, description: Name of the optimizer., default: Adadelta,
          optional: true}
        - {name: optimizer_parameters, type: JsonObject, description: Optimizer parameters
            in dictionary form., optional: true}
        - {name: batch_size, type: Integer, description: Number of training samples to use
            in each batch., default: '32', optional: true}
        - {name: metric_names, type: JsonArray, description: A list of metrics to evaluate
            during the training., optional: true}
        - {name: random_seed, type: Integer, description: Controls the seed of the random
            processes., default: '0', optional: true}
        outputs:
        - {name: trained_model, type: TensorflowSavedModel, description: Trained model in
            TensorFlow format.}
        implementation:
          container:
            image: tensorflow/tensorflow:2.8.0
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'tensorflow-io==0.25.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
              --quiet --no-warn-script-location 'tensorflow-io==0.25.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_model_using_Keras_on_ApacheParquet(
                  training_data_path,
                  model_path,
                  trained_model_path,
                  label_column_name,
                  loss_function_name = "mean_squared_error",
                  number_of_epochs = 1,
                  learning_rate = 0.1,
                  optimizer_name = "Adadelta",
                  optimizer_parameters = None,
                  batch_size = 32,
                  metric_names = None,
                  random_seed = 0,
              ):
                  """Trains TensorFlow model.

                  Args:
                      training_data_path: Tabular dataset for training.
                      model_path: Model in TensorFlow format.
                      trained_model_path: Trained model in TensorFlow format.
                      label_column_name: Name of the table column to use as label.
                      loss_function_name: Name of the loss function.
                      number_of_epochs: Number of training epochs.
                      learning_rate: Learning rate of the optimizer.
                      optimizer_name: Name of the optimizer.
                      optimizer_parameters: Optimizer parameters in dictionary form.
                      batch_size: Number of training samples to use in each batch.
                      metric_names: A list of metrics to evaluate during the training.
                      random_seed: Controls the seed of the random processes.
                  """
                  import tensorflow as tf
                  import tensorflow_io as tfio

                  tf.random.set_seed(seed=random_seed)

                  FEATURES_COLUMN_NAME = "features"
                  SHUFFLE_BUFFER_SIZE = 10000

                  # Loading model using Keras. Model loaded using TensorFlow does not have .fit.
                  # model = tf.saved_model.load(export_dir=model_path)
                  keras_model = tf.keras.models.load_model(filepath=model_path)

                  optimizer_parameters = optimizer_parameters or {}
                  optimizer_parameters["learning_rate"] = learning_rate
                  optimizer_config = {
                      "class_name": optimizer_name,
                      "config": optimizer_parameters,
                  }
                  optimizer = tf.keras.optimizers.get(optimizer_config)
                  loss = tf.keras.losses.get(loss_function_name)

                  def stack_feature_batches(columns_batch_dict):
                      label_batch = columns_batch_dict.pop(label_column_name.encode())
                      if FEATURES_COLUMN_NAME in columns_batch_dict:
                          features_batch = columns_batch_dict[FEATURES_COLUMN_NAME]
                      else:
                          # Need to stack individual feature columns to create a single feature tensor
                          # Need to cast all column tensor types to float
                          list_of_feature_batches = list(
                              tf.cast(x=feature_batch, dtype=tf.float32)
                              for feature_batch in columns_batch_dict.values()
                          )
                          features_batch = tf.stack(list_of_feature_batches, axis=-1)
                      return features_batch, label_batch

                  # ! parquet::ParquetException is thrown if the Parquet dataset has nulls values
                  training_dataset = tfio.IODataset.from_parquet(filename=training_data_path)

                  training_dataset = (
                      training_dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE, seed=random_seed)
                      .repeat(count=number_of_epochs)
                      .batch(
                          batch_size=batch_size,
                          num_parallel_calls=tf.data.AUTOTUNE,
                          deterministic=True,
                      )
                      .map(
                          map_func=stack_feature_batches,
                          num_parallel_calls=tf.data.AUTOTUNE,
                          deterministic=True,
                      )
                  )

                  # Need to compile the model to prevent error:
                  # ValueError: No gradients provided for any variable: [..., ...].
                  keras_model.compile(
                      optimizer=optimizer,
                      loss=loss,
                      metrics=metric_names,
                  )
                  keras_model.fit(
                      training_dataset,
                      epochs=number_of_epochs,
                  )

                  # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:
                  # tf.saved_model.save(keras_model, trained_model_path)
                  # ValueError: Unable to create a Keras model from this SavedModel.
                  # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.
                  # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.
                  # See https://github.com/keras-team/keras/issues/16451
                  tf.keras.models.save_model(keras_model, trained_model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Train model using Keras on ApacheParquet', description='Trains TensorFlow model.')
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function-name", dest="loss_function_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--number-of-epochs", dest="number_of_epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-name", dest="optimizer_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--optimizer-parameters", dest="optimizer_parameters", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--metric-names", dest="metric_names", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--trained-model", dest="trained_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_model_using_Keras_on_ApacheParquet(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - --model
            - {inputPath: model}
            - --label-column-name
            - {inputValue: label_column_name}
            - if:
                cond: {isPresent: loss_function_name}
                then:
                - --loss-function-name
                - {inputValue: loss_function_name}
            - if:
                cond: {isPresent: number_of_epochs}
                then:
                - --number-of-epochs
                - {inputValue: number_of_epochs}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: optimizer_name}
                then:
                - --optimizer-name
                - {inputValue: optimizer_name}
            - if:
                cond: {isPresent: optimizer_parameters}
                then:
                - --optimizer-parameters
                - {inputValue: optimizer_parameters}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - if:
                cond: {isPresent: metric_names}
                then:
                - --metric-names
                - {inputValue: metric_names}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - --trained-model
            - {outputPath: trained_model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/tensorflow/Predict/on_ApacheParquet/component.yaml
      digest: 7e760242a9c9ce013a367b866d3d1d7411986367bcfa59d847d853d3219f7fe8
      text: |
        name: Predict with TensorFlow model on ApacheParquet data
        description: Makes predictions using TensorFlow model.
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Predict/on_ApacheParquet/component.yaml'}
        inputs:
        - {name: dataset, type: ApacheParquet, description: Tabular dataset for prediction.}
        - {name: model, type: TensorflowSavedModel, description: Trained model in TensorFlow
            format.}
        - {name: label_column_name, type: String, description: Name of the table column to
            use as label., optional: true}
        - {name: batch_size, type: Integer, description: Number of samples to use in each
            batch., default: '1000', optional: true}
        outputs:
        - {name: predictions, description: Predictions in multiline text format.}
        implementation:
          container:
            image: tensorflow/tensorflow:2.9.1
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'tensorflow-io==0.26.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
              --quiet --no-warn-script-location 'tensorflow-io==0.26.0' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def predict_with_TensorFlow_model_on_ApacheParquet_data(
                  dataset_path,
                  model_path,
                  predictions_path,
                  label_column_name = None,
                  batch_size = 1000,
              ):
                  """Makes predictions using TensorFlow model.

                  Args:
                      dataset_path: Tabular dataset for prediction.
                      model_path: Trained model in TensorFlow format.
                      predictions_path: Predictions in multiline text format.
                      label_column_name: Name of the table column to use as label.
                      batch_size: Number of samples to use in each batch.
                  """
                  import numpy
                  import tensorflow as tf
                  import tensorflow_io as tfio

                  FEATURES_COLUMN_NAME = "features"

                  model = tf.saved_model.load(export_dir=model_path)

                  def stack_feature_batches_and_drop_labels(columns_batch_dict):
                      if label_column_name:
                          # batch dict keys have bytes type
                          columns_batch_dict.pop(label_column_name.encode())

                      if FEATURES_COLUMN_NAME in columns_batch_dict:
                          features_batch = columns_batch_dict[FEATURES_COLUMN_NAME]
                      else:
                          # Need to stack individual feature columns to create a single feature tensor
                          # Need to cast all column tensor types to float
                          list_of_feature_batches = list(
                              tf.cast(x=feature_batch, dtype=tf.float32)
                              for feature_batch in columns_batch_dict.values()
                          )
                          features_batch = tf.stack(list_of_feature_batches, axis=-1)
                      return features_batch

                  # ! parquet::ParquetException is thrown if the Parquet dataset has nulls values
                  dataset = tfio.IODataset.from_parquet(filename=dataset_path)

                  dataset = dataset.batch(
                      batch_size=batch_size,
                      num_parallel_calls=tf.data.AUTOTUNE,
                      deterministic=True,
                  ).map(
                      map_func=stack_feature_batches_and_drop_labels,
                      num_parallel_calls=tf.data.AUTOTUNE,
                      deterministic=True,
                  )

                  with open(predictions_path, "w") as predictions_file:
                      for features_batch in dataset:
                          predictions_tensor = model(features_batch)
                          numpy.savetxt(predictions_file, predictions_tensor.numpy())

              import argparse
              _parser = argparse.ArgumentParser(prog='Predict with TensorFlow model on ApacheParquet data', description='Makes predictions using TensorFlow model.')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = predict_with_TensorFlow_model_on_ApacheParquet_data(**_parsed_args)
            args:
            - --dataset
            - {inputPath: dataset}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - --predictions
            - {outputPath: predictions}
  - name: CatBoost
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_regression/from_CSV/component.yaml
      digest: ac04356d7b08e9d3e90279befd4df446832641517efb34dea75354312347295a
      text: |
        name: Catboost train regression
        description: |-
          Train a CatBoost classifier model.

              Args:
                  training_data_path: Path for the training data in CSV format.
                  model_path: Output path for the trained model in binary CatBoostModel format.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column: Column containing the label data.

                  loss_function: The metric to use in training and also selector of the machine learning
                      problem to solve. Default = 'RMSE'. Possible values:
                      'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'
                  num_iterations: Number of trees to add to the ensemble.
                  learning_rate: Step size shrinkage used in update to prevents overfitting.
                      Default value is selected automatically for binary classification with other parameters set to default.
                      In all other cases default is 0.03.
                  depth: Depth of a tree. All trees are the same depth. Default = 6
                  random_seed: Random number seed. Default = 0

                  cat_features: A list of Categorical features (indices or names).
                  additional_training_options: A dictionary with additional options to pass to CatBoostRegressor

              Outputs:
                  model: Trained model in binary CatBoostModel format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: training_data, type: CSV}
        - {name: starting_model, type: CatBoostModel, optional: true}
        - {name: label_column, type: Integer, default: '0', optional: true}
        - {name: loss_function, type: String, default: RMSE, optional: true}
        - {name: num_iterations, type: Integer, default: '500', optional: true}
        - {name: learning_rate, type: Float, optional: true}
        - {name: depth, type: Integer, default: '6', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        - {name: cat_features, type: JsonArray, optional: true}
        - {name: additional_training_options, type: JsonObject, default: '{}', optional: true}
        outputs:
        - {name: model, type: CatBoostModel}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_regression/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_train_regression(
                  training_data_path,
                  model_path,
                  starting_model_path = None,
                  label_column = 0,

                  loss_function = 'RMSE',
                  num_iterations = 500,
                  learning_rate = None,
                  depth = 6,
                  random_seed = 0,

                  cat_features = None,

                  additional_training_options = {},
              ):
                  '''Train a CatBoost classifier model.

                  Args:
                      training_data_path: Path for the training data in CSV format.
                      model_path: Output path for the trained model in binary CatBoostModel format.
                      starting_model_path: Path for the existing trained model to start from.
                      label_column: Column containing the label data.

                      loss_function: The metric to use in training and also selector of the machine learning
                          problem to solve. Default = 'RMSE'. Possible values:
                          'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'
                      num_iterations: Number of trees to add to the ensemble.
                      learning_rate: Step size shrinkage used in update to prevents overfitting.
                          Default value is selected automatically for binary classification with other parameters set to default.
                          In all other cases default is 0.03.
                      depth: Depth of a tree. All trees are the same depth. Default = 6
                      random_seed: Random number seed. Default = 0

                      cat_features: A list of Categorical features (indices or names).
                      additional_training_options: A dictionary with additional options to pass to CatBoostRegressor

                  Outputs:
                      model: Trained model in binary CatBoostModel format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile
                  from pathlib import Path

                  from catboost import CatBoostRegressor, Pool

                  column_descriptions = {label_column: 'Label'}
                  column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                  with open(column_description_path, 'w') as column_description_file:
                      for idx, kind in column_descriptions.items():
                          column_description_file.write('{}\t{}\n'.format(idx, kind))

                  train_data = Pool(
                      training_data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoostRegressor(
                      iterations=num_iterations,
                      depth=depth,
                      learning_rate=learning_rate,
                      loss_function=loss_function,
                      random_seed=random_seed,
                      verbose=True,
                      **additional_training_options,
                  )

                  model.fit(
                      train_data,
                      cat_features=cat_features,
                      init_model=starting_model_path,
                      #verbose=False,
                      #plot=True,
                  )
                  Path(model_path).parent.mkdir(parents=True, exist_ok=True)
                  model.save_model(model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost train regression', description="Train a CatBoost classifier model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary CatBoostModel format.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n\n        loss_function: The metric to use in training and also selector of the machine learning\n            problem to solve. Default = 'RMSE'. Possible values:\n            'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'\n        num_iterations: Number of trees to add to the ensemble.\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\n            Default value is selected automatically for binary classification with other parameters set to default.\n            In all other cases default is 0.03.\n        depth: Depth of a tree. All trees are the same depth. Default = 6\n        random_seed: Random number seed. Default = 0\n\n        cat_features: A list of Categorical features (indices or names).\n        additional_training_options: A dictionary with additional options to pass to CatBoostRegressor\n\n    Outputs:\n        model: Trained model in binary CatBoostModel format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function", dest="loss_function", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--depth", dest="depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--cat-features", dest="cat_features", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--additional-training-options", dest="additional_training_options", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_train_regression(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - if:
                cond: {isPresent: loss_function}
                then:
                - --loss-function
                - {inputValue: loss_function}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: depth}
                then:
                - --depth
                - {inputValue: depth}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - if:
                cond: {isPresent: cat_features}
                then:
                - --cat-features
                - {inputValue: cat_features}
            - if:
                cond: {isPresent: additional_training_options}
                then:
                - --additional-training-options
                - {inputValue: additional_training_options}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_classifier/from_CSV/component.yaml
      digest: f4ef9b474377248847c6487a827aa7b14d092a8d031c368168f2857a8ff11cad
      text: |
        name: Catboost train classifier
        description: |-
          Train a CatBoost classifier model.

              Args:
                  training_data_path: Path for the training data in CSV format.
                  model_path: Output path for the trained model in binary CatBoostModel format.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column: Column containing the label data.

                  loss_function: The metric to use in training and also selector of the machine learning
                      problem to solve. Default = 'Logloss'
                  num_iterations: Number of trees to add to the ensemble.
                  learning_rate: Step size shrinkage used in update to prevents overfitting.
                      Default value is selected automatically for binary classification with other parameters set to default.
                      In all other cases default is 0.03.
                  depth: Depth of a tree. All trees are the same depth. Default = 6
                  random_seed: Random number seed. Default = 0

                  cat_features: A list of Categorical features (indices or names).
                  text_features: A list of Text features (indices or names).
                  additional_training_options: A dictionary with additional options to pass to CatBoostClassifier

              Outputs:
                  model: Trained model in binary CatBoostModel format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: training_data, type: CSV}
        - {name: starting_model, type: CatBoostModel, optional: true}
        - {name: label_column, type: Integer, default: '0', optional: true}
        - {name: loss_function, type: String, default: Logloss, optional: true}
        - {name: num_iterations, type: Integer, default: '500', optional: true}
        - {name: learning_rate, type: Float, optional: true}
        - {name: depth, type: Integer, default: '6', optional: true}
        - {name: random_seed, type: Integer, default: '0', optional: true}
        - {name: cat_features, type: JsonArray, optional: true}
        - {name: text_features, type: JsonArray, optional: true}
        - {name: additional_training_options, type: JsonObject, default: '{}', optional: true}
        outputs:
        - {name: model, type: CatBoostModel}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_classifier/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_train_classifier(
                  training_data_path,
                  model_path,
                  starting_model_path = None,
                  label_column = 0,

                  loss_function = 'Logloss',
                  num_iterations = 500,
                  learning_rate = None,
                  depth = 6,
                  random_seed = 0,

                  cat_features = None,
                  text_features = None,

                  additional_training_options = {},
              ):
                  '''Train a CatBoost classifier model.

                  Args:
                      training_data_path: Path for the training data in CSV format.
                      model_path: Output path for the trained model in binary CatBoostModel format.
                      starting_model_path: Path for the existing trained model to start from.
                      label_column: Column containing the label data.

                      loss_function: The metric to use in training and also selector of the machine learning
                          problem to solve. Default = 'Logloss'
                      num_iterations: Number of trees to add to the ensemble.
                      learning_rate: Step size shrinkage used in update to prevents overfitting.
                          Default value is selected automatically for binary classification with other parameters set to default.
                          In all other cases default is 0.03.
                      depth: Depth of a tree. All trees are the same depth. Default = 6
                      random_seed: Random number seed. Default = 0

                      cat_features: A list of Categorical features (indices or names).
                      text_features: A list of Text features (indices or names).
                      additional_training_options: A dictionary with additional options to pass to CatBoostClassifier

                  Outputs:
                      model: Trained model in binary CatBoostModel format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile
                  from pathlib import Path

                  from catboost import CatBoostClassifier, Pool

                  column_descriptions = {label_column: 'Label'}
                  column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                  with open(column_description_path, 'w') as column_description_file:
                      for idx, kind in column_descriptions.items():
                          column_description_file.write('{}\t{}\n'.format(idx, kind))

                  train_data = Pool(
                      training_data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoostClassifier(
                      iterations=num_iterations,
                      depth=depth,
                      learning_rate=learning_rate,
                      loss_function=loss_function,
                      random_seed=random_seed,
                      verbose=True,
                      **additional_training_options,
                  )

                  model.fit(
                      train_data,
                      cat_features=cat_features,
                      text_features=text_features,
                      init_model=starting_model_path,
                      #verbose=False,
                      #plot=True,
                  )
                  Path(model_path).parent.mkdir(parents=True, exist_ok=True)
                  model.save_model(model_path)

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost train classifier', description="Train a CatBoost classifier model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary CatBoostModel format.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n\n        loss_function: The metric to use in training and also selector of the machine learning\n            problem to solve. Default = 'Logloss'\n        num_iterations: Number of trees to add to the ensemble.\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\n            Default value is selected automatically for binary classification with other parameters set to default.\n            In all other cases default is 0.03.\n        depth: Depth of a tree. All trees are the same depth. Default = 6\n        random_seed: Random number seed. Default = 0\n\n        cat_features: A list of Categorical features (indices or names).\n        text_features: A list of Text features (indices or names).\n        additional_training_options: A dictionary with additional options to pass to CatBoostClassifier\n\n    Outputs:\n        model: Trained model in binary CatBoostModel format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
              _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--loss-function", dest="loss_function", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--depth", dest="depth", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--cat-features", dest="cat_features", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--text-features", dest="text_features", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--additional-training-options", dest="additional_training_options", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_train_classifier(**_parsed_args)
            args:
            - --training-data
            - {inputPath: training_data}
            - if:
                cond: {isPresent: starting_model}
                then:
                - --starting-model
                - {inputPath: starting_model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - if:
                cond: {isPresent: loss_function}
                then:
                - --loss-function
                - {inputValue: loss_function}
            - if:
                cond: {isPresent: num_iterations}
                then:
                - --num-iterations
                - {inputValue: num_iterations}
            - if:
                cond: {isPresent: learning_rate}
                then:
                - --learning-rate
                - {inputValue: learning_rate}
            - if:
                cond: {isPresent: depth}
                then:
                - --depth
                - {inputValue: depth}
            - if:
                cond: {isPresent: random_seed}
                then:
                - --random-seed
                - {inputValue: random_seed}
            - if:
                cond: {isPresent: cat_features}
                then:
                - --cat-features
                - {inputValue: cat_features}
            - if:
                cond: {isPresent: text_features}
                then:
                - --text-features
                - {inputValue: text_features}
            - if:
                cond: {isPresent: additional_training_options}
                then:
                - --additional-training-options
                - {inputValue: additional_training_options}
            - --model
            - {outputPath: model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_values/from_CSV/component.yaml
      digest: 64f4651cd80d1bdbd524ff0cd2272916ebd3855f7b1f71d8abf474ccb0ec3dfb
      text: |
        name: Catboost predict values
        description: |-
          Predict values with a CatBoost model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: data, type: CSV}
        - {name: model, type: CatBoostModel}
        - {name: label_column, type: Integer, optional: true}
        outputs:
        - {name: predictions}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_values/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_predict_values(
                  data_path,
                  model_path,
                  predictions_path,

                  label_column = None,
              ):
                  '''Predict values with a CatBoost model.

                  Args:
                      data_path: Path for the data in CSV format.
                      model_path: Path for the trained model in binary CatBoostModel format.
                      label_column: Column containing the label data.
                      predictions_path: Output path for the predictions.

                  Outputs:
                      predictions: Predictions in text format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile

                  from catboost import CatBoost, Pool
                  import numpy

                  if label_column:
                      column_descriptions = {label_column: 'Label'}
                      column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                      with open(column_description_path, 'w') as column_description_file:
                          for idx, kind in column_descriptions.items():
                              column_description_file.write('{}\t{}\n'.format(idx, kind))
                  else:
                      column_description_path = None

                  eval_data = Pool(
                      data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoost()
                  model.load_model(model_path)

                  predictions = model.predict(eval_data, prediction_type='RawFormulaVal')
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost predict values', description='Predict values with a CatBoost model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_predict_values(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_classes/from_CSV/component.yaml
      digest: 6e45f6a3b7a7805aa3c99a9d6c1bbf7434c365ad6f23de930014902bf19dc78f
      text: |
        name: Catboost predict classes
        description: |-
          Predict classes using the CatBoost classifier model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Class predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: data, type: CSV}
        - {name: model, type: CatBoostModel}
        - {name: label_column, type: Integer, optional: true}
        outputs:
        - {name: predictions}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_classes/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.22' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_predict_classes(
                  data_path,
                  model_path,
                  predictions_path,

                  label_column = None,
              ):
                  '''Predict classes using the CatBoost classifier model.

                  Args:
                      data_path: Path for the data in CSV format.
                      model_path: Path for the trained model in binary CatBoostModel format.
                      label_column: Column containing the label data.
                      predictions_path: Output path for the predictions.

                  Outputs:
                      predictions: Class predictions in text format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile

                  from catboost import CatBoostClassifier, Pool
                  import numpy

                  if label_column:
                      column_descriptions = {label_column: 'Label'}
                      column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                      with open(column_description_path, 'w') as column_description_file:
                          for idx, kind in column_descriptions.items():
                              column_description_file.write('{}\t{}\n'.format(idx, kind))
                  else:
                      column_description_path = None

                  eval_data = Pool(
                      data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoostClassifier()
                  model.load_model(model_path)

                  predictions = model.predict(eval_data)
                  numpy.savetxt(predictions_path, predictions, fmt='%s')

              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost predict classes', description='Predict classes using the CatBoost classifier model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Class predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_predict_classes(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml
      digest: 9074952dc787000001b2f75291a287a1cc968648c711a8f648605fd0053999bb
      text: |
        name: Catboost predict class probabilities
        description: |-
          Predict class probabilities with a CatBoost model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: data, type: CSV}
        - {name: model, type: CatBoostModel}
        - {name: label_column, type: Integer, optional: true}
        outputs:
        - {name: predictions}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def catboost_predict_class_probabilities(
                  data_path,
                  model_path,
                  predictions_path,

                  label_column = None,
              ):
                  '''Predict class probabilities with a CatBoost model.

                  Args:
                      data_path: Path for the data in CSV format.
                      model_path: Path for the trained model in binary CatBoostModel format.
                      label_column: Column containing the label data.
                      predictions_path: Output path for the predictions.

                  Outputs:
                      predictions: Predictions in text format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  import tempfile

                  from catboost import CatBoost, Pool
                  import numpy

                  if label_column:
                      column_descriptions = {label_column: 'Label'}
                      column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                      with open(column_description_path, 'w') as column_description_file:
                          for idx, kind in column_descriptions.items():
                              column_description_file.write('{}\t{}\n'.format(idx, kind))
                  else:
                      column_description_path = None

                  eval_data = Pool(
                      data_path,
                      column_description=column_description_path,
                      has_header=True,
                      delimiter=',',
                  )

                  model = CatBoost()
                  model.load_model(model_path)

                  predictions = model.predict(eval_data, prediction_type='Probability')
                  numpy.savetxt(predictions_path, predictions)

              import argparse
              _parser = argparse.ArgumentParser(prog='Catboost predict class probabilities', description='Predict class probabilities with a CatBoost model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = catboost_predict_class_probabilities(**_parsed_args)
            args:
            - --data
            - {inputPath: data}
            - --model
            - {inputPath: model}
            - if:
                cond: {isPresent: label_column}
                then:
                - --label-column
                - {inputValue: label_column}
            - --predictions
            - {outputPath: predictions}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml
      digest: f56b73b1e1e6b9e8745a4dfb49423ec33194adde2aac0b9d20d4c7bf77731907
      text: |
        name: Convert CatBoostModel to ONNX
        description: |-
          Convert CatBoost model to ONNX format.

              Args:
                  model_path: Path of a trained model in binary CatBoost model format.
                  converted_model_path: Output path for the converted model.

              Outputs:
                  converted_model: Model in ONNX format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
        inputs:
        - {name: model, type: CatBoostModel}
        outputs:
        - {name: converted_model, type: ONNX}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml'
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'catboost==0.22' --user) && "$0" "$@"
            - python3
            - -u
            - -c
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def convert_CatBoostModel_to_ONNX(
                  model_path,
                  converted_model_path,
              ):
                  '''Convert CatBoost model to ONNX format.

                  Args:
                      model_path: Path of a trained model in binary CatBoost model format.
                      converted_model_path: Output path for the converted model.

                  Outputs:
                      converted_model: Model in ONNX format.

                  Annotations:
                      author: Alexey Volkov <alexey.volkov@ark-kun.com>
                  '''
                  from catboost import CatBoost

                  model = CatBoost()
                  model.load_model(model_path)
                  model.save_model(converted_model_path, format="onnx")

              import argparse
              _parser = argparse.ArgumentParser(prog='Convert CatBoostModel to ONNX', description='Convert CatBoost model to ONNX format.\n\n    Args:\n        model_path: Path of a trained model in binary CatBoost model format.\n        converted_model_path: Output path for the converted model.\n\n    Outputs:\n        converted_model: Model in ONNX format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
              _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = convert_CatBoostModel_to_ONNX(**_parsed_args)
            args:
            - --model
            - {inputPath: model}
            - --converted-model
            - {outputPath: converted_model}
  - name: Vowpal_Wabbit
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Vowpal_Wabbit/Create_JSON_dataset/from_CSV/component.yaml
      digest: 2e5d07f6df3fa21f7d0438136526d952a0c726302971c2e00ac42fe26856e674
      text: |
        name: Create Vowpal Wabbit JSON dataset from CSV
        metadata:
          annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Create_JSON_dataset/from_CSV/component.yaml'}
        inputs:
        - {name: dataset, type: CSV}
        - {name: label_column_name, type: String, optional: true}
        outputs:
        - {name: converted_dataset, type: VowpalWabbitJsonDataset}
        implementation:
          container:
            image: python:3.9
            command:
            - sh
            - -c
            - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
              'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
              --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && "$0" "$@"
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def create_Vowpal_Wabbit_JSON_dataset_from_CSV(
                  dataset_path,
                  converted_dataset_path,
                  label_column_name = None,
              ):
                  import json
                  import pandas

                  df = pandas.read_csv(dataset_path).convert_dtypes()

                  if label_column_name:
                      label_series = df[label_column_name]
                      features_df = df.drop(columns=[label_column_name])
                      label_values_list = label_series.to_list()
                      feature_records_list = features_df.to_dict("records")

                      with open(converted_dataset_path, "w") as f:
                          for features, label in zip(feature_records_list, label_values_list):
                              non_nan_features = {
                                  k: v for k, v in features.items() if v == v and v is not None
                              }
                              vw_record = {
                                  "_label": label,
                              }
                              vw_record.update(non_nan_features)
                              vw_record_line = json.dumps(vw_record)
                              f.write(vw_record_line + "\n")
                  else:
                      features_df = df
                      feature_records_list = features_df.to_dict("records")

                      with open(converted_dataset_path, "w") as f:
                          for features in feature_records_list:
                              non_nan_features = {
                                  k: v for k, v in features.items() if v == v and v is not None
                              }
                              vw_record = non_nan_features
                              vw_record_line = json.dumps(vw_record)
                              f.write(vw_record_line + "\n")

              import argparse
              _parser = argparse.ArgumentParser(prog='Create Vowpal Wabbit JSON dataset from CSV', description='')
              _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--converted-dataset", dest="converted_dataset_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = create_Vowpal_Wabbit_JSON_dataset_from_CSV(**_parsed_args)
            args:
            - --dataset
            - {inputPath: dataset}
            - if:
                cond: {isPresent: label_column_name}
                then:
                - --label-column-name
                - {inputValue: label_column_name}
            - --converted-dataset
            - {outputPath: converted_dataset}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/a2a629e776d5fa0204ce71370cab23282d3e4278/components/ML_frameworks/Vowpal_Wabbit/Train_regression_model/from_VowpalWabbitJsonDataset/component.yaml
      digest: 601b68212b611b2b798767f4fcf3e77de662688b772daaae0ad3eb55426f56bc
      text: |
        name: Train regression model using Vowpal Wabbit on VowpalWabbitDataset
        metadata:
          annotations:
            author: "Alexey Volkov <alexey.volkov@ark-kun.com>"
            canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Train_regression_model/from_VowpalWabbitJsonDataset/component.yaml"
        inputs:
        - {name: Dataset, type: VowpalWabbitJsonDataset}
        - {name: Initial model, type: VowpalWabbitRegressorModel, optional: true}
        - {name: Number of passes, type: Integer, default: "1"}
        - {name: Loss function, type: String, default: "squared", description: "Supported values: squared, hinge, logistic, quantile, poisson. See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Loss-functions"}
        outputs:
        - {name: Model, type: VowpalWabbitRegressorModel}
        - {name: Readable model, type: VowpalWabbitReadableHashRegressorModel}
        implementation:
          container:
            image: vowpalwabbit/vw-rel-alpine:9.0.1
            # See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-line-arguments
            command:
              - sh
              - -exc
              - |
                # Creating directories for the outputs
                mkdir -p "$(dirname "$4")" # Model
                mkdir -p "$(dirname "$6")" # Readable model
                "$0" "$@"
              - ./vw
              - --data
              - {inputPath: Dataset}
              - --final_regressor
              - {outputPath: Model}
              - --invert_hash
              - {outputPath: Readable model}
              - --passes
              - {inputValue: Number of passes}
              - --loss_function
              - {inputValue: Loss function}
              # Enable JSON parsing
              - --json
              - if:
                  cond: {isPresent: Initial model}
                  then:
                    - --initial_regressor
                    - {inputPath: Initial model}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/a2a629e776d5fa0204ce71370cab23282d3e4278/components/ML_frameworks/Vowpal_Wabbit/Predict/from_VowpalWabbitJsonDataset/component.yaml
      digest: f3eecb52c5f4d31d89090d17217e05f881600f8c7a18fc8de11eea1bf9d5fdf8
      text: |
        name: Predict using Vowpal Wabbit model on VowpalWabbitJsonDataset
        metadata:
          annotations:
            author: "Alexey Volkov <alexey.volkov@ark-kun.com>"
            canonical_location: "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Predict/from_VowpalWabbitJsonDataset/component.yaml"
        inputs:
        - {name: Dataset, type: VowpalWabbitJsonDataset}
        - {name: Model, type: VowpalWabbitRegressorModel}
        outputs:
        - {name: Predictions}
        implementation:
          container:
            image: vowpalwabbit/vw-rel-alpine:9.0.1
            # See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-line-arguments
            command:
              - sh
              - -exc
              - |
                # Creating directories for the outputs
                mkdir -p "$(dirname "$6")" # Predictions
                "$0" "$@"
              - ./vw
              - --data
              - {inputPath: Dataset}
              - --initial_regressor
              - {inputPath: Model}
              - --predictions
              - {outputPath: Predictions}
              # Ignore label information and just test
              - --testonly
              # Enable JSON parsing
              - --json
  - name: TFX
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/CsvExampleGen/component.yaml
      digest: 7f2e484b67b89d822941810268a3b53f82c4651f744065a2086d5ffb8a23f56c
      text: |
        name: CsvExampleGen
        inputs:
        - {name: input_base, type: String}
        - name: input_config
          type:
            JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}
        - name: output_config
          type:
            JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}
        - name: range_config
          type:
            JsonObject: {data_type: 'proto:tfx.configs.RangeConfig'}
          optional: true
        outputs:
        - {name: examples, type: Examples}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/CsvExampleGen/component.yaml'
        implementation:
          container:
            image: tensorflow/tfx:0.29.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def CsvExampleGen(
                  examples_path,
                  input_base,
                  input_config,
                  output_config,
                  range_config = None,
              ):
                  from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen as component_class

                  #Generated code
                  import os
                  import tempfile
                  from tensorflow.io import gfile
                  from google.protobuf import json_format, message
                  from tfx.types import channel_utils, artifact_utils
                  from tfx.components.base import base_executor

                  arguments = locals().copy()

                  component_class_args = {}

                  for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                      argument_value = arguments.get(name, None)
                      if argument_value is None:
                          continue
                      parameter_type = execution_parameter.type
                      if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                          argument_value_obj = parameter_type()
                          json_format.Parse(argument_value, argument_value_obj)
                      else:
                          argument_value_obj = argument_value
                      component_class_args[name] = argument_value_obj

                  for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                      artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel_parameter.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                              # Recovering splits
                              subdirs = gfile.listdir(artifact_path)
                              # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                              subdirs = [subdir.rstrip('/') for subdir in subdirs]
                              split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                              artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                          component_class_args[name] = channel_utils.as_channel([artifact])

                  component_class_instance = component_class(**component_class_args)

                  input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
                  output_dict = {}
                  exec_properties = component_class_instance.exec_properties

                  # Generating paths for output artifacts
                  for name, channel in component_class_instance.outputs.items():
                      artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          artifact_list = [artifact]
                          channel._artifacts = artifact_list
                          output_dict[name] = artifact_list

                  print('component instance: ' + str(component_class_instance))

                  executor_context = base_executor.BaseExecutor.Context(
                      beam_pipeline_args=arguments.get('beam_pipeline_args'),
                      tmp_dir=tempfile.gettempdir(),
                      unique_id='tfx_component',
                  )
                  executor = component_class_instance.executor_spec.executor_class(executor_context)
                  executor.Do(
                      input_dict=input_dict,
                      output_dict=output_dict,
                      exec_properties=exec_properties,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='CsvExampleGen', description='')
              _parser.add_argument("--input-base", dest="input_base", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--input-config", dest="input_config", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--output-config", dest="output_config", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--range-config", dest="range_config", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--examples", dest="examples_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = CsvExampleGen(**_parsed_args)
            args:
            - --input-base
            - {inputValue: input_base}
            - --input-config
            - {inputValue: input_config}
            - --output-config
            - {inputValue: output_config}
            - if:
                cond: {isPresent: range_config}
                then:
                - --range-config
                - {inputValue: range_config}
            - --examples
            - {outputPath: examples}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/StatisticsGen/component.yaml
      digest: 32816822efeb651f196ad25b3d88c4dc2e55613e570537a0a710b4b8ee83562f
      text: |
        name: StatisticsGen
        inputs:
        - {name: examples, type: Examples}
        - {name: schema, type: Schema, optional: true}
        - {name: exclude_splits, type: String, optional: true}
        outputs:
        - {name: statistics, type: ExampleStatistics}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/StatisticsGen/component.yaml'
        implementation:
          container:
            image: tensorflow/tfx:0.29.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def StatisticsGen(
                  examples_path,
                  statistics_path,
                  schema_path = None,
                  exclude_splits = None,
              ):
                  from tfx.components.statistics_gen.component import StatisticsGen as component_class

                  #Generated code
                  import os
                  import tempfile
                  from tensorflow.io import gfile
                  from google.protobuf import json_format, message
                  from tfx.types import channel_utils, artifact_utils
                  from tfx.components.base import base_executor

                  arguments = locals().copy()

                  component_class_args = {}

                  for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                      argument_value = arguments.get(name, None)
                      if argument_value is None:
                          continue
                      parameter_type = execution_parameter.type
                      if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                          argument_value_obj = parameter_type()
                          json_format.Parse(argument_value, argument_value_obj)
                      else:
                          argument_value_obj = argument_value
                      component_class_args[name] = argument_value_obj

                  for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                      artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel_parameter.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                              # Recovering splits
                              subdirs = gfile.listdir(artifact_path)
                              # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                              subdirs = [subdir.rstrip('/') for subdir in subdirs]
                              split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                              artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                          component_class_args[name] = channel_utils.as_channel([artifact])

                  component_class_instance = component_class(**component_class_args)

                  input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
                  output_dict = {}
                  exec_properties = component_class_instance.exec_properties

                  # Generating paths for output artifacts
                  for name, channel in component_class_instance.outputs.items():
                      artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          artifact_list = [artifact]
                          channel._artifacts = artifact_list
                          output_dict[name] = artifact_list

                  print('component instance: ' + str(component_class_instance))

                  executor_context = base_executor.BaseExecutor.Context(
                      beam_pipeline_args=arguments.get('beam_pipeline_args'),
                      tmp_dir=tempfile.gettempdir(),
                      unique_id='tfx_component',
                  )
                  executor = component_class_instance.executor_spec.executor_class(executor_context)
                  executor.Do(
                      input_dict=input_dict,
                      output_dict=output_dict,
                      exec_properties=exec_properties,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='StatisticsGen', description='')
              _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--schema", dest="schema_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--statistics", dest="statistics_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = StatisticsGen(**_parsed_args)
            args:
            - --examples
            - {inputPath: examples}
            - if:
                cond: {isPresent: schema}
                then:
                - --schema
                - {inputPath: schema}
            - if:
                cond: {isPresent: exclude_splits}
                then:
                - --exclude-splits
                - {inputValue: exclude_splits}
            - --statistics
            - {outputPath: statistics}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/SchemaGen/component.yaml
      digest: 83ab7b74d3252b6a82159f898d748857af7a5e6460f9b16109a8132646519eac
      text: |
        name: SchemaGen
        inputs:
        - {name: statistics, type: ExampleStatistics}
        - {name: infer_feature_shape, type: Integer, optional: true}
        - {name: exclude_splits, type: String, optional: true}
        outputs:
        - {name: schema, type: Schema}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/SchemaGen/component.yaml'
        implementation:
          container:
            image: tensorflow/tfx:0.29.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def SchemaGen(
                  statistics_path,
                  schema_path,
                  infer_feature_shape = None,
                  exclude_splits = None,
              ):
                  from tfx.components.schema_gen.component import SchemaGen as component_class

                  #Generated code
                  import os
                  import tempfile
                  from tensorflow.io import gfile
                  from google.protobuf import json_format, message
                  from tfx.types import channel_utils, artifact_utils
                  from tfx.components.base import base_executor

                  arguments = locals().copy()

                  component_class_args = {}

                  for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                      argument_value = arguments.get(name, None)
                      if argument_value is None:
                          continue
                      parameter_type = execution_parameter.type
                      if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                          argument_value_obj = parameter_type()
                          json_format.Parse(argument_value, argument_value_obj)
                      else:
                          argument_value_obj = argument_value
                      component_class_args[name] = argument_value_obj

                  for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                      artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel_parameter.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                              # Recovering splits
                              subdirs = gfile.listdir(artifact_path)
                              # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                              subdirs = [subdir.rstrip('/') for subdir in subdirs]
                              split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                              artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                          component_class_args[name] = channel_utils.as_channel([artifact])

                  component_class_instance = component_class(**component_class_args)

                  input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
                  output_dict = {}
                  exec_properties = component_class_instance.exec_properties

                  # Generating paths for output artifacts
                  for name, channel in component_class_instance.outputs.items():
                      artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          artifact_list = [artifact]
                          channel._artifacts = artifact_list
                          output_dict[name] = artifact_list

                  print('component instance: ' + str(component_class_instance))

                  executor_context = base_executor.BaseExecutor.Context(
                      beam_pipeline_args=arguments.get('beam_pipeline_args'),
                      tmp_dir=tempfile.gettempdir(),
                      unique_id='tfx_component',
                  )
                  executor = component_class_instance.executor_spec.executor_class(executor_context)
                  executor.Do(
                      input_dict=input_dict,
                      output_dict=output_dict,
                      exec_properties=exec_properties,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='SchemaGen', description='')
              _parser.add_argument("--statistics", dest="statistics_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--infer-feature-shape", dest="infer_feature_shape", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--schema", dest="schema_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = SchemaGen(**_parsed_args)
            args:
            - --statistics
            - {inputPath: statistics}
            - if:
                cond: {isPresent: infer_feature_shape}
                then:
                - --infer-feature-shape
                - {inputValue: infer_feature_shape}
            - if:
                cond: {isPresent: exclude_splits}
                then:
                - --exclude-splits
                - {inputValue: exclude_splits}
            - --schema
            - {outputPath: schema}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleValidator/component.yaml
      digest: 1485b7118f740fbf00f64366dbd54cc5776842816722afc016ca2dc648611c60
      text: |
        name: ExampleValidator
        inputs:
        - {name: statistics, type: ExampleStatistics}
        - {name: schema, type: Schema}
        - {name: exclude_splits, type: String, optional: true}
        outputs:
        - {name: anomalies, type: ExampleAnomalies}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleValidator/component.yaml'
        implementation:
          container:
            image: tensorflow/tfx:0.29.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def ExampleValidator(
                  statistics_path,
                  schema_path,
                  anomalies_path,
                  exclude_splits = None,
              ):
                  from tfx.components.example_validator.component import ExampleValidator as component_class

                  #Generated code
                  import os
                  import tempfile
                  from tensorflow.io import gfile
                  from google.protobuf import json_format, message
                  from tfx.types import channel_utils, artifact_utils
                  from tfx.components.base import base_executor

                  arguments = locals().copy()

                  component_class_args = {}

                  for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                      argument_value = arguments.get(name, None)
                      if argument_value is None:
                          continue
                      parameter_type = execution_parameter.type
                      if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                          argument_value_obj = parameter_type()
                          json_format.Parse(argument_value, argument_value_obj)
                      else:
                          argument_value_obj = argument_value
                      component_class_args[name] = argument_value_obj

                  for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                      artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel_parameter.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                              # Recovering splits
                              subdirs = gfile.listdir(artifact_path)
                              # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                              subdirs = [subdir.rstrip('/') for subdir in subdirs]
                              split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                              artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                          component_class_args[name] = channel_utils.as_channel([artifact])

                  component_class_instance = component_class(**component_class_args)

                  input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
                  output_dict = {}
                  exec_properties = component_class_instance.exec_properties

                  # Generating paths for output artifacts
                  for name, channel in component_class_instance.outputs.items():
                      artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          artifact_list = [artifact]
                          channel._artifacts = artifact_list
                          output_dict[name] = artifact_list

                  print('component instance: ' + str(component_class_instance))

                  executor_context = base_executor.BaseExecutor.Context(
                      beam_pipeline_args=arguments.get('beam_pipeline_args'),
                      tmp_dir=tempfile.gettempdir(),
                      unique_id='tfx_component',
                  )
                  executor = component_class_instance.executor_spec.executor_class(executor_context)
                  executor.Do(
                      input_dict=input_dict,
                      output_dict=output_dict,
                      exec_properties=exec_properties,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='ExampleValidator', description='')
              _parser.add_argument("--statistics", dest="statistics_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--schema", dest="schema_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--anomalies", dest="anomalies_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = ExampleValidator(**_parsed_args)
            args:
            - --statistics
            - {inputPath: statistics}
            - --schema
            - {inputPath: schema}
            - if:
                cond: {isPresent: exclude_splits}
                then:
                - --exclude-splits
                - {inputValue: exclude_splits}
            - --anomalies
            - {outputPath: anomalies}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Transform/component.yaml
      digest: 1a3ac504652bbdd592ba640fbc6c6da7c3d877022b17c2b3d759ef8075d16daf
      text: |
        name: Transform
        inputs:
        - {name: examples, type: Examples}
        - {name: schema, type: Schema}
        - {name: analyzer_cache, type: TransformCache, optional: true}
        - {name: module_file, type: String, optional: true}
        - {name: preprocessing_fn, type: String, optional: true}
        - {name: force_tf_compat_v1, type: Integer, optional: true}
        - {name: custom_config, type: String, optional: true}
        - name: splits_config
          type:
            JsonObject: {data_type: 'proto:tfx.components.transform.SplitsConfig'}
          optional: true
        outputs:
        - {name: transform_graph, type: TransformGraph}
        - {name: transformed_examples, type: Examples}
        - {name: updated_analyzer_cache, type: TransformCache}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Transform/component.yaml'
        implementation:
          container:
            image: tensorflow/tfx:0.29.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def Transform(
                  examples_path,
                  schema_path,
                  transform_graph_path,
                  transformed_examples_path,
                  updated_analyzer_cache_path,
                  analyzer_cache_path = None,
                  module_file = None,
                  preprocessing_fn = None,
                  force_tf_compat_v1 = None,
                  custom_config = None,
                  splits_config = None,
              ):
                  from tfx.components.transform.component import Transform as component_class

                  #Generated code
                  import os
                  import tempfile
                  from tensorflow.io import gfile
                  from google.protobuf import json_format, message
                  from tfx.types import channel_utils, artifact_utils
                  from tfx.components.base import base_executor

                  arguments = locals().copy()

                  component_class_args = {}

                  for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                      argument_value = arguments.get(name, None)
                      if argument_value is None:
                          continue
                      parameter_type = execution_parameter.type
                      if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                          argument_value_obj = parameter_type()
                          json_format.Parse(argument_value, argument_value_obj)
                      else:
                          argument_value_obj = argument_value
                      component_class_args[name] = argument_value_obj

                  for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                      artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel_parameter.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                              # Recovering splits
                              subdirs = gfile.listdir(artifact_path)
                              # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                              subdirs = [subdir.rstrip('/') for subdir in subdirs]
                              split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                              artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                          component_class_args[name] = channel_utils.as_channel([artifact])

                  component_class_instance = component_class(**component_class_args)

                  input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
                  output_dict = {}
                  exec_properties = component_class_instance.exec_properties

                  # Generating paths for output artifacts
                  for name, channel in component_class_instance.outputs.items():
                      artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          artifact_list = [artifact]
                          channel._artifacts = artifact_list
                          output_dict[name] = artifact_list

                  print('component instance: ' + str(component_class_instance))

                  executor_context = base_executor.BaseExecutor.Context(
                      beam_pipeline_args=arguments.get('beam_pipeline_args'),
                      tmp_dir=tempfile.gettempdir(),
                      unique_id='tfx_component',
                  )
                  executor = component_class_instance.executor_spec.executor_class(executor_context)
                  executor.Do(
                      input_dict=input_dict,
                      output_dict=output_dict,
                      exec_properties=exec_properties,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='Transform', description='')
              _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--schema", dest="schema_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--analyzer-cache", dest="analyzer_cache_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--preprocessing-fn", dest="preprocessing_fn", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--force-tf-compat-v1", dest="force_tf_compat_v1", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--custom-config", dest="custom_config", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--splits-config", dest="splits_config", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-graph", dest="transform_graph_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transformed-examples", dest="transformed_examples_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--updated-analyzer-cache", dest="updated_analyzer_cache_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = Transform(**_parsed_args)
            args:
            - --examples
            - {inputPath: examples}
            - --schema
            - {inputPath: schema}
            - if:
                cond: {isPresent: analyzer_cache}
                then:
                - --analyzer-cache
                - {inputPath: analyzer_cache}
            - if:
                cond: {isPresent: module_file}
                then:
                - --module-file
                - {inputValue: module_file}
            - if:
                cond: {isPresent: preprocessing_fn}
                then:
                - --preprocessing-fn
                - {inputValue: preprocessing_fn}
            - if:
                cond: {isPresent: force_tf_compat_v1}
                then:
                - --force-tf-compat-v1
                - {inputValue: force_tf_compat_v1}
            - if:
                cond: {isPresent: custom_config}
                then:
                - --custom-config
                - {inputValue: custom_config}
            - if:
                cond: {isPresent: splits_config}
                then:
                - --splits-config
                - {inputValue: splits_config}
            - --transform-graph
            - {outputPath: transform_graph}
            - --transformed-examples
            - {outputPath: transformed_examples}
            - --updated-analyzer-cache
            - {outputPath: updated_analyzer_cache}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Trainer/component.yaml
      digest: 343627821fb4979e40edadf8cd4a40624dd6eeeaac3156aec790e85ecc546303
      text: |
        name: Trainer
        inputs:
        - {name: examples, type: Examples}
        - name: train_args
          type:
            JsonObject: {data_type: 'proto:tfx.components.trainer.TrainArgs'}
        - name: eval_args
          type:
            JsonObject: {data_type: 'proto:tfx.components.trainer.EvalArgs'}
        - {name: transform_graph, type: TransformGraph, optional: true}
        - {name: schema, type: Schema, optional: true}
        - {name: base_model, type: Model, optional: true}
        - {name: hyperparameters, type: HyperParameters, optional: true}
        - {name: module_file, type: String, optional: true}
        - {name: run_fn, type: String, optional: true}
        - {name: trainer_fn, type: String, optional: true}
        - {name: custom_config, type: String, optional: true}
        outputs:
        - {name: model, type: Model}
        - {name: model_run, type: ModelRun}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Trainer/component.yaml'
        implementation:
          container:
            image: tensorflow/tfx:0.29.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def Trainer(
                  examples_path,
                  model_path,
                  model_run_path,
                  train_args,
                  eval_args,
                  transform_graph_path = None,
                  schema_path = None,
                  base_model_path = None,
                  hyperparameters_path = None,
                  module_file = None,
                  run_fn = None,
                  trainer_fn = None,
                  custom_config = None,
              ):
                  from tfx.components.trainer.component import Trainer as component_class

                  #Generated code
                  import os
                  import tempfile
                  from tensorflow.io import gfile
                  from google.protobuf import json_format, message
                  from tfx.types import channel_utils, artifact_utils
                  from tfx.components.base import base_executor

                  arguments = locals().copy()

                  component_class_args = {}

                  for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                      argument_value = arguments.get(name, None)
                      if argument_value is None:
                          continue
                      parameter_type = execution_parameter.type
                      if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                          argument_value_obj = parameter_type()
                          json_format.Parse(argument_value, argument_value_obj)
                      else:
                          argument_value_obj = argument_value
                      component_class_args[name] = argument_value_obj

                  for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                      artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel_parameter.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                              # Recovering splits
                              subdirs = gfile.listdir(artifact_path)
                              # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                              subdirs = [subdir.rstrip('/') for subdir in subdirs]
                              split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                              artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                          component_class_args[name] = channel_utils.as_channel([artifact])

                  component_class_instance = component_class(**component_class_args)

                  input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
                  output_dict = {}
                  exec_properties = component_class_instance.exec_properties

                  # Generating paths for output artifacts
                  for name, channel in component_class_instance.outputs.items():
                      artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          artifact_list = [artifact]
                          channel._artifacts = artifact_list
                          output_dict[name] = artifact_list

                  print('component instance: ' + str(component_class_instance))

                  executor_context = base_executor.BaseExecutor.Context(
                      beam_pipeline_args=arguments.get('beam_pipeline_args'),
                      tmp_dir=tempfile.gettempdir(),
                      unique_id='tfx_component',
                  )
                  executor = component_class_instance.executor_spec.executor_class(executor_context)
                  executor.Do(
                      input_dict=input_dict,
                      output_dict=output_dict,
                      exec_properties=exec_properties,
                  )

              import argparse
              _parser = argparse.ArgumentParser(prog='Trainer', description='')
              _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--train-args", dest="train_args", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--eval-args", dest="eval_args", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--transform-graph", dest="transform_graph_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--schema", dest="schema_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--base-model", dest="base_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--hyperparameters", dest="hyperparameters_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--run-fn", dest="run_fn", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--trainer-fn", dest="trainer_fn", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--custom-config", dest="custom_config", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model-run", dest="model_run_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = Trainer(**_parsed_args)
            args:
            - --examples
            - {inputPath: examples}
            - --train-args
            - {inputValue: train_args}
            - --eval-args
            - {inputValue: eval_args}
            - if:
                cond: {isPresent: transform_graph}
                then:
                - --transform-graph
                - {inputPath: transform_graph}
            - if:
                cond: {isPresent: schema}
                then:
                - --schema
                - {inputPath: schema}
            - if:
                cond: {isPresent: base_model}
                then:
                - --base-model
                - {inputPath: base_model}
            - if:
                cond: {isPresent: hyperparameters}
                then:
                - --hyperparameters
                - {inputPath: hyperparameters}
            - if:
                cond: {isPresent: module_file}
                then:
                - --module-file
                - {inputValue: module_file}
            - if:
                cond: {isPresent: run_fn}
                then:
                - --run-fn
                - {inputValue: run_fn}
            - if:
                cond: {isPresent: trainer_fn}
                then:
                - --trainer-fn
                - {inputValue: trainer_fn}
            - if:
                cond: {isPresent: custom_config}
                then:
                - --custom-config
                - {inputValue: custom_config}
            - --model
            - {outputPath: model}
            - --model-run
            - {outputPath: model_run}
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Evaluator/component.yaml
      digest: 7c046e59173ffbafcc845954c606657f8898424140012168b4d53a6394f67d10
      text: |
        name: Evaluator
        inputs:
        - {name: examples, type: Examples}
        - {name: model, type: Model, optional: true}
        - {name: baseline_model, type: Model, optional: true}
        - {name: schema, type: Schema, optional: true}
        - name: eval_config
          type:
            JsonObject: {data_type: 'proto:tensorflow_model_analysis.EvalConfig'}
          optional: true
        - name: feature_slicing_spec
          type:
            JsonObject: {data_type: 'proto:tfx.components.evaluator.FeatureSlicingSpec'}
          optional: true
        - {name: fairness_indicator_thresholds, type: JsonArray, optional: true}
        - {name: example_splits, type: String, optional: true}
        - {name: module_file, type: String, optional: true}
        - {name: module_path, type: String, optional: true}
        outputs:
        - {name: evaluation, type: ModelEvaluation}
        - {name: blessing, type: ModelBlessing}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Evaluator/component.yaml'
        implementation:
          container:
            image: tensorflow/tfx:0.29.0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def Evaluator(
                  examples_path,
                  evaluation_path,
                  blessing_path,
                  model_path = None,
                  baseline_model_path = None,
                  schema_path = None,
                  eval_config = None,
                  feature_slicing_spec = None,
                  fairness_indicator_thresholds = None,
                  example_splits = None,
                  module_file = None,
                  module_path = None,
              ):
                  from tfx.components.evaluator.component import Evaluator as component_class

                  #Generated code
                  import os
                  import tempfile
                  from tensorflow.io import gfile
                  from google.protobuf import json_format, message
                  from tfx.types import channel_utils, artifact_utils
                  from tfx.components.base import base_executor

                  arguments = locals().copy()

                  component_class_args = {}

                  for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                      argument_value = arguments.get(name, None)
                      if argument_value is None:
                          continue
                      parameter_type = execution_parameter.type
                      if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                          argument_value_obj = parameter_type()
                          json_format.Parse(argument_value, argument_value_obj)
                      else:
                          argument_value_obj = argument_value
                      component_class_args[name] = argument_value_obj

                  for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                      artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel_parameter.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                              # Recovering splits
                              subdirs = gfile.listdir(artifact_path)
                              # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                              subdirs = [subdir.rstrip('/') for subdir in subdirs]
                              split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                              artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                          component_class_args[name] = channel_utils.as_channel([artifact])

                  component_class_instance = component_class(**component_class_args)

                  input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
                  output_dict = {}
                  exec_properties = component_class_instance.exec_properties

                  # Generating paths for output artifacts
                  for name, channel in component_class_instance.outputs.items():
                      artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                      if artifact_path:
                          artifact = channel.type()
                          artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                          artifact_list = [artifact]
                          channel._artifacts = artifact_list
                          output_dict[name] = artifact_list

                  print('component instance: ' + str(component_class_instance))

                  executor_context = base_executor.BaseExecutor.Context(
                      beam_pipeline_args=arguments.get('beam_pipeline_args'),
                      tmp_dir=tempfile.gettempdir(),
                      unique_id='tfx_component',
                  )
                  executor = component_class_instance.executor_spec.executor_class(executor_context)
                  executor.Do(
                      input_dict=input_dict,
                      output_dict=output_dict,
                      exec_properties=exec_properties,
                  )

              import json
              import argparse
              _parser = argparse.ArgumentParser(prog='Evaluator', description='')
              _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--model", dest="model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--baseline-model", dest="baseline_model_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--schema", dest="schema_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--eval-config", dest="eval_config", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--feature-slicing-spec", dest="feature_slicing_spec", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--fairness-indicator-thresholds", dest="fairness_indicator_thresholds", type=json.loads, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--example-splits", dest="example_splits", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--module-path", dest="module_path", type=str, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--evaluation", dest="evaluation_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--blessing", dest="blessing_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = Evaluator(**_parsed_args)
            args:
            - --examples
            - {inputPath: examples}
            - if:
                cond: {isPresent: model}
                then:
                - --model
                - {inputPath: model}
            - if:
                cond: {isPresent: baseline_model}
                then:
                - --baseline-model
                - {inputPath: baseline_model}
            - if:
                cond: {isPresent: schema}
                then:
                - --schema
                - {inputPath: schema}
            - if:
                cond: {isPresent: eval_config}
                then:
                - --eval-config
                - {inputValue: eval_config}
            - if:
                cond: {isPresent: feature_slicing_spec}
                then:
                - --feature-slicing-spec
                - {inputValue: feature_slicing_spec}
            - if:
                cond: {isPresent: fairness_indicator_thresholds}
                then:
                - --fairness-indicator-thresholds
                - {inputValue: fairness_indicator_thresholds}
            - if:
                cond: {isPresent: example_splits}
                then:
                - --example-splits
                - {inputValue: example_splits}
            - if:
                cond: {isPresent: module_file}
                then:
                - --module-file
                - {inputValue: module_file}
            - if:
                cond: {isPresent: module_path}
                then:
                - --module-path
                - {inputValue: module_path}
            - --evaluation
            - {outputPath: evaluation}
            - --blessing
            - {outputPath: blessing}
- name: Converters
  components:
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_CSV/component.yaml
    digest: 01be9da9b7e2c1d63a72090cb1c47abf9a581b037c4a477584fbfdf1dcf85c86
    text: |
      name: Convert csv to apache parquet
      description: |-
        Converts CSV table to Apache Parquet.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: CSV}
      outputs:
      - {name: output_data, type: ApacheParquet}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_CSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
            --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_csv_to_apache_parquet(
                data_path,
                output_data_path,
            ):
                '''Converts CSV table to Apache Parquet.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import csv, parquet

                table = csv.read_csv(data_path)
                parquet.write_table(table, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert csv to apache parquet', description='Converts CSV table to Apache Parquet.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_csv_to_apache_parquet(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_TSV/component.yaml
    digest: 50b66cb4e967d5fd1ea3ca0e9de8956b1f313c9f3abb271ca426b050d4573eac
    text: |
      name: Convert tsv to apache parquet
      description: |-
        Converts TSV table to Apache Parquet.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: TSV}
      outputs:
      - {name: output_data, type: ApacheParquet}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_TSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
            --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_tsv_to_apache_parquet(
                data_path,
                output_data_path,
            ):
                '''Converts TSV table to Apache Parquet.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import csv, parquet

                table = csv.read_csv(data_path, parse_options=csv.ParseOptions(delimiter='\t'))
                parquet.write_table(table, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert tsv to apache parquet', description='Converts TSV table to Apache Parquet.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_tsv_to_apache_parquet(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml
    digest: 9ad7aee6ca2fb82841c30cf2995b75c063f19b1f9c935bda1a9ef3d8ac1efed6
    text: |
      name: Convert apache arrow feather to apache parquet
      description: |-
        Converts Apache Arrow Feather to Apache Parquet.

            [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheArrowFeather}
      outputs:
      - {name: output_data, type: ApacheParquet}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
            --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_arrow_feather_to_apache_parquet(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Arrow Feather to Apache Parquet.

                [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import feather, parquet

                table = feather.read_table(data_path)
                parquet.write_table(table, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache arrow feather to apache parquet', description='Converts Apache Arrow Feather to Apache Parquet.\n\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_apache_arrow_feather_to_apache_parquet(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_CSV/component.yaml
    digest: fc9d4d0537d59e61dae838cbce8bbc58a338ea5e87eb504f941ef35965e08cc9
    text: |
      name: Convert apache parquet to csv
      description: |-
        Converts Apache Parquet to CSV.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheParquet}
      outputs:
      - {name: output_data, type: CSV}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_CSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'
            --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_parquet_to_csv(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Parquet to CSV.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import parquet

                data_frame = parquet.read_pandas(data_path).to_pandas()
                data_frame.to_csv(
                    output_data_path,
                    index=False,
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache parquet to csv', description='Converts Apache Parquet to CSV.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_apache_parquet_to_csv(**_parsed_args)
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_TSV/component.yaml
    digest: 7812764bbc6b9bdd4d71e406dea9575943c084ba7799cafd7e378085d76d4c42
    text: |
      name: Convert apache parquet to tsv
      description: |-
        Converts Apache Parquet to TSV.

            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheParquet}
      outputs:
      - {name: output_data, type: TSV}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_TSV/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'
            --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_parquet_to_tsv(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Parquet to TSV.

                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import parquet

                data_frame = parquet.read_pandas(data_path).to_pandas()
                data_frame.to_csv(
                    output_data_path,
                    index=False,
                    sep='\t',
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache parquet to tsv', description='Converts Apache Parquet to TSV.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_apache_parquet_to_tsv(**_parsed_args)
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml
    digest: 53a6051a8cac9ce5eb5fe0950c40c48b5ec78ecddbe76bbafdeffbc586f280a6
    text: |
      name: Convert apache parquet to apache arrow feather
      description: |-
        Converts Apache Parquet to Apache Arrow Feather.

            [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
            [Apache Parquet](https://parquet.apache.org/)

            Annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
      inputs:
      - {name: data, type: ApacheParquet}
      outputs:
      - {name: output_data, type: ApacheArrowFeather}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml'
      implementation:
        container:
          image: python:3.7
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'
            --user) && "$0" "$@"
          - python3
          - -u
          - -c
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_apache_parquet_to_apache_arrow_feather(
                data_path,
                output_data_path,
            ):
                '''Converts Apache Parquet to Apache Arrow Feather.

                [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
                [Apache Parquet](https://parquet.apache.org/)

                Annotations:
                    author: Alexey Volkov <alexey.volkov@ark-kun.com>
                '''
                from pyarrow import feather, parquet

                data_frame = parquet.read_pandas(data_path).to_pandas()
                feather.write_feather(data_frame, output_data_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert apache parquet to apache arrow feather', description='Converts Apache Parquet to Apache Arrow Feather.\n\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
            _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = convert_apache_parquet_to_apache_arrow_feather(**_parsed_args)

            _output_serializers = [

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          args:
          - --data
          - {inputPath: data}
          - --output-data
          - {outputPath: output_data}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml
    digest: f82823ba99ab6fc6696e37685a00918aa94a9186b0a1979206b86ca3818099a4
    text: |
      name: Convert to XGBoostJsonModel from XGBoostModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostModel}
      outputs:
      - {name: converted_model, type: XGBoostJsonModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'xgboost==1.5.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_XGBoostJsonModel_from_XGBoostModel(
                model_path,
                converted_model_path,
            ):
                import os
                import xgboost

                model = xgboost.Booster(model_file=model_path)

                # The file path needs to have .json extension so that the model is saved in the JSON format.
                tmp_converted_model_path = converted_model_path + ".json"
                model.save_model(tmp_converted_model_path)
                os.rename(tmp_converted_model_path, converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to XGBoostJsonModel from XGBoostModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_XGBoostJsonModel_from_XGBoostModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml
    digest: b45de01b867736c16b2a68f53aa808c02f8e0caa2bffe2172aad7a20cdd17063
    text: |
      name: Convert to XGBoostModel from XGBoostJsonModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostJsonModel}
      outputs:
      - {name: converted_model, type: XGBoostModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'xgboost==1.5.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_XGBoostModel_from_XGBoostJsonModel(
                model_path,
                converted_model_path,
            ):
                import os
                import shutil
                import tempfile
                import xgboost

                # The file path needs to have .json extension so that the model is loaded as JSON format.
                with tempfile.NamedTemporaryFile(suffix=".json") as tmp_model_file:
                    tmp_model_path = tmp_model_file.name
                    shutil.copy(model_path, tmp_model_path)
                    model = xgboost.Booster(model_file=tmp_model_path)

                tmp_converted_model_path = converted_model_path + ".bst"
                model.save_model(tmp_converted_model_path)
                os.rename(tmp_converted_model_path, converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to XGBoostModel from XGBoostJsonModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_XGBoostModel_from_XGBoostJsonModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/4619f84eebc54230153ba48b0e67ac8446dc31c6/components/_converters/OnnxModel/from_XGBoostJsonModel/component.yaml
    digest: 5e02e5fa2f12181a6b9588b152e9ee78d4952fea7f3d5740dfdbd15b3b2c0ee9
    text: |
      name: Convert to OnnxModel from XGBoostJsonModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_XGBoostJsonModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostJsonModel}
      - {name: model_graph_name, type: String, optional: true}
      - {name: doc_string, type: String, default: '', optional: true}
      - {name: target_opset, type: Integer, optional: true}
      outputs:
      - {name: converted_model, type: OnnxModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.2' 'onnx==1.11.0' 'onnxmltools==1.10.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
            python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.5.2' 'onnx==1.11.0'
            'onnxmltools==1.10.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_OnnxModel_from_XGBoostJsonModel(
                model_path,
                converted_model_path,
                model_graph_name = None,
                doc_string = "",
                target_opset = None,
            ):
                import xgboost
                import onnx
                import onnxmltools

                # The file path needs to have .json extension so that the model is loaded as JSON format.
                import os
                import shutil
                import tempfile
                with tempfile.NamedTemporaryFile(suffix=".json") as tmp_model_file:
                    tmp_model_path = tmp_model_file.name
                    shutil.copy(model_path, tmp_model_path)
                    model = xgboost.Booster(model_file=tmp_model_path)

                # Workaround for https://github.com/onnx/onnxmltools/issues/499
                # Although I'm not sure this formula is correct given https://github.com/dmlc/xgboost/pull/6569
                model.best_ntree_limit = model.num_boosted_rounds()

                converted_model = onnxmltools.convert_xgboost(
                    model=model,
                    name=model_graph_name,
                    initial_types=[
                        (
                            "input",
                            onnxmltools.convert.common.data_types.FloatTensorType(
                                shape=[None, model.num_features()]
                            ),
                        )
                    ],
                    doc_string=doc_string,
                    target_opset=target_opset,
                )
                onnx.save_model(proto=converted_model, f=converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from XGBoostJsonModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-graph-name", dest="model_graph_name", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--doc-string", dest="doc_string", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--target-opset", dest="target_opset", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_OnnxModel_from_XGBoostJsonModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: model_graph_name}
              then:
              - --model-graph-name
              - {inputValue: model_graph_name}
          - if:
              cond: {isPresent: doc_string}
              then:
              - --doc-string
              - {inputValue: doc_string}
          - if:
              cond: {isPresent: target_opset}
              then:
              - --target-opset
              - {inputValue: target_opset}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/ae9d04e833d973d785809e7734021b06bcfea9bc/components/_converters/OnnxModel/from_XGBoostModel/component.yaml
    digest: 3cdc24652d1ab9b436dbe935d291cd03a1d6e14c039f8fc6e67e25ff6f80b0e5
    text: |
      name: Convert to OnnxModel from XGBoostModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_XGBoostModel/component.yaml'}
      inputs:
      - {name: model, type: XGBoostModel}
      - {name: model_graph_name, type: String, optional: true}
      - {name: doc_string, type: String, default: '', optional: true}
      - {name: target_opset, type: Integer, optional: true}
      outputs:
      - {name: converted_model, type: OnnxModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'xgboost==1.5.2' 'onnx==1.11.0' 'onnxmltools==1.10.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
            python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.5.2' 'onnx==1.11.0'
            'onnxmltools==1.10.0' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_OnnxModel_from_XGBoostModel(
                model_path,
                converted_model_path,
                model_graph_name = None,
                doc_string = "",
                target_opset = None,
            ):
                import xgboost
                import onnx
                import onnxmltools

                model = xgboost.Booster(model_file=model_path)

                # Workaround for https://github.com/onnx/onnxmltools/issues/499
                # Although I'm not sure this formula is correct given https://github.com/dmlc/xgboost/pull/6569
                model.best_ntree_limit = model.num_boosted_rounds()

                converted_model = onnxmltools.convert_xgboost(
                    model=model,
                    name=model_graph_name,
                    initial_types=[
                        (
                            "input",
                            onnxmltools.convert.common.data_types.FloatTensorType(
                                shape=[None, model.num_features()]
                            ),
                        )
                    ],
                    doc_string=doc_string,
                    target_opset=target_opset,
                )
                onnx.save_model(proto=converted_model, f=converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from XGBoostModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-graph-name", dest="model_graph_name", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--doc-string", dest="doc_string", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--target-opset", dest="target_opset", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_OnnxModel_from_XGBoostModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: model_graph_name}
              then:
              - --model-graph-name
              - {inputValue: model_graph_name}
          - if:
              cond: {isPresent: doc_string}
              then:
              - --doc-string
              - {inputValue: doc_string}
          - if:
              cond: {isPresent: target_opset}
              then:
              - --target-opset
              - {inputValue: target_opset}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml
    digest: d44f6b94387edfc6560bf452c5662de431d2e1141b670e7ab9e921eba25396a8
    text: |
      name: To ONNX from Tensorflow SavedModel
      inputs:
      - {name: Model, type: TensorflowSavedModel}
      outputs:
      - {name: Model, type: OnnxModel}
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml'
      implementation:
        container:
          image: tensorflow/tensorflow:2.3.0
          command:
          - sh
          - -exc
          - python3 -m pip install tf2onnx==1.6.3 && "$0" "$@"
          - python3
          - -m
          - tf2onnx.convert
          - --saved-model
          - {inputPath: Model}
          - --output
          - {outputPath: Model}
          - --fold_const
          - --verbose
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/2fc275072568cd0cdf73743ab49aa90928303f2c/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml
    digest: 93f892f4f2bdace722e473cae22141715b6328bdc0a508cd6878f050449ff92d
    text: |
      name: Convert to TensorflowSavedModel from OnnxModel
      metadata:
        annotations:
          author: Alexey Volkov <alexey.volkov@ark-kun.com>
          canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml'
      inputs:
      - {name: model, type: OnnxModel}
      outputs:
      - {name: converted_model, type: TensorflowSavedModel}
      implementation:
        container:
          image: tensorflow/tensorflow:2.8.0
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'onnx-tf==1.9.0' 'onnx==1.11.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
            pip install --quiet --no-warn-script-location 'onnx-tf==1.9.0' 'onnx==1.11.0'
            --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_TensorflowSavedModel_from_OnnxModel(
                model_path,
                converted_model_path,
            ):
                import onnx
                import onnx_tf

                onnx_model = onnx.load(model_path)
                tf_rep = onnx_tf.backend.prepare(onnx_model)
                tf_rep.export_graph(converted_model_path)

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to TensorflowSavedModel from OnnxModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_TensorflowSavedModel_from_OnnxModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - --converted-model
          - {outputPath: converted_model}
  - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/f7f79decd10955b8408541f6f695a614538a6901/components/_converters/ScikitLearnPickleModel/to_OnnxModel/component.yaml
    digest: 1899985bd972ba5849be119fc841633e940f293f2314ae52d5cdd9ace2fda15f
    text: |
      name: Convert to OnnxModel from ScikitLearnPickleModel
      metadata:
        annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ScikitLearnPickleModel/to_OnnxModel/component.yaml'}
      inputs:
      - {name: model, type: ScikitLearnPickleModel}
      - {name: doc_string, type: String, default: '', optional: true}
      - {name: target_opset, type: Integer, optional: true}
      outputs:
      - {name: converted_model, type: OnnxModel}
      implementation:
        container:
          image: python:3.9
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'skl2onnx==1.11' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'skl2onnx==1.11' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def convert_to_OnnxModel_from_ScikitLearnPickleModel(
                model_path,
                converted_model_path,
                doc_string = "",
                target_opset = None,
            ):
                import onnx
                import pickle
                import skl2onnx

                with open(model_path, "rb") as model_file:
                    model = pickle.load(model_file)

                # Funny hack to infer the model input shape
                # Just try passing arrays of different size to the model.predict method and check what works, lol.
                def get_input_output_shapes(model):
                    for input_length_candidate in range(100000):
                        try:
                            prediction = model.predict(X=[[0.0] * input_length_candidate])
                            input_length = input_length_candidate
                            output_shape = prediction.shape[1:]
                            return (input_length, output_shape)
                        except:
                            pass
                    return None

                input_length, _ = get_input_output_shapes(model)

                # Setting model name is not necessary, but why not.
                model_type = type(model)
                model_type_name = model_type.__module__ + "." + model_type.__name__

                onnx_model = skl2onnx.convert_sklearn(
                    model=model,
                    initial_types=[
                        ("input", skl2onnx.common.data_types.FloatTensorType([None, input_length]))
                    ],
                    name=model_type_name,
                    verbose=1,
                    # TODO: Include the original model hash digest so that the model can be traced.
                    doc_string=doc_string,
                    target_opset=target_opset,
                )
                print(onnx_model)
                onnx.save_model(
                    proto=onnx_model, f=converted_model_path,
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from ScikitLearnPickleModel', description='')
            _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--doc-string", dest="doc_string", type=str, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--target-opset", dest="target_opset", type=int, required=False, default=argparse.SUPPRESS)
            _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = convert_to_OnnxModel_from_ScikitLearnPickleModel(**_parsed_args)
          args:
          - --model
          - {inputPath: model}
          - if:
              cond: {isPresent: doc_string}
              then:
              - --doc-string
              - {inputValue: doc_string}
          - if:
              cond: {isPresent: target_opset}
              then:
              - --target-opset
              - {inputValue: target_opset}
          - --converted-model
          - {outputPath: converted_model}
- name: Google Cloud
  folders:
  - name: Vertex AI
    folders:
    - name: AutoML
      components:
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_CSV/component.yaml
        digest: ef9dcd63041f77cbbd9df81637a115eb5765e1d11120b867e1b2d78bb2383bb4
        text: |
          name: Create tabular dataset from CSV for Google Cloud Vertex AI
          description: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in
            GCS.
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml'}
          inputs:
          - {name: data, type: CSV, description: Data in CSV format that should be imported
              into the dataset.}
          - name: display_name
            type: String
            description: |-
              Display name for the AutoML Dataset.
              Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
            optional: true
          - {name: labels, type: JsonObject, optional: true}
          - {name: project, type: String, description: 'Google Cloud project ID. If not set,
              the default one will be used.', optional: true}
          - {name: location, type: String, description: Google Cloud region. AutoML Tables only
              supports us-central1., default: us-central1, optional: true}
          - {name: encryption_spec_key_name, type: String, optional: true}
          - {name: staging_bucket, type: String, optional: true}
          outputs:
          - {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}
          - {name: dataset_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'
                'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                -m pip install --quiet --no-warn-script-location 'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'
                'google-api-python-client==2.29.0' --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def create_tabular_dataset_from_CSV_for_Google_Cloud_Vertex_AI(
                    data_path,  # data_type: "CSV"
                    display_name = None,
                    labels = None,
                    project = None,
                    location = 'us-central1',
                    encryption_spec_key_name = None,
                    staging_bucket = None,
                ):
                    '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.

                    Annotations:
                        author: Alexey Volkov <alexey.volkov@ark-kun.com>

                    Args:
                        data_path: Data in CSV format that should be imported into the dataset.
                        display_name: Display name for the AutoML Dataset.
                            Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                        project: Google Cloud project ID. If not set, the default one will be used.
                        location: Google Cloud region. AutoML Tables only supports us-central1.
                    Returns:
                        dataset_name: Dataset name (fully-qualified)
                        dataset_dict: Dataset object in JSON format
                    '''

                    import datetime
                    import json
                    import logging
                    import os
                    import tempfile

                    from google.cloud import aiplatform
                    from google.cloud.aiplatform import utils as aiplatform_utils
                    from google.protobuf import json_format

                    logging.getLogger().setLevel(logging.INFO)

                    if not display_name:
                        display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

                    # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
                    # This leads to failure when trying to create any resource in the project.
                    # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
                    # We can try and get the GCP project ID/number from the environment variables.
                    if not project:
                        project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                        if project_number:
                            print(f"Inferred project number: {project_number}")
                            project = project_number
                            # To improve the naming we try to convert the project number into the user project ID.
                            try:
                                from googleapiclient import discovery

                                cloud_resource_manager_service = discovery.build(
                                    "cloudresourcemanager", "v3"
                                )
                                project_id = (
                                    cloud_resource_manager_service.projects()
                                    .get(name=f"projects/{project_number}")
                                    .execute()["projectId"]
                                )
                                if project_id:
                                    print(f"Inferred project ID: {project_id}")
                                    project = project_id
                            except Exception as e:
                                print(e)

                    if not location:
                        location = os.environ.get("CLOUD_ML_REGION")

                    aiplatform.init(
                        project=project,
                        location=location,
                        staging_bucket=staging_bucket,
                        encryption_spec_key_name=encryption_spec_key_name,
                    )

                    # Stage the data
                    # The file needs to have .CSV extension, so we need to rename or link it.
                    staging_dir = tempfile.mkdtemp()
                    staged_data_path = os.path.join(staging_dir, "dataset.csv")
                    # Need to use symlink to prevent OSError: [Errno 18] Invalid cross-device link.
                    os.symlink(src=data_path, dst=staged_data_path)
                    staged_data_uri = aiplatform_utils.stage_local_data_in_gcs(data_path=staged_data_path)

                    labels = labels or {}
                    labels["component-source"] = "github-com-ark-kun-pipeline-components"

                    # Create the dataset
                    dataset = aiplatform.TabularDataset.create(
                        display_name=display_name,
                        gcs_source=staged_data_uri,
                        labels=labels,
                    )
                    (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')
                    dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'
                    logging.info(f'Created dataset {dataset.name}.')
                    logging.info(f'Link: {dataset_web_url}')
                    dataset_json = json.dumps(dataset.to_dict(), indent=2)
                    print(dataset_json)
                    return (dataset.resource_name, dataset_json, dataset_web_url)

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import json
                import argparse
                _parser = argparse.ArgumentParser(prog='Create tabular dataset from CSV for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')
                _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--labels", dest="labels", type=json.loads, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--staging-bucket", dest="staging_bucket", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = create_tabular_dataset_from_CSV_for_Google_Cloud_Vertex_AI(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --data
              - {inputPath: data}
              - if:
                  cond: {isPresent: display_name}
                  then:
                  - --display-name
                  - {inputValue: display_name}
              - if:
                  cond: {isPresent: labels}
                  then:
                  - --labels
                  - {inputValue: labels}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - if:
                  cond: {isPresent: encryption_spec_key_name}
                  then:
                  - --encryption-spec-key-name
                  - {inputValue: encryption_spec_key_name}
              - if:
                  cond: {isPresent: staging_bucket}
                  then:
                  - --staging-bucket
                  - {inputValue: staging_bucket}
              - '----output-paths'
              - {outputPath: dataset_name}
              - {outputPath: dataset_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/00d020c29a144cee7fd35f2d05053addb942f536/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml
        digest: 1dd993468cad985c6bcaa40e8f8ccd685e579d4ed0d6d900e43cce2b4f762c9f
        text: |
          name: Create tabular dataset from GCS for Google Cloud Vertex AI
          description: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in
            GCS.
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml'}
          inputs:
          - name: data_uri
            type: GoogleCloudStorageUri
            description: |-
              Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
              The bucket must be a regional bucket in the us-central1 region.
              The file name must have a (case-insensitive) '.CSV' file extension.
          - name: display_name
            type: String
            description: |-
              Display name for the AutoML Dataset.
              Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
            optional: true
          - name: encryption_spec_key_name
            type: String
            description: |-
              Optional. The Cloud KMS resource identifier of the customer
              managed encryption key used to protect a resource. Has the
              form:
              ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
              The key needs to be in the same region as where the compute
              resource is created.
            optional: true
          - {name: project, type: String, description: 'Google Cloud project ID. If not set,
              the default one will be used.', optional: true}
          - {name: location, type: String, description: Google Cloud region. AutoML Tables only
              supports us-central1., default: us-central1, optional: true}
          outputs:
          - {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}
          - {name: dataset_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-cloud-aiplatform==1.1.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.1.1'
                --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(
                    data_uri,  # data_type: "CSV"
                    display_name = None,
                    encryption_spec_key_name = None,
                    project = None,
                    location = 'us-central1',
                ):
                    '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.

                    Annotations:
                        author: Alexey Volkov <alexey.volkov@ark-kun.com>

                    Args:
                        data_uri: Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
                            The bucket must be a regional bucket in the us-central1 region.
                            The file name must have a (case-insensitive) '.CSV' file extension.
                        display_name: Display name for the AutoML Dataset.
                            Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                        encryption_spec_key_name (Optional[str]):
                            Optional. The Cloud KMS resource identifier of the customer
                            managed encryption key used to protect a resource. Has the
                            form:
                            ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
                            The key needs to be in the same region as where the compute
                            resource is created.
                        project: Google Cloud project ID. If not set, the default one will be used.
                        location: Google Cloud region. AutoML Tables only supports us-central1.
                    Returns:
                        dataset_name: Dataset name (fully-qualified)
                        dataset_dict: Dataset object in JSON format
                    '''

                    import datetime
                    import json
                    import logging
                    import os

                    from google.cloud import aiplatform
                    from google.protobuf import json_format

                    logging.getLogger().setLevel(logging.INFO)

                    if not display_name:
                        display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

                    # Hack to enable passing multiple URIs
                    # I could have created another component or added another input, but it seems to be too much hassle for now.
                    # An alternative would have been to accept comma-delimited or semicolon-delimited URLs.
                    if data_uri.startswith("["):
                        data_uris = json.loads(data_uri)
                    else:
                        data_uris = [data_uri]

                    # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
                    # This leads to failure when trying to create any resource in the project.
                    # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
                    # We can try and get the GCP project ID/number from the environment variables.
                    if not project:
                        project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                        if project_number:
                            print(f"Inferred project number: {project_number}")
                            project = project_number

                    if not location:
                        location = os.environ.get("CLOUD_ML_REGION")

                    aiplatform.init(
                        project=project,
                        location=location,
                        encryption_spec_key_name=encryption_spec_key_name,
                    )
                    dataset = aiplatform.TabularDataset.create(
                        display_name=display_name,
                        gcs_source=data_uris,
                    )
                    (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')
                    dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'
                    logging.info(f'Created dataset {dataset.name}.')
                    logging.info(f'Link: {dataset_web_url}')
                    dataset_json = json_format.MessageToJson(dataset._gca_resource._pb)
                    print(dataset_json)
                    return (dataset.resource_name, dataset_json, dataset_web_url)

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import argparse
                _parser = argparse.ArgumentParser(prog='Create tabular dataset from GCS for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')
                _parser.add_argument("--data-uri", dest="data_uri", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --data-uri
              - {inputValue: data_uri}
              - if:
                  cond: {isPresent: display_name}
                  then:
                  - --display-name
                  - {inputValue: display_name}
              - if:
                  cond: {isPresent: encryption_spec_key_name}
                  then:
                  - --encryption-spec-key-name
                  - {inputValue: encryption_spec_key_name}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - '----output-paths'
              - {outputPath: dataset_name}
              - {outputPath: dataset_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/00d020c29a144cee7fd35f2d05053addb942f536/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml
        digest: d2a9d89dbdd18292df8bcea8b9a8dcf570be3fd984f13d70727bbf2463d83592
        text: |
          name: Create tabular dataset from BigQuery for Google Cloud Vertex AI
          description: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in
            GCS.
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml'}
          inputs:
          - name: data_uri
            type: GoogleCloudBigQueryUri
            description: |-
              Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.
              The bucket must be a regional bucket in the us-central1 region.
              The file name must have a (case-insensitive) '.CSV' file extension.
          - name: display_name
            type: String
            description: |-
              Display name for the AutoML Dataset.
              Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
            optional: true
          - name: encryption_spec_key_name
            type: String
            description: |-
              Optional. The Cloud KMS resource identifier of the customer
              managed encryption key used to protect a resource. Has the
              form:
              ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
              The key needs to be in the same region as where the compute
              resource is created.
            optional: true
          - {name: project, type: String, description: 'Google Cloud project ID. If not set,
              the default one will be used.', optional: true}
          - {name: location, type: String, description: Google Cloud region. AutoML Tables only
              supports us-central1., default: us-central1, optional: true}
          outputs:
          - {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}
          - {name: dataset_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-cloud-aiplatform==1.1.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.1.1'
                --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def create_tabular_dataset_from_BigQuery_for_Google_Cloud_Vertex_AI(
                    data_uri,
                    display_name = None,
                    encryption_spec_key_name = None,
                    project = None,
                    location = 'us-central1',
                ):
                    '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.

                    Annotations:
                        author: Alexey Volkov <alexey.volkov@ark-kun.com>

                    Args:
                        data_uri: Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.
                            The bucket must be a regional bucket in the us-central1 region.
                            The file name must have a (case-insensitive) '.CSV' file extension.
                        display_name: Display name for the AutoML Dataset.
                            Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                        encryption_spec_key_name (Optional[str]):
                            Optional. The Cloud KMS resource identifier of the customer
                            managed encryption key used to protect a resource. Has the
                            form:
                            ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
                            The key needs to be in the same region as where the compute
                            resource is created.
                        project: Google Cloud project ID. If not set, the default one will be used.
                        location: Google Cloud region. AutoML Tables only supports us-central1.
                    Returns:
                        dataset_name: Dataset name (fully-qualified)
                        dataset_dict: Dataset object in JSON format
                    '''

                    import datetime
                    import json
                    import logging
                    import os

                    from google.cloud import aiplatform
                    from google.protobuf import json_format

                    logging.getLogger().setLevel(logging.INFO)

                    if not display_name:
                        display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

                    # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
                    # This leads to failure when trying to create any resource in the project.
                    # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
                    # We can try and get the GCP project ID/number from the environment variables.
                    if not project:
                        project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                        if project_number:
                            print(f"Inferred project number: {project_number}")
                            project = project_number

                    if not location:
                        location = os.environ.get("CLOUD_ML_REGION")

                    aiplatform.init(
                        project=project,
                        location=location,
                        encryption_spec_key_name=encryption_spec_key_name,
                    )
                    dataset = aiplatform.TabularDataset.create(
                        display_name=display_name,
                        bq_source=data_uri,
                    )
                    (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')
                    dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'
                    logging.info(f'Created dataset {dataset.name}.')
                    logging.info(f'Link: {dataset_web_url}')
                    dataset_json = json_format.MessageToJson(dataset._gca_resource._pb)
                    print(dataset_json)
                    return (dataset.resource_name, dataset_json, dataset_web_url)

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import argparse
                _parser = argparse.ArgumentParser(prog='Create tabular dataset from BigQuery for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')
                _parser.add_argument("--data-uri", dest="data_uri", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = create_tabular_dataset_from_BigQuery_for_Google_Cloud_Vertex_AI(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --data-uri
              - {inputValue: data_uri}
              - if:
                  cond: {isPresent: display_name}
                  then:
                  - --display-name
                  - {inputValue: display_name}
              - if:
                  cond: {isPresent: encryption_spec_key_name}
                  then:
                  - --encryption-spec-key-name
                  - {inputValue: encryption_spec_key_name}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - '----output-paths'
              - {outputPath: dataset_name}
              - {outputPath: dataset_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/ab85ecc9c30d4d68a2993ca87861f5e531a4f41b/components/google-cloud/Vertex_AI/AutoML/Tables/Train_model/component.yaml
        digest: d55cf5568f1ed1ddbbf4a3837e77c555e46fb247f11bd21a30c5d2418366b36e
        text: |
          name: Train tabular model using Google Cloud Vertex AI AutoML
          description: Trains model using Google Cloud Vertex AI AutoML.
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Train_model/component.yaml'}
          inputs:
          - name: dataset_name
            type: GoogleCloudVertexAiTabularDatasetName
            description: |-
              Required. The full name of dataset  (datasets.TabularDataset) within the same Project from which data will be used to train the Model. The
              Dataset must use schema compatible with Model being trained,
              and what is compatible should be described in the used
              TrainingPipeline's [training_task_definition]
              [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].
              For tabular Datasets, all their data is exported to
              training, to pick and choose from.
          - {name: target_column, type: String, description: Required. The name of the column
              values of which the Model is to predict.}
          - name: optimization_prediction_type
            type: String
            description: |-
              The type of prediction the Model is to produce.
              "classification" - Predict one out of multiple target values is
              picked for each row.
              "regression" - Predict a value based on its relation to other values.
              This type is available only to columns that contain
              semantically numeric values, i.e. integers or floating
              point number, even if stored as e.g. strings.
          - name: training_fraction_split
            type: Float
            description: |-
              Required. The fraction of the input data that is to be
              used to train the Model. This is ignored if Dataset is not provided.
            default: '0.8'
            optional: true
          - name: validation_fraction_split
            type: Float
            description: |-
              Required. The fraction of the input data that is to be
              used to validate the Model. This is ignored if Dataset is not provided.
            default: '0.1'
            optional: true
          - name: test_fraction_split
            type: Float
            description: |-
              Required. The fraction of the input data that is to be
              used to evaluate the Model. This is ignored if Dataset is not provided.
            default: '0.1'
            optional: true
          - name: predefined_split_column_name
            type: String
            description: |-
              Optional. The key is a name of one of the Dataset's data
              columns. The value of the key (either the label's value or
              value in the column) must be one of {``training``,
              ``validation``, ``test``}, and it defines to which set the
              given piece of data is assigned. If for a piece of data the
              key is not present or has an invalid value, that piece is
              ignored by the pipeline.

              Supported only for tabular and time series Datasets.
            optional: true
          - name: weight_column
            type: String
            description: |-
              Optional. Name of the column that should be used as the weight column.
              Higher values in this column give more importance to the row
              during Model training. The column must have numeric values between 0 and
              10000 inclusively, and 0 value means that the row is ignored.
              If the weight column field is not set, then all rows are assumed to have
              equal weight of 1.
            optional: true
          - name: budget_milli_node_hours
            type: Integer
            description: |-
              Optional. The train budget of creating this Model, expressed in milli node
              hours i.e. 1,000 value in this field means 1 node hour.
              The training cost of the model will not exceed this budget. The final
              cost will be attempted to be close to the budget, though may end up
              being (even) noticeably smaller - at the backend's discretion. This
              especially may happen when further model training ceases to provide
              any improvements.
              If the budget is set to a value known to be insufficient to train a
              Model for the given training set, the training won't be attempted and
              will error.
              The minimum value is 1000 and the maximum is 72000.
            default: '1000'
            optional: true
          - name: model_display_name
            type: String
            description: |-
              Optional. If the script produces a managed Vertex AI Model. The display name of
              the Model. The name can be up to 128 characters long and can be consist
              of any UTF-8 characters.

              If not provided upon creation, the job's display_name is used.
            optional: true
          - name: disable_early_stopping
            type: Boolean
            description: |-
              Required. If true, the entire budget is used. This disables the early stopping
              feature. By default, the early stopping feature is enabled, which means
              that training might stop before the entire training budget has been
              used, if further training does no longer brings significant improvement
              to the model.
            default: "False"
            optional: true
          - name: optimization_objective
            type: String
            description: |-
              Optional. Objective function the Model is to be optimized towards. The training
              task creates a Model that maximizes/minimizes the value of the objective
              function over the validation set.

              The supported optimization objectives depend on the prediction type, and
              in the case of classification also the number of distinct values in the
              target column (two distint values -> binary, 3 or more distinct values
              -> multi class).
              If the field is not set, the default objective function is used.

              Classification (binary):
              "maximize-au-roc" (default) - Maximize the area under the receiver
                                          operating characteristic (ROC) curve.
              "minimize-log-loss" - Minimize log loss.
              "maximize-au-prc" - Maximize the area under the precision-recall curve.
              "maximize-precision-at-recall" - Maximize precision for a specified
                                              recall value.
              "maximize-recall-at-precision" - Maximize recall for a specified
                                              precision value.

              Classification (multi class):
              "minimize-log-loss" (default) - Minimize log loss.

              Regression:
              "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE).
              "minimize-mae" - Minimize mean-absolute error (MAE).
              "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
            optional: true
          - name: optimization_objective_recall_value
            type: Float
            description: |-
              Optional. Required when maximize-precision-at-recall optimizationObjective was
              picked, represents the recall value at which the optimization is done.

              The minimum value is 0 and the maximum is 1.0.
            optional: true
          - name: optimization_objective_precision_value
            type: Float
            description: |-
              Optional. Required when maximize-recall-at-precision optimizationObjective was
              picked, represents the precision value at which the optimization is
              done.

              The minimum value is 0 and the maximum is 1.0.
            optional: true
          - {name: project, type: String, optional: true}
          - {name: location, type: String, default: us-central1, optional: true}
          - {name: encryption_spec_key_name, type: String, optional: true}
          outputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName}
          - {name: model_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-cloud-aiplatform==1.6.2' 'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
                python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.6.2'
                'google-api-python-client==2.29.0' --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def train_tabular_model_using_Google_Cloud_Vertex_AI_AutoML(
                    # AutoMLTabularTrainingJob.run required parameters
                    dataset_name,
                    target_column,

                    # AutoMLTabularTrainingJob.__init__ required parameters
                    # display_name: str,
                    optimization_prediction_type,

                    # AutoMLTabularTrainingJob.run parameters
                    training_fraction_split = 0.8,
                    validation_fraction_split = 0.1,
                    test_fraction_split = 0.1,
                    predefined_split_column_name = None,
                    weight_column = None,
                    budget_milli_node_hours = 1000,
                    model_display_name = None,
                    disable_early_stopping = False,

                    # AutoMLTabularTrainingJob.__init__ parameters
                    optimization_objective = None,
                    #column_transformations: Union[Dict, List[Dict], NoneType] = None,
                    optimization_objective_recall_value = None,
                    optimization_objective_precision_value = None,

                    project = None,
                    location = 'us-central1',
                    #training_encryption_spec_key_name: str = None,
                    #model_encryption_spec_key_name: str = None,
                    encryption_spec_key_name = None,
                ):
                    '''Trains model using Google Cloud Vertex AI AutoML.

                    Data fraction splits:
                    Any of ``training_fraction_split``, ``validation_fraction_split`` and
                    ``test_fraction_split`` may optionally be provided, they must sum to up to 1. If
                    the provided ones sum to less than 1, the remainder is assigned to sets as
                    decided by Vertex AI. If none of the fractions are set, by default roughly 80%
                    of data will be used for training, 10% for validation, and 10% for test.

                    Annotations:
                        author: Alexey Volkov <alexey.volkov@ark-kun.com>

                    Args:
                        dataset_name:
                            Required. The full name of dataset  (datasets.TabularDataset) within the same Project from which data will be used to train the Model. The
                            Dataset must use schema compatible with Model being trained,
                            and what is compatible should be described in the used
                            TrainingPipeline's [training_task_definition]
                            [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].
                            For tabular Datasets, all their data is exported to
                            training, to pick and choose from.
                        target_column (str):
                            Required. The name of the column values of which the Model is to predict.
                        training_fraction_split (float):
                            Required. The fraction of the input data that is to be
                            used to train the Model. This is ignored if Dataset is not provided.
                        validation_fraction_split (float):
                            Required. The fraction of the input data that is to be
                            used to validate the Model. This is ignored if Dataset is not provided.
                        test_fraction_split (float):
                            Required. The fraction of the input data that is to be
                            used to evaluate the Model. This is ignored if Dataset is not provided.
                        predefined_split_column_name (str):
                            Optional. The key is a name of one of the Dataset's data
                            columns. The value of the key (either the label's value or
                            value in the column) must be one of {``training``,
                            ``validation``, ``test``}, and it defines to which set the
                            given piece of data is assigned. If for a piece of data the
                            key is not present or has an invalid value, that piece is
                            ignored by the pipeline.

                            Supported only for tabular and time series Datasets.
                        weight_column (str):
                            Optional. Name of the column that should be used as the weight column.
                            Higher values in this column give more importance to the row
                            during Model training. The column must have numeric values between 0 and
                            10000 inclusively, and 0 value means that the row is ignored.
                            If the weight column field is not set, then all rows are assumed to have
                            equal weight of 1.
                        budget_milli_node_hours (int):
                            Optional. The train budget of creating this Model, expressed in milli node
                            hours i.e. 1,000 value in this field means 1 node hour.
                            The training cost of the model will not exceed this budget. The final
                            cost will be attempted to be close to the budget, though may end up
                            being (even) noticeably smaller - at the backend's discretion. This
                            especially may happen when further model training ceases to provide
                            any improvements.
                            If the budget is set to a value known to be insufficient to train a
                            Model for the given training set, the training won't be attempted and
                            will error.
                            The minimum value is 1000 and the maximum is 72000.
                        model_display_name (str):
                            Optional. If the script produces a managed Vertex AI Model. The display name of
                            the Model. The name can be up to 128 characters long and can be consist
                            of any UTF-8 characters.

                            If not provided upon creation, the job's display_name is used.
                        disable_early_stopping (bool):
                            Required. If true, the entire budget is used. This disables the early stopping
                            feature. By default, the early stopping feature is enabled, which means
                            that training might stop before the entire training budget has been
                            used, if further training does no longer brings significant improvement
                            to the model.

                        optimization_prediction_type (str):
                            The type of prediction the Model is to produce.
                            "classification" - Predict one out of multiple target values is
                            picked for each row.
                            "regression" - Predict a value based on its relation to other values.
                            This type is available only to columns that contain
                            semantically numeric values, i.e. integers or floating
                            point number, even if stored as e.g. strings.
                        optimization_objective (str):
                            Optional. Objective function the Model is to be optimized towards. The training
                            task creates a Model that maximizes/minimizes the value of the objective
                            function over the validation set.

                            The supported optimization objectives depend on the prediction type, and
                            in the case of classification also the number of distinct values in the
                            target column (two distint values -> binary, 3 or more distinct values
                            -> multi class).
                            If the field is not set, the default objective function is used.

                            Classification (binary):
                            "maximize-au-roc" (default) - Maximize the area under the receiver
                                                        operating characteristic (ROC) curve.
                            "minimize-log-loss" - Minimize log loss.
                            "maximize-au-prc" - Maximize the area under the precision-recall curve.
                            "maximize-precision-at-recall" - Maximize precision for a specified
                                                            recall value.
                            "maximize-recall-at-precision" - Maximize recall for a specified
                                                            precision value.

                            Classification (multi class):
                            "minimize-log-loss" (default) - Minimize log loss.

                            Regression:
                            "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE).
                            "minimize-mae" - Minimize mean-absolute error (MAE).
                            "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
                        column_transformations (Optional[Union[Dict, List[Dict]]]):
                            Optional. Transformations to apply to the input columns (i.e. columns other
                            than the targetColumn). Each transformation may produce multiple
                            result values from the column's value, and all are used for training.
                            When creating transformation for BigQuery Struct column, the column
                            should be flattened using "." as the delimiter.
                            If an input column has no transformations on it, such a column is
                            ignored by the training, except for the targetColumn, which should have
                            no transformations defined on.
                        optimization_objective_recall_value (float):
                            Optional. Required when maximize-precision-at-recall optimizationObjective was
                            picked, represents the recall value at which the optimization is done.

                            The minimum value is 0 and the maximum is 1.0.
                        optimization_objective_precision_value (float):
                            Optional. Required when maximize-recall-at-precision optimizationObjective was
                            picked, represents the precision value at which the optimization is
                            done.

                            The minimum value is 0 and the maximum is 1.0.

                    Returns:
                        model_name: Model name (fully-qualified)
                        model_dict: Model metadata in JSON format
                    '''

                    import datetime
                    import logging
                    import os

                    from google.cloud import aiplatform
                    from google.protobuf import json_format

                    logging.getLogger().setLevel(logging.INFO)

                    if not model_display_name:
                        model_display_name = 'TablesModel_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

                    # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
                    # This leads to failure when trying to create any resource in the project.
                    # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
                    # We can try and get the GCP project ID/number from the environment variables.
                    if not project:
                        project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                        if project_number:
                            print(f"Inferred project number: {project_number}")
                            project = project_number
                            # To improve the naming we try to convert the project number into the user project ID.
                            try:
                                from googleapiclient import discovery

                                cloud_resource_manager_service = discovery.build(
                                    "cloudresourcemanager", "v3"
                                )
                                project_id = (
                                    cloud_resource_manager_service.projects()
                                    .get(name=f"projects/{project_number}")
                                    .execute()["projectId"]
                                )
                                if project_id:
                                    print(f"Inferred project ID: {project_id}")
                                    project = project_id
                            except Exception as e:
                                print(e)

                    aiplatform.init(
                        project=project,
                        location=location,
                        encryption_spec_key_name=encryption_spec_key_name,
                    )

                    model = aiplatform.AutoMLTabularTrainingJob(
                        display_name='AutoMLTabularTrainingJob_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S"),
                        optimization_prediction_type=optimization_prediction_type,
                        optimization_objective=optimization_objective,
                        #column_transformations=column_transformations,
                        optimization_objective_recall_value=optimization_objective_recall_value,
                        optimization_objective_precision_value=optimization_objective_precision_value,
                    ).run(
                        dataset=aiplatform.TabularDataset(dataset_name=dataset_name),
                        target_column=target_column,
                        training_fraction_split=training_fraction_split,
                        validation_fraction_split=validation_fraction_split,
                        test_fraction_split=test_fraction_split,
                        predefined_split_column_name=predefined_split_column_name,
                        weight_column=weight_column,
                        budget_milli_node_hours=budget_milli_node_hours,
                        model_display_name=model_display_name,
                        disable_early_stopping=disable_early_stopping,
                    )

                    (_, model_project, _, model_location, _, model_id) = model.resource_name.split('/')
                    model_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{model_location}/models/{model_id}/evaluate?project={model_project}'
                    logging.info(f'Created model {model.name}.')
                    logging.info(f'Link: {model_web_url}')
                    model_json = json_format.MessageToJson(model._gca_resource._pb)
                    print(model_json)
                    return (model.resource_name, model_json, model_web_url)

                def _deserialize_bool(s) -> bool:
                    from distutils.util import strtobool
                    return strtobool(s) == 1

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import argparse
                _parser = argparse.ArgumentParser(prog='Train tabular model using Google Cloud Vertex AI AutoML', description='Trains model using Google Cloud Vertex AI AutoML.')
                _parser.add_argument("--dataset-name", dest="dataset_name", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--target-column", dest="target_column", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--optimization-prediction-type", dest="optimization_prediction_type", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--training-fraction-split", dest="training_fraction_split", type=float, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--validation-fraction-split", dest="validation_fraction_split", type=float, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--test-fraction-split", dest="test_fraction_split", type=float, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--predefined-split-column-name", dest="predefined_split_column_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--weight-column", dest="weight_column", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--budget-milli-node-hours", dest="budget_milli_node_hours", type=int, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--model-display-name", dest="model_display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--disable-early-stopping", dest="disable_early_stopping", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--optimization-objective", dest="optimization_objective", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--optimization-objective-recall-value", dest="optimization_objective_recall_value", type=float, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--optimization-objective-precision-value", dest="optimization_objective_precision_value", type=float, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = train_tabular_model_using_Google_Cloud_Vertex_AI_AutoML(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --dataset-name
              - {inputValue: dataset_name}
              - --target-column
              - {inputValue: target_column}
              - --optimization-prediction-type
              - {inputValue: optimization_prediction_type}
              - if:
                  cond: {isPresent: training_fraction_split}
                  then:
                  - --training-fraction-split
                  - {inputValue: training_fraction_split}
              - if:
                  cond: {isPresent: validation_fraction_split}
                  then:
                  - --validation-fraction-split
                  - {inputValue: validation_fraction_split}
              - if:
                  cond: {isPresent: test_fraction_split}
                  then:
                  - --test-fraction-split
                  - {inputValue: test_fraction_split}
              - if:
                  cond: {isPresent: predefined_split_column_name}
                  then:
                  - --predefined-split-column-name
                  - {inputValue: predefined_split_column_name}
              - if:
                  cond: {isPresent: weight_column}
                  then:
                  - --weight-column
                  - {inputValue: weight_column}
              - if:
                  cond: {isPresent: budget_milli_node_hours}
                  then:
                  - --budget-milli-node-hours
                  - {inputValue: budget_milli_node_hours}
              - if:
                  cond: {isPresent: model_display_name}
                  then:
                  - --model-display-name
                  - {inputValue: model_display_name}
              - if:
                  cond: {isPresent: disable_early_stopping}
                  then:
                  - --disable-early-stopping
                  - {inputValue: disable_early_stopping}
              - if:
                  cond: {isPresent: optimization_objective}
                  then:
                  - --optimization-objective
                  - {inputValue: optimization_objective}
              - if:
                  cond: {isPresent: optimization_objective_recall_value}
                  then:
                  - --optimization-objective-recall-value
                  - {inputValue: optimization_objective_recall_value}
              - if:
                  cond: {isPresent: optimization_objective_precision_value}
                  then:
                  - --optimization-objective-precision-value
                  - {inputValue: optimization_objective_precision_value}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - if:
                  cond: {isPresent: encryption_spec_key_name}
                  then:
                  - --encryption-spec-key-name
                  - {inputValue: encryption_spec_key_name}
              - '----output-paths'
              - {outputPath: model_name}
              - {outputPath: model_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/a31b7c9652646f2cd035a0b3a23e0723c632521b/components/google-cloud/Vertex_AI/AutoML/Tables/Get_model_tuning_trials/component.yaml
        digest: ae740e7eee8da0a5ef27268370d03baaf86d2d10ef8d3640fbfda6604cf309f1
        text: |
          name: Get model tuning trials for Google Cloud Vertex AI AutoML Tables
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Get_model_tuning_trials/component.yaml'}
          inputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName}
          outputs:
          - {name: tuning_trials, type: JsonArray}
          - {name: model_structures, type: JsonArray}
          - {name: extra_entries, type: JsonArray}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-cloud-logging==2.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
                pip install --quiet --no-warn-script-location 'google-cloud-logging==2.7.0'
                --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def get_model_tuning_trials_for_Google_Cloud_Vertex_AI_AutoML_Tables(
                    model_name,
                ):
                    import json
                    from google.cloud import logging as cloud_logging

                    (_, project, _, location, _, model_id) = model_name.split("/")

                    # Need to specify project when initializing client.
                    # Otherwise we'll get error when running on Vertex AI Pipelines:
                    # google.api_core.exceptions.PermissionDenied: 403 The caller does not have permission
                    cloud_logging_client = cloud_logging.Client(project=project)

                    # Full filter:
                    # resource.type="cloudml_job" resource.labels.job_id="{job_id}" resource.labels.project_id="{project_id}" labels.log_type="automl_tables" jsonPayload."@type"="type.googleapis.com/google.cloud.automl.master.TuningTrial"
                    log_filter=f'resource.labels.job_id="{model_id}"'
                    log_entry_list = list(cloud_logging_client.list_entries(filter_=log_filter))

                    tuning_trials = []
                    model_structures = []
                    extra_entries = []
                    for entry in log_entry_list:
                        if entry.payload.get("@type") == "type.googleapis.com/google.cloud.automl.master.TuningTrial":
                            tuning_trials.append(entry.payload)
                        elif entry.payload.get("@type") == "type.googleapis.com/google.cloud.automl.master.TablesModelStructure":
                            model_structures.append(entry.payload)
                        else:
                            extra_entries.append(entry.payload)

                    # Manually serializing the results for pretty and stable output
                    print("Tuning trials:")
                    tuning_trials_json = json.dumps(tuning_trials, sort_keys=True, indent=2)
                    print(tuning_trials_json)

                    print("Model structures:")
                    model_structures_json = json.dumps(model_structures, sort_keys=True, indent=2)
                    print(model_structures_json)

                    print("Extra entries:")
                    extra_entries_json = json.dumps(extra_entries, sort_keys=True, indent=2)
                    print(extra_entries_json)

                    return (tuning_trials_json, model_structures_json, extra_entries_json)

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import argparse
                _parser = argparse.ArgumentParser(prog='Get model tuning trials for Google Cloud Vertex AI AutoML Tables', description='')
                _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = get_model_tuning_trials_for_Google_Cloud_Vertex_AI_AutoML_Tables(**_parsed_args)

                _output_serializers = [
                    _serialize_json,
                    _serialize_json,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --model-name
              - {inputValue: model_name}
              - '----output-paths'
              - {outputPath: tuning_trials}
              - {outputPath: model_structures}
              - {outputPath: extra_entries}
    - name: Models
      components:
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/2c24c0c0730c818b89f676c4dc5c9d6cb90ab01d/components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml
        digest: 28cee22ce598009bdc4b54c7ba604a43f69d1de2f5eef0070544d10742b9502a
        text: |
          name: Upload XGBoost model to Google Cloud Vertex AI
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml'}
          inputs:
          - {name: model, type: XGBoostModel}
          - {name: xgboost_version, type: String, optional: true}
          - {name: display_name, type: String, optional: true}
          - {name: description, type: String, optional: true}
          - {name: project, type: String, optional: true}
          - {name: location, type: String, default: us-central1, optional: true}
          - {name: labels, type: JsonObject, optional: true}
          - {name: staging_bucket, type: String, optional: true}
          outputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName}
          - {name: model_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'
                'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                -m pip install --quiet --no-warn-script-location 'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'
                'google-api-python-client==2.29.0' --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def upload_XGBoost_model_to_Google_Cloud_Vertex_AI(
                    model_path,
                    xgboost_version = None,

                    display_name = None,
                    description = None,

                    # Uncomment when anyone requests these:
                    # instance_schema_uri: str = None,
                    # parameters_schema_uri: str = None,
                    # prediction_schema_uri: str = None,
                    # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
                    # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,

                    project = None,
                    location = "us-central1",
                    labels = None,
                    # encryption_spec_key_name: str = None,
                    staging_bucket = None,
                ):
                    kwargs = locals()
                    kwargs.pop("model_path")

                    import json
                    import os
                    from google.cloud import aiplatform

                    # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
                    # This leads to failure when trying to create any resource in the project.
                    # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
                    # We can try and get the GCP project ID/number from the environment variables.
                    if not project:
                        project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                        if project_number:
                            print(f"Inferred project number: {project_number}")
                            kwargs["project"] = project_number
                            # To improve the naming we try to convert the project number into the user project ID.
                            try:
                                from googleapiclient import discovery

                                cloud_resource_manager_service = discovery.build(
                                    "cloudresourcemanager", "v3"
                                )
                                project_id = (
                                    cloud_resource_manager_service.projects()
                                    .get(name=f"projects/{project_number}")
                                    .execute()["projectId"]
                                )
                                if project_id:
                                    print(f"Inferred project ID: {project_id}")
                                    kwargs["project"] = project_id
                            except Exception as e:
                                print(e)

                    if not location:
                        kwargs["location"] = os.environ.get("CLOUD_ML_REGION")

                    if not labels:
                        kwargs["labels"] = {}
                    kwargs["labels"]["component-source"] = "github-com-ark-kun-pipeline-components"

                    model = aiplatform.Model.upload_xgboost_model_file(
                        model_file_path=model_path,
                        **kwargs,
                    )
                    model_json = json.dumps(model.to_dict(), indent=2)
                    print(model_json)
                    return (model.resource_name, model_json)

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import json
                import argparse
                _parser = argparse.ArgumentParser(prog='Upload XGBoost model to Google Cloud Vertex AI', description='')
                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--xgboost-version", dest="xgboost_version", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--description", dest="description", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--labels", dest="labels", type=json.loads, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--staging-bucket", dest="staging_bucket", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = upload_XGBoost_model_to_Google_Cloud_Vertex_AI(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --model
              - {inputPath: model}
              - if:
                  cond: {isPresent: xgboost_version}
                  then:
                  - --xgboost-version
                  - {inputValue: xgboost_version}
              - if:
                  cond: {isPresent: display_name}
                  then:
                  - --display-name
                  - {inputValue: display_name}
              - if:
                  cond: {isPresent: description}
                  then:
                  - --description
                  - {inputValue: description}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - if:
                  cond: {isPresent: labels}
                  then:
                  - --labels
                  - {inputValue: labels}
              - if:
                  cond: {isPresent: staging_bucket}
                  then:
                  - --staging-bucket
                  - {inputValue: staging_bucket}
              - '----output-paths'
              - {outputPath: model_name}
              - {outputPath: model_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/25dc317e649a19a53139a08ccbe496a248693fe4/components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml
        digest: e9b9a7a27e5df18ba90de9757137a1ae23a8c20ae5be5487942dc4d2abd1dcdc
        text: |
          name: Upload Scikit learn pickle model to Google Cloud Vertex AI
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml'}
          inputs:
          - {name: model, type: ScikitLearnPickleModel}
          - {name: sklearn_version, type: String, optional: true}
          - {name: display_name, type: String, optional: true}
          - {name: description, type: String, optional: true}
          - {name: project, type: String, optional: true}
          - {name: location, type: String, default: us-central1, optional: true}
          - {name: labels, type: JsonObject, optional: true}
          - {name: staging_bucket, type: String, optional: true}
          outputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName}
          - {name: model_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'git+https://github.com/Ark-kun/python-aiplatform@9b50f62b9d1409644656fb3202edc7be19c722f4#egg=google-cloud-aiplatform&subdirectory=.'
                || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'git+https://github.com/Ark-kun/python-aiplatform@9b50f62b9d1409644656fb3202edc7be19c722f4#egg=google-cloud-aiplatform&subdirectory=.'
                --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI(
                    model_path,
                    sklearn_version = None,

                    display_name = None,
                    description = None,

                    # Uncomment when anyone requests these:
                    # instance_schema_uri: str = None,
                    # parameters_schema_uri: str = None,
                    # prediction_schema_uri: str = None,
                    # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
                    # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,

                    project = None,
                    location = "us-central1",
                    labels = None,
                    # encryption_spec_key_name: str = None,
                    staging_bucket = None,
                ):
                    import datetime
                    import json
                    import os
                    import shutil
                    import tempfile
                    from google.cloud import aiplatform

                    if not location:
                        location = os.environ.get("CLOUD_ML_REGION")

                    if not labels:
                        labels = {}
                    labels["component-source"] = "github-com-ark-kun-pipeline-components"

                    # The serving container decides the model type based on the model file extension.
                    # So we need to rename the mode file (e.g. /tmp/inputs/model/data) to *.pkl
                    _, renamed_model_path = tempfile.mkstemp(suffix=".pkl")
                    shutil.copyfile(src=model_path, dst=renamed_model_path)

                    display_name = display_name or "Scikit-learn model " + datetime.datetime.now().isoformat(sep=" ")

                    model = aiplatform.Model.upload_scikit_learn_model_file(
                        model_file_path=renamed_model_path,
                        sklearn_version=sklearn_version,

                        display_name=display_name,
                        description=description,

                        # instance_schema_uri=instance_schema_uri,
                        # parameters_schema_uri=parameters_schema_uri,
                        # prediction_schema_uri=prediction_schema_uri,
                        # explanation_metadata=explanation_metadata,
                        # explanation_parameters=explanation_parameters,

                        project=project,
                        location=location,
                        labels=labels,
                        # encryption_spec_key_name=encryption_spec_key_name,
                        staging_bucket=staging_bucket,
                    )
                    model_json = json.dumps(model.to_dict(), indent=2)
                    print(model_json)
                    return (model.resource_name, model_json)

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import json
                import argparse
                _parser = argparse.ArgumentParser(prog='Upload Scikit learn pickle model to Google Cloud Vertex AI', description='')
                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--sklearn-version", dest="sklearn_version", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--description", dest="description", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--labels", dest="labels", type=json.loads, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--staging-bucket", dest="staging_bucket", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --model
              - {inputPath: model}
              - if:
                  cond: {isPresent: sklearn_version}
                  then:
                  - --sklearn-version
                  - {inputValue: sklearn_version}
              - if:
                  cond: {isPresent: display_name}
                  then:
                  - --display-name
                  - {inputValue: display_name}
              - if:
                  cond: {isPresent: description}
                  then:
                  - --description
                  - {inputValue: description}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - if:
                  cond: {isPresent: labels}
                  then:
                  - --labels
                  - {inputValue: labels}
              - if:
                  cond: {isPresent: staging_bucket}
                  then:
                  - --staging-bucket
                  - {inputValue: staging_bucket}
              - '----output-paths'
              - {outputPath: model_name}
              - {outputPath: model_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/719783ef44c04348ea23e247a93021d91cfe602d/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml
        digest: d56fa60de311e41e8a50970330a5f23ea5c3c75f950ffa030714383dad971c5d
        text: |
          name: Upload Tensorflow model to Google Cloud Vertex AI
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml'}
          inputs:
          - {name: model, type: TensorflowSavedModel}
          - {name: tensorflow_version, type: String, default: '2.7', optional: true}
          - name: use_gpu
            type: Boolean
            default: "False"
            optional: true
          - {name: display_name, type: String, default: Tensorflow model, optional: true}
          - {name: description, type: String, optional: true}
          - {name: project, type: String, optional: true}
          - {name: location, type: String, default: us-central1, optional: true}
          - {name: labels, type: JsonObject, optional: true}
          - {name: staging_bucket, type: String, optional: true}
          outputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName}
          - {name: model_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'git+https://github.com/Ark-kun/python-aiplatform@1e1cbfef76b7c5c8db705d5c4c17c3691de7b032#egg=google-cloud-aiplatform&subdirectory=.'
                || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'git+https://github.com/Ark-kun/python-aiplatform@1e1cbfef76b7c5c8db705d5c4c17c3691de7b032#egg=google-cloud-aiplatform&subdirectory=.'
                --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def upload_Tensorflow_model_to_Google_Cloud_Vertex_AI(
                    model_path,
                    tensorflow_version = "2.7",  # TODO: Remove the explicit default once the upload_tensorflow_saved_model supports None
                    use_gpu = False,

                    display_name = "Tensorflow model",
                    description = None,

                    # Uncomment when anyone requests these:
                    # instance_schema_uri: str = None,
                    # parameters_schema_uri: str = None,
                    # prediction_schema_uri: str = None,
                    # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
                    # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,

                    project = None,
                    location = "us-central1",
                    labels = None,
                    # encryption_spec_key_name: str = None,
                    staging_bucket = None,
                ):
                    import json
                    import os
                    from google.cloud import aiplatform

                    if not location:
                        location = os.environ.get("CLOUD_ML_REGION")

                    if not labels:
                        labels = {}
                    labels["component-source"] = "github-com-ark-kun-pipeline-components"

                    model = aiplatform.Model.upload_tensorflow_saved_model(
                        saved_model_dir=model_path,
                        tensorflow_version=tensorflow_version,
                        use_gpu=use_gpu,

                        display_name=display_name,
                        description=description,

                        # instance_schema_uri=instance_schema_uri,
                        # parameters_schema_uri=parameters_schema_uri,
                        # prediction_schema_uri=prediction_schema_uri,
                        # explanation_metadata=explanation_metadata,
                        # explanation_parameters=explanation_parameters,

                        project=project,
                        location=location,
                        labels=labels,
                        # encryption_spec_key_name=encryption_spec_key_name,
                        staging_bucket=staging_bucket,
                    )
                    model_json = json.dumps(model.to_dict(), indent=2)
                    print(model_json)
                    return (model.resource_name, model_json)

                def _deserialize_bool(s) -> bool:
                    from distutils.util import strtobool
                    return strtobool(s) == 1

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import json
                import argparse
                _parser = argparse.ArgumentParser(prog='Upload Tensorflow model to Google Cloud Vertex AI', description='')
                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--tensorflow-version", dest="tensorflow_version", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--use-gpu", dest="use_gpu", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--description", dest="description", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--labels", dest="labels", type=json.loads, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--staging-bucket", dest="staging_bucket", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --model
              - {inputPath: model}
              - if:
                  cond: {isPresent: tensorflow_version}
                  then:
                  - --tensorflow-version
                  - {inputValue: tensorflow_version}
              - if:
                  cond: {isPresent: use_gpu}
                  then:
                  - --use-gpu
                  - {inputValue: use_gpu}
              - if:
                  cond: {isPresent: display_name}
                  then:
                  - --display-name
                  - {inputValue: display_name}
              - if:
                  cond: {isPresent: description}
                  then:
                  - --description
                  - {inputValue: description}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - if:
                  cond: {isPresent: labels}
                  then:
                  - --labels
                  - {inputValue: labels}
              - if:
                  cond: {isPresent: staging_bucket}
                  then:
                  - --staging-bucket
                  - {inputValue: staging_bucket}
              - '----output-paths'
              - {outputPath: model_name}
              - {outputPath: model_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d1e7a3ccf8f8e0324e15922d6fd90d667fc5281b/components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml
        digest: 9ebdd5840bc86cb37320040183998d3856b917be42ca0f9f034f59139a140550
        text: |
          name: Upload PyTorch model archive to Google Cloud Vertex AI
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml'}
          inputs:
          - {name: model_archive, type: PyTorchModelArchive}
          - {name: torchserve_version, type: String, default: 0.6.0, optional: true}
          - name: use_gpu
            type: Boolean
            default: "False"
            optional: true
          - {name: display_name, type: String, optional: true}
          - {name: description, type: String, optional: true}
          - {name: project, type: String, optional: true}
          - {name: location, type: String, optional: true}
          - {name: labels, type: JsonObject, optional: true}
          - {name: staging_bucket, type: String, optional: true}
          outputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName}
          - {name: model_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-cloud-aiplatform==1.13.1' 'google-cloud-build==3.8.3' || PIP_DISABLE_PIP_VERSION_CHECK=1
                python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.13.1'
                'google-cloud-build==3.8.3' --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI(
                    model_archive_path,
                    torchserve_version = "0.6.0",
                    use_gpu = False,

                    display_name = None,
                    description = None,

                    # Uncomment when anyone requests these:
                    # instance_schema_uri: str = None,
                    # parameters_schema_uri: str = None,
                    # prediction_schema_uri: str = None,
                    # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
                    # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,

                    project = None,
                    location = None,
                    labels = None,
                    # encryption_spec_key_name: str = None,
                    staging_bucket = None,
                ):
                    import json
                    import os
                    from google.cloud import aiplatform

                    if not location:
                        location = os.environ.get("CLOUD_ML_REGION")

                    if not labels:
                        labels = {}
                    labels["component-source"] = "github-com-ark-kun-pipeline-components"

                    container_image_tag = torchserve_version + "-" + ("gpu" if use_gpu else "cpu")
                    container_image_uri = f"pytorch/torchserve:{container_image_tag}"

                    # Vertex Endpoints refuse to support non-Google container registries.
                    # We have to work around this to reduce user frustration
                    # TODO: Remove this code when Vertex Endpoints service starts supporting other container registries.
                    def copy_container_image(
                        src_container_image_uri,
                        dst_container_image_uri,
                        project_id,
                    ):
                        from google.cloud.devtools import cloudbuild
                        from google import protobuf
                        build_client = cloudbuild.CloudBuildClient()
                        build_config = cloudbuild.Build(
                            images=[dst_container_image_uri],
                            steps=[
                                cloudbuild.BuildStep(
                                    name="gcr.io/cloud-builders/docker",
                                    entrypoint="bash",
                                    args=[
                                        "-exc",
                                        'docker pull --quiet "$0" && docker tag "$0" "$1"',
                                        src_container_image_uri,
                                        dst_container_image_uri,
                                    ],
                                ),
                            ],
                            timeout=protobuf.duration_pb2.Duration(
                                seconds=1800,
                            ),
                        )
                        build_operation = build_client.create_build(
                            project_id=project_id,
                            build=build_config,
                        )
                        try:
                            result = build_operation.result()
                        except:
                            print(f"Logs are available at [{build_operation.metadata.build.log_url}].")
                            raise
                        return result

                    project_id = aiplatform.initializer.global_config.project
                    mirrored_container_uri = f"gcr.io/{project_id}/container_mirror/{container_image_uri}"
                    # FIX: Only mirror when image does not exist
                    # docker does is unable to get the registry data from inside container (it cannot connecto to docker socket):
                    # docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))
                    # import docker
                    # try:
                    #     docker_client = docker.from_env()
                    #     docker_client.images.get_registry_data(mirrored_container_uri)
                    # except docker.errors.NotFound:
                    if True:
                        print(f"Mirroring {container_image_uri} to {mirrored_container_uri}")
                        copy_container_image(
                            src_container_image_uri=container_image_uri,
                            dst_container_image_uri=mirrored_container_uri,
                            project_id=project_id,
                        )
                    container_image_uri = mirrored_container_uri
                    # End of container image mirroring code

                    model_archive_file_name = os.path.basename(model_archive_path)
                    model_archive_dir = os.path.dirname(model_archive_path)

                    model = aiplatform.Model.upload(
                        # FIX: Use public image or mirror the official image
                        #serving_container_image_uri="gcr.io/avolkov-31337/mirror/pytorch/torchserve",
                        serving_container_image_uri=container_image_uri,
                        artifact_uri=model_archive_dir,
                        serving_container_command=[
                            "bash",
                            "-exc",
                            '''
                model_archive_uri="$0"
                #model_archive_local_path=$(mktemp --suffix ".mar")
                # For some reason the model must already be inside the model-store directory.
                model_archive_local_path=./model-store/model.mar

                # Downloading the model archive from GCS
                # TODO: Fix gsutil bugs (requires project ID, has auth issues) and use gsutil instead.
                # gsutil cp "$model_archive_uri" "$model_archive_local_path"
                pip install google-cloud-storage
                python -c '
                import sys
                from google.cloud import storage

                model_archive_uri = sys.argv[1]
                model_archive_local_path = sys.argv[2]

                storage_client = storage.Client()
                blob = storage.Blob.from_string(uri=model_archive_uri, client=storage_client)
                blob.download_to_filename(filename=model_archive_local_path)
                ' "$model_archive_uri" "$model_archive_local_path"

                #Note: config.properties is owned by root. Our user is not root.
                echo "
                service_envelope=json
                # Needed for external access
                inference_address=http://0.0.0.0:8080
                management_address=http://0.0.0.0:8081
                " > config2.properties
                torchserve --start --foreground --no-config-snapshots --models main-model="$model_archive_local_path" --model-store ./model-store/ --ts-config config2.properties
                            ''',
                            "$(AIP_STORAGE_URI)/" + model_archive_file_name,
                        ],
                        serving_container_predict_route="/predictions/main-model",
                        #serving_container_predict_route="/v1/models/main-model:predict",
                        serving_container_health_route="/ping",
                        serving_container_ports=[8080],

                        display_name=display_name,
                        description=description,

                        # instance_schema_uri=instance_schema_uri,
                        # parameters_schema_uri=parameters_schema_uri,
                        # prediction_schema_uri=prediction_schema_uri,
                        # explanation_metadata=explanation_metadata,
                        # explanation_parameters=explanation_parameters,

                        project=project,
                        location=location,
                        labels=labels,
                        # encryption_spec_key_name=encryption_spec_key_name,
                        staging_bucket=staging_bucket,
                    )
                    model_json = json.dumps(model.to_dict(), indent=2)
                    print(model_json)
                    return (model.resource_name, model_json)

                def _deserialize_bool(s) -> bool:
                    from distutils.util import strtobool
                    return strtobool(s) == 1

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import json
                import argparse
                _parser = argparse.ArgumentParser(prog='Upload PyTorch model archive to Google Cloud Vertex AI', description='')
                _parser.add_argument("--model-archive", dest="model_archive_path", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--torchserve-version", dest="torchserve_version", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--use-gpu", dest="use_gpu", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--description", dest="description", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--labels", dest="labels", type=json.loads, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--staging-bucket", dest="staging_bucket", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --model-archive
              - {inputPath: model_archive}
              - if:
                  cond: {isPresent: torchserve_version}
                  then:
                  - --torchserve-version
                  - {inputValue: torchserve_version}
              - if:
                  cond: {isPresent: use_gpu}
                  then:
                  - --use-gpu
                  - {inputValue: use_gpu}
              - if:
                  cond: {isPresent: display_name}
                  then:
                  - --display-name
                  - {inputValue: display_name}
              - if:
                  cond: {isPresent: description}
                  then:
                  - --description
                  - {inputValue: description}
              - if:
                  cond: {isPresent: project}
                  then:
                  - --project
                  - {inputValue: project}
              - if:
                  cond: {isPresent: location}
                  then:
                  - --location
                  - {inputValue: location}
              - if:
                  cond: {isPresent: labels}
                  then:
                  - --labels
                  - {inputValue: labels}
              - if:
                  cond: {isPresent: staging_bucket}
                  then:
                  - --staging-bucket
                  - {inputValue: staging_bucket}
              - '----output-paths'
              - {outputPath: model_name}
              - {outputPath: model_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/b2cdd60fe93d609111729ef64e79a8b8a2713435/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml
        digest: 3d9ecac802bc46de2614d3eee883cfdae58acc3d1be34f867aad2f81e519db12
        text: |
          name: Deploy model to endpoint for Google Cloud Vertex AI Model
          description: Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml'}
          inputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName, description: Full resource
              name of a Google Cloud Vertex AI Model}
          - name: endpoint_name
            type: GoogleCloudVertexAiEndpointName
            description: |-
              Optional. Full name of Google Cloud Vertex Endpoint. A new
              endpoint is created if the name is not passed.
            optional: true
          - name: machine_type
            type: String
            description: |-
              The type of the machine. See the [list of machine types
              supported for prediction
              ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
              Defaults to "n1-standard-2"
            default: n1-standard-2
            optional: true
          - name: min_replica_count
            type: Integer
            description: |-
              Optional. The minimum number of machine replicas this deployed
              model will be always deployed on. If traffic against it increases,
              it may dynamically be deployed onto more replicas, and as traffic
              decreases, some of these extra replicas may be freed.
            default: '1'
            optional: true
          - name: max_replica_count
            type: Integer
            description: |-
              Optional. The maximum number of replicas this deployed model may
              be deployed on when the traffic against it increases. If requested
              value is too large, the deployment will error, but if deployment
              succeeds then the ability to scale the model to that many replicas
              is guaranteed (barring service outages). If traffic against the
              deployed model increases beyond what its replicas at maximum may
              handle, a portion of the traffic will be dropped. If this value
              is not provided, the smaller value of min_replica_count or 1 will
              be used.
            default: '1'
            optional: true
          - name: accelerator_type
            type: String
            description: |-
              Optional. Hardware accelerator type. Must also set accelerator_count if used.
              One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,
              NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
            optional: true
          - {name: accelerator_count, type: Integer, description: Optional. The number of accelerators
              to attach to a worker replica., optional: true}
          outputs:
          - {name: endpoint_name, type: GoogleCloudVertexAiEndpointName}
          - {name: endpoint_dict, type: JsonObject}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-cloud-aiplatform==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.7.0'
                --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(
                    model_name,
                    endpoint_name = None,
                    machine_type = "n1-standard-2",
                    min_replica_count = 1,
                    max_replica_count = 1,
                    accelerator_type = None,
                    accelerator_count = None,
                    #
                    # Uncomment when anyone requests these:
                    # deployed_model_display_name: str = None,
                    # traffic_percentage: int = 0,
                    # traffic_split: dict = None,
                    # service_account: str = None,
                    # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
                    # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,
                    #
                    # encryption_spec_key_name: str = None,
                ):
                    """Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.

                    Args:
                        model_name: Full resource name of a Google Cloud Vertex AI Model
                        endpoint_name: Optional. Full name of Google Cloud Vertex Endpoint. A new
                            endpoint is created if the name is not passed.
                        machine_type: The type of the machine. See the [list of machine types
                            supported for prediction
                            ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
                            Defaults to "n1-standard-2"
                        min_replica_count (int):
                            Optional. The minimum number of machine replicas this deployed
                            model will be always deployed on. If traffic against it increases,
                            it may dynamically be deployed onto more replicas, and as traffic
                            decreases, some of these extra replicas may be freed.
                        max_replica_count (int):
                            Optional. The maximum number of replicas this deployed model may
                            be deployed on when the traffic against it increases. If requested
                            value is too large, the deployment will error, but if deployment
                            succeeds then the ability to scale the model to that many replicas
                            is guaranteed (barring service outages). If traffic against the
                            deployed model increases beyond what its replicas at maximum may
                            handle, a portion of the traffic will be dropped. If this value
                            is not provided, the smaller value of min_replica_count or 1 will
                            be used.
                        accelerator_type (str):
                            Optional. Hardware accelerator type. Must also set accelerator_count if used.
                            One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,
                            NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
                        accelerator_count (int):
                            Optional. The number of accelerators to attach to a worker replica.
                    """
                    import json
                    from google.cloud import aiplatform

                    model = aiplatform.Model(model_name=model_name)

                    if endpoint_name:
                        endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)
                    else:
                        endpoint_display_name = model.display_name[:118] + "_endpoint"
                        endpoint = aiplatform.Endpoint.create(
                            display_name=endpoint_display_name,
                            project=model.project,
                            location=model.location,
                            # encryption_spec_key_name=encryption_spec_key_name,
                            labels={"component-source": "github-com-ark-kun-pipeline-components"},
                        )

                    endpoint = model.deploy(
                        endpoint=endpoint,
                        # deployed_model_display_name=deployed_model_display_name,
                        machine_type=machine_type,
                        min_replica_count=min_replica_count,
                        max_replica_count=max_replica_count,
                        accelerator_type=accelerator_type,
                        accelerator_count=accelerator_count,
                        # service_account=service_account,
                        # explanation_metadata=explanation_metadata,
                        # explanation_parameters=explanation_parameters,
                        # encryption_spec_key_name=encryption_spec_key_name,
                    )

                    endpoint_json = json.dumps(endpoint.to_dict(), indent=2)
                    print(endpoint_json)
                    return (endpoint.resource_name, endpoint_json)

                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                import argparse
                _parser = argparse.ArgumentParser(prog='Deploy model to endpoint for Google Cloud Vertex AI Model', description='Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.')
                _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--endpoint-name", dest="endpoint_name", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--machine-type", dest="machine_type", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--min-replica-count", dest="min_replica_count", type=int, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--max-replica-count", dest="max_replica_count", type=int, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--accelerator-type", dest="accelerator_type", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--accelerator-count", dest="accelerator_count", type=int, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(**_parsed_args)

                _output_serializers = [
                    str,
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --model-name
              - {inputValue: model_name}
              - if:
                  cond: {isPresent: endpoint_name}
                  then:
                  - --endpoint-name
                  - {inputValue: endpoint_name}
              - if:
                  cond: {isPresent: machine_type}
                  then:
                  - --machine-type
                  - {inputValue: machine_type}
              - if:
                  cond: {isPresent: min_replica_count}
                  then:
                  - --min-replica-count
                  - {inputValue: min_replica_count}
              - if:
                  cond: {isPresent: max_replica_count}
                  then:
                  - --max-replica-count
                  - {inputValue: max_replica_count}
              - if:
                  cond: {isPresent: accelerator_type}
                  then:
                  - --accelerator-type
                  - {inputValue: accelerator_type}
              - if:
                  cond: {isPresent: accelerator_count}
                  then:
                  - --accelerator-count
                  - {inputValue: accelerator_count}
              - '----output-paths'
              - {outputPath: endpoint_name}
              - {outputPath: endpoint_dict}
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d45e011ad8b62b4fe36c12289a624e5e1573c68d/components/google-cloud/Vertex_AI/Models/Export/to_GCS/component.yaml
        digest: 7525be19088b075bf2f404f4ca8173ccb18220250198699a323b3a836ae814f9
        text: |
          name: Export model to GCS for Google Cloud Vertex AI Model
          metadata:
            annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Export/to_GCS/component.yaml'}
          inputs:
          - {name: model_name, type: GoogleCloudVertexAiModelName}
          - {name: output_prefix_gcs_uri, type: String}
          - {name: export_format, type: String, optional: true}
          outputs:
          - {name: model_dir_uri, type: String}
          implementation:
            container:
              image: python:3.9
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-cloud-aiplatform==1.6.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.6.2'
                --user) && "$0" "$@"
              - sh
              - -ec
              - |
                program_path=$(mktemp)
                printf "%s" "$0" > "$program_path"
                python3 -u "$program_path" "$@"
              - |
                def export_model_to_GCS_for_Google_Cloud_Vertex_AI_Model(
                    model_name,
                    output_prefix_gcs_uri,  # GoogleCloudStorageURI
                    export_format = None,
                ):
                    # Choose output_prefix_gcs_uri properly to avoid the following error:
                    # google.api_core.exceptions.FailedPrecondition: 400 The Cloud Storage bucket of `gs://<output_prefix_gcs_uri>/model-7972079425934065664/tf-saved-model/2021-11-08T10:44:57.671790Z` is in location `us`.
                    # It must be in the same regional location as the service location `us-central1`.
                    from google.cloud import aiplatform

                    model = aiplatform.Model(model_name=model_name)

                    print("Available export formats:")
                    print(model.supported_export_formats)
                    if not export_format:
                        export_format = list(model.supported_export_formats.keys())[0]
                        print(f"Auto-selected export formats: {export_format}")

                    result = model.export_model(
                        export_format_id=export_format,
                        artifact_destination=output_prefix_gcs_uri,
                    )

                    # == "gs://<artifact_destination>/model-7972079425934065664/tf-saved-model/2021-11-08T00:54:18.367871Z"
                    artifact_output_uri = result["artifactOutputUri"]

                    return (artifact_output_uri,)

                def _serialize_str(str_value: str) -> str:
                    if not isinstance(str_value, str):
                        raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
                    return str_value

                import argparse
                _parser = argparse.ArgumentParser(prog='Export model to GCS for Google Cloud Vertex AI Model', description='')
                _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--output-prefix-gcs-uri", dest="output_prefix_gcs_uri", type=str, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--export-format", dest="export_format", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = export_model_to_GCS_for_Google_Cloud_Vertex_AI_Model(**_parsed_args)

                _output_serializers = [
                    _serialize_str,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --model-name
              - {inputValue: model_name}
              - --output-prefix-gcs-uri
              - {inputValue: output_prefix_gcs_uri}
              - if:
                  cond: {isPresent: export_format}
                  then:
                  - --export-format
                  - {inputValue: export_format}
              - '----output-paths'
              - {outputPath: model_dir_uri}
  - name: Storage
    components:
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml
      digest: 30c424ac6156c478aa0c3027b470baf9cb7dbbf90aebcabde7469bfbd02a512e
      text: |
        name: Download from GCS
        inputs:
        - {name: GCS path, type: URI}
        outputs:
        - {name: Data}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download/component.yaml'
        implementation:
            container:
                image: google/cloud-sdk
                command:
                - bash # Pattern comparison only works in Bash
                - -ex
                - -c
                - |
                    if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                        gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                    fi

                    uri="$0"
                    output_path="$1"

                    # Checking whether the URI points to a single blob, a directory or a URI pattern
                    # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI
                    if [[ "$uri" != */ ]] && (gsutil ls "$uri" | grep --fixed-strings --line-regexp "$uri"); then
                        mkdir -p "$(dirname "$output_path")"
                        gsutil -m cp -r "$uri" "$output_path"
                    else
                        mkdir -p "$output_path" # When source path is a directory, gsutil requires the destination to also be a directory
                        gsutil -m rsync -r "$uri" "$output_path" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.
                    fi
                - inputValue: GCS path
                - outputPath: Data
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml
      digest: e3cfa607ab9e2e0312ef4268dad157edf30c3d4ba5059b2e295fe047258aa31d
      text: |
        name: Upload to GCS
        inputs:
        - {name: Data}
        - {name: GCS path, type: URI}
        outputs:
        - {name: GCS path, type: String}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'
        implementation:
            container:
                image: google/cloud-sdk
                command:
                - sh
                - -ex
                - -c
                - |
                    if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                        gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                    fi
                    gsutil cp -r "$0" "$1"
                    mkdir -p "$(dirname "$2")"
                    printf "%s" "$1" > "$2"
                - inputPath: Data
                - inputValue: GCS path
                - outputPath: GCS path
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml
      digest: 074b5c68679117f5aeb41e36ee75b8c349ec88631ba552cbf6991bd5942d9192
      text: |-
        name: Upload to GCS with unique name
        description: Upload to GCS with unique URI suffix
        inputs:
        - {name: Data}
        - {name: GCS path prefix, type: URI}
        outputs:
        - {name: GCS path, type: String}
        metadata:
          annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_unique_uri/component.yaml'
        implementation:
            container:
                image: google/cloud-sdk
                command:
                - sh
                - -ex
                - -c
                - |
                    data_path="$0"
                    url_prefix="$1"
                    output_path="$2"
                    random_string=$(< dev/urandom tr -dc A-Za-z0-9 | head -c 64)
                    uri="${url_prefix}${random_string}"
                    if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                        gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                    fi
                    gsutil cp -r "$data_path" "$uri"
                    mkdir -p "$(dirname "$output_path")"
                    printf "%s" "$uri" > "$output_path"
                - inputPath: Data
                - {inputValue: GCS path prefix}
                - outputPath: GCS path
    - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/list/component.yaml
      digest: 8283b7ce8e2ffafe453833f8e1695f43243ae19547b1c84c71f4f32b2f4c780b
      text: |
        name: List blobs
        inputs:
        - {name: GCS path, type: URI, description: 'GCS path for listing. For recursive listing use the "gs://bucket/path/**" syntax".'}
        outputs:
        - {name: Paths}
        metadata:
            annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
                canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/list/component.yaml'
                volatile_component: 'true'
        implementation:
            container:
                image: google/cloud-sdk
                command:
                - sh
                - -ex
                - -c
                - |
                    if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                        gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                    fi
                    mkdir -p "$(dirname "$1")"
                    gsutil ls "$0" > "$1"
                - inputValue: GCS path
                - outputPath: Paths
  - name: AI Platform (legacy)
    folders:
    - name: Optimizer
      components:
      - url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/1b87c0bdfde5d7ec039401af8561783432731402/components/google-cloud/Optimizer/Suggest_parameter_sets_based_on_measurements/component.yaml
        digest: 3c81f810d29ed692b1579064afa15202779789a0648a0b365723adb4c6b78860
        text: |
          name: Suggest parameter sets from measurements using gcp ai platform optimizer
          description: Suggests trials (parameter sets) to evaluate.
          inputs:
          - {name: parameter_specs, type: JsonArray, description: 'List of parameter specs.
              See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec'}
          - {name: metrics_for_parameter_sets, type: JsonArray, description: 'List of parameter
              sets and evaluation metrics for them. Each list item contains "parameters" dict
              and "metrics" dict. Example: {"parameters": {"p1": 1.1, "p2": 2.2}, "metrics":
              {"metric1": 101, "metric2": 102} }'}
          - {name: suggestion_count, type: Integer, description: Number of suggestions to request.}
          - name: maximize
            type: Boolean
            description: Whether to miaximize or minimize when optimizing a single metric.Default
              is to minimize. Ignored if metric_specs list is provided.
            default: "False"
            optional: true
          - {name: metric_specs, type: JsonArray, description: 'List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec',
            optional: true}
          - {name: gcp_project_id, type: String, optional: true}
          - {name: gcp_region, type: String, default: us-central1, optional: true}
          outputs:
          - {name: suggested_parameter_sets, type: JsonArray}
          metadata:
            annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
              canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Suggest_parameter_sets_based_on_measurements/component.yaml'
          implementation:
            container:
              image: python:3.8
              command:
              - sh
              - -c
              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-api-python-client==1.12.3' 'google-auth==1.21.3'
                || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                'google-api-python-client==1.12.3' 'google-auth==1.21.3'
                --user) && "$0" "$@"
              - python3
              - -u
              - -c
              - |
                def suggest_parameter_sets_from_measurements_using_gcp_ai_platform_optimizer(
                    parameter_specs,
                    metrics_for_parameter_sets,
                    suggestion_count,
                    maximize = False,
                    metric_specs = None,
                    gcp_project_id = None,
                    gcp_region = "us-central1",
                ):
                    """Suggests trials (parameter sets) to evaluate.
                    See https://cloud.google.com/ai-platform/optimizer/docs

                    Annotations:
                        author: Alexey Volkov <alexey.volkov@ark-kun.com>

                    Args:
                        parameter_specs: List of parameter specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec
                        metrics_for_parameter_sets: List of parameter sets and evaluation metrics for them. Each list item contains "parameters" dict and "metrics" dict. Example: {"parameters": {"p1": 1.1, "p2": 2.2}, "metrics": {"metric1": 101, "metric2": 102} }
                        maximize: Whether to miaximize or minimize when optimizing a single metric.Default is to minimize. Ignored if metric_specs list is provided.
                        metric_specs: List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec
                        suggestion_count: Number of suggestions to request.

                        suggested_parameter_sets: List of parameter set dictionaries.
                    """

                    import logging
                    import random
                    import time

                    import google.auth
                    from googleapiclient import discovery

                    logging.getLogger().setLevel(logging.INFO)

                    client_id = 'client1'

                    credentials, default_project_id = google.auth.default()

                    # Validating and inferring the arguments
                    if not gcp_project_id:
                        gcp_project_id = default_project_id

                    # Building the API client.
                    # The main API does not work, so we need to build from the published discovery document.
                    def create_caip_optimizer_client(project_id):
                        from googleapiclient import discovery
                        # The discovery is broken. See https://github.com/googleapis/google-api-python-client/issues/1470
                        # return discovery.build("ml", "v1")
                        return discovery.build("ml", "v1", discoveryServiceUrl='https://storage.googleapis.com/caip-optimizer-public/api/ml_public_google_rest_v1.json')

                    # Workaround for the Optimizer bug: Optimizer returns resource names that use project number, but only supports resource names with project IDs when making requests
                    def get_project_number(project_id):
                        service = discovery.build('cloudresourcemanager', 'v1', credentials=credentials)
                        response = service.projects().get(projectId=project_id).execute()
                        return response['projectNumber']

                    gcp_project_number = get_project_number(gcp_project_id)

                    def fix_resource_name(name):
                        return name.replace(gcp_project_number, gcp_project_id)

                    ml_api = create_caip_optimizer_client(gcp_project_id)
                    studies_api = ml_api.projects().locations().studies()
                    trials_api = ml_api.projects().locations().studies().trials()
                    operations_api = ml_api.projects().locations().operations()

                    random_integer = random.SystemRandom().getrandbits(256)
                    study_id = '{:064x}'.format(random_integer)

                    if not metric_specs:
                        metric_specs=[{
                            'metric': 'metric',
                            'goal': 'MAXIMIZE' if maximize else 'MINIMIZE',
                        }]
                    study_config = {
                        'algorithm': 'ALGORITHM_UNSPECIFIED',  # Let the service choose the `default` algorithm.
                        'parameters': parameter_specs,
                        'metrics': metric_specs,
                    }
                    study = {'study_config': study_config}

                    logging.info(f'Creating temporary study {study_id}')
                    create_study_request = studies_api.create(
                        parent=f'projects/{gcp_project_id}/locations/{gcp_region}',
                        studyId=study_id,
                        body=study,
                    )
                    create_study_response = create_study_request.execute()
                    study_name = create_study_response['name']

                    paremeter_type_names = {parameter_spec['parameter']: parameter_spec['type'] for parameter_spec in parameter_specs}
                    def parameter_name_and_value_to_dict(parameter_name, parameter_value):
                        result = {'parameter': parameter_name}
                        paremeter_type_name = paremeter_type_names[parameter_name]
                        if paremeter_type_name in ['DOUBLE', 'DISCRETE']:
                            result['floatValue'] = parameter_value
                        elif paremeter_type_name == 'INTEGER':
                            result['intValue'] = parameter_value
                        elif paremeter_type_name == 'CATEGORICAL':
                            result['stringValue'] = parameter_value
                        else:
                            raise TypeError(f'Unsupported parameter type "{paremeter_type_name}"')
                        return result

                    try:
                        logging.info(f'Adding {len(metrics_for_parameter_sets)} measurements to the study.')
                        for parameters_and_metrics in metrics_for_parameter_sets:
                            parameter_set = parameters_and_metrics['parameters']
                            metrics_set = parameters_and_metrics['metrics']
                            trial = {
                                'parameters': [
                                    parameter_name_and_value_to_dict(parameter_name, parameter_value)
                                    for parameter_name, parameter_value in parameter_set.items()
                                ],
                                'finalMeasurement': {
                                    'metrics': [
                                        {
                                            'metric': metric_name,
                                            'value': metric_value,
                                        }
                                        for metric_name, metric_value in metrics_set.items()
                                    ],
                                },
                                'state': 'COMPLETED',
                            }
                            create_trial_response = trials_api.create(
                                parent=fix_resource_name(study_name),
                                body=trial,
                            ).execute()
                            trial_name = create_trial_response["name"]
                            logging.info(f'Added trial "{trial_name}" to the study.')

                        logging.info(f'Requesting suggestions.')
                        suggest_trials_request = trials_api.suggest(
                            parent=fix_resource_name(study_name),
                            body=dict(
                                suggestionCount=suggestion_count,
                                clientId=client_id,
                            ),
                        )
                        suggest_trials_response = suggest_trials_request.execute()
                        operation_name = suggest_trials_response['name']
                        while True:
                            get_operation_response = operations_api.get(
                                name=fix_resource_name(operation_name),
                            ).execute()
                            # Knowledge: The "done" key is just missing until the result is available
                            if get_operation_response.get('done'):
                                break
                            logging.info('Operation not finished yet: ' + str(get_operation_response))
                            time.sleep(10)
                        operation_response = get_operation_response['response']
                        suggested_trials = operation_response['trials']

                        suggested_parameter_sets = [
                            {
                                parameter['parameter']: parameter.get('floatValue') or parameter.get('intValue') or parameter.get('stringValue') or 0.0
                                for parameter in trial['parameters']
                            }
                            for trial in suggested_trials
                        ]
                        return (suggested_parameter_sets,)
                    finally:
                        logging.info(f'Deleting study: "{study_name}"')
                        studies_api.delete(name=fix_resource_name(study_name))

                import json
                def _serialize_json(obj) -> str:
                    if isinstance(obj, str):
                        return obj
                    import json
                    def default_serializer(obj):
                        if hasattr(obj, 'to_struct'):
                            return obj.to_struct()
                        else:
                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                def _deserialize_bool(s) -> bool:
                    from distutils.util import strtobool
                    return strtobool(s) == 1

                import argparse
                _parser = argparse.ArgumentParser(prog='Suggest parameter sets from measurements using gcp ai platform optimizer', description='Suggests trials (parameter sets) to evaluate.')
                _parser.add_argument("--parameter-specs", dest="parameter_specs", type=json.loads, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--metrics-for-parameter-sets", dest="metrics_for_parameter_sets", type=json.loads, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--suggestion-count", dest="suggestion_count", type=int, required=True, default=argparse.SUPPRESS)
                _parser.add_argument("--maximize", dest="maximize", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--metric-specs", dest="metric_specs", type=json.loads, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
                _parsed_args = vars(_parser.parse_args())
                _output_files = _parsed_args.pop("_output_paths", [])

                _outputs = suggest_parameter_sets_from_measurements_using_gcp_ai_platform_optimizer(**_parsed_args)

                _output_serializers = [
                    _serialize_json,

                ]

                import os
                for idx, output_file in enumerate(_output_files):
                    try:
                        os.makedirs(os.path.dirname(output_file))
                    except OSError:
                        pass
                    with open(output_file, 'w') as f:
                        f.write(_output_serializers[idx](_outputs[idx]))
              args:
              - --parameter-specs
              - {inputValue: parameter_specs}
              - --metrics-for-parameter-sets
              - {inputValue: metrics_for_parameter_sets}
              - --suggestion-count
              - {inputValue: suggestion_count}
              - if:
                  cond: {isPresent: maximize}
                  then:
                  - --maximize
                  - {inputValue: maximize}
              - if:
                  cond: {isPresent: metric_specs}
                  then:
                  - --metric-specs
                  - {inputValue: metric_specs}
              - if:
                  cond: {isPresent: gcp_project_id}
                  then:
                  - --gcp-project-id
                  - {inputValue: gcp_project_id}
              - if:
                  cond: {isPresent: gcp_region}
                  then:
                  - --gcp-region
                  - {inputValue: gcp_region}
              - '----output-paths'
              - {outputPath: suggested_parameter_sets}
